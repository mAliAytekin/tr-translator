# en-tr translator with transformer architecture

trained on mac m3 pro about 260 minutes
has 17,979,168 parameters
hyperparameters 
- vocab_size=800
- d_model=256
- max_seq_len = 128
- num_heads=8
- num_encoder_layers=6
- num_decoder_layers=6
- d_ff=2048

<img width="1427" height="598" alt="image" src="https://github.com/user-attachments/assets/db126add-24c9-4c80-a7be-5d2faf6f4701" />
<img width="1335" height="582" alt="image" src="https://github.com/user-attachments/assets/78216e11-e6c3-4d09-9dcf-437784b23c59" />

