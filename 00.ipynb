{
 "cells": [
  {
   "cell_type": "code",
   "id": "449c13138aefa1af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:14:49.885710Z",
     "start_time": "2026-01-04T11:14:49.883118Z"
    }
   },
   "source": [
    "# ! pip install sentencepiece"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "f679653e84ed1692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:14:50.385345Z",
     "start_time": "2026-01-04T11:14:49.894959Z"
    }
   },
   "source": [
    "import torch\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"MPS built:\", torch.backends.mps.is_built())\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import torch.nn as nn\n",
    "import math"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.1\n",
      "MPS built: True\n",
      "MPS available: True\n",
      "mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "168da269a86dbf81",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f8dc25d6e717770",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:14:50.391793Z",
     "start_time": "2026-01-04T11:14:50.390223Z"
    }
   },
   "source": [
    "vocab_size=800\n",
    "d_model=256\n",
    "max_seq_len = 128\n",
    "num_heads=8\n",
    "num_encoder_layers=6\n",
    "num_decoder_layers=6\n",
    "d_ff=2048"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "4ce80cc886c0c92",
   "metadata": {},
   "source": [
    "### files"
   ]
  },
  {
   "cell_type": "code",
   "id": "82e03f3683b5afad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:14:50.793431Z",
     "start_time": "2026-01-04T11:14:50.398301Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def create_data_files(source_file, target_file,dataset,max_line_count=None):\n",
    "    line_count = 0\n",
    "    for example in tqdm(dataset):\n",
    "        source_text = example[\"source\"].replace(\"\\n\", \" \").strip()\n",
    "        target_text = example[\"translated\"].replace(\"\\n\", \" \").strip()\n",
    "        source_file.write(source_text + \"\\n\")\n",
    "        target_file.write(target_text + \"\\n\")\n",
    "        line_count=line_count+1\n",
    "        if max_line_count is not None:\n",
    "            if line_count >= max_line_count:\n",
    "                break\n",
    "\n",
    "    source_file.close()\n",
    "    target_file.close()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mytkn/PycharmProjects/SmallLanguageModels/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "9344747991e9c9a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:14:50.798699Z",
     "start_time": "2026-01-04T11:14:50.797138Z"
    }
   },
   "source": [
    "check_file = True\n",
    "if check_file:\n",
    "    ds = load_dataset(\"cturan/high-quality-english-turkish-sentences\")\n",
    "\n",
    "    train_line_count = None\n",
    "\n",
    "    train_source_file = open(\"data/train_source.txt\", \"w\")\n",
    "    train_target_file = open(\"data/train_target.txt\", \"w\")\n",
    "\n",
    "    create_data_files(train_source_file, train_target_file,ds[\"train\"],train_line_count)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "5c4d76a81f700851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:14:50.810618Z",
     "start_time": "2026-01-04T11:14:50.806931Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "def delete_files_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f'Deleted: {file_path}')\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "\n",
    "delete_files_in_folder('tokenizers')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: tokenizers/tr_bpe_32k.model\n",
      "Deleted: tokenizers/en_bpe_32k.vocab\n",
      "Deleted: tokenizers/en_bpe_32k.model\n",
      "Deleted: tokenizers/tr_bpe_32k.vocab\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "4068bde82cdb02d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:14:50.827568Z",
     "start_time": "2026-01-04T11:14:50.825812Z"
    }
   },
   "source": [
    "def print_test_heading(heading):\n",
    "    print(\"-\"*60)\n",
    "    print(heading)\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "ab28405666fc022c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.707904Z",
     "start_time": "2026-01-04T11:14:50.830824Z"
    }
   },
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# TÜRKÇE TOKENİZER EĞİTİMİ\n",
    "# ============================================================\n",
    "\n",
    "def train_tokenizer(input_file, model_prefix, vocab_size=32000):\n",
    "    \"\"\"\n",
    "    SentencePiece BPE tokenizer eğit.\n",
    "\n",
    "    Args:\n",
    "        input_file: Eğitim corpus dosyası (her satır bir cümle)\n",
    "        model_prefix: Çıktı model ismi (örn: 'tr_bpe_32k')\n",
    "        vocab_size: Vocabulary boyutu\n",
    "    \"\"\"\n",
    "    print(f\"✅ Tokenizer eğitimi başladı: {model_prefix}.model\")\n",
    "\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=input_file,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        model_type='bpe',\n",
    "\n",
    "        # Özel tokenler\n",
    "        pad_id=0,\n",
    "        unk_id=1,\n",
    "        bos_id=2,\n",
    "        eos_id=3,\n",
    "        pad_piece='<PAD>',\n",
    "        unk_piece='<UNK>',\n",
    "        bos_piece='<BOS>',\n",
    "        eos_piece='<EOS>',\n",
    "\n",
    "        # Eğitim parametreleri\n",
    "        character_coverage=0.9995,  # Türkçe karakterler için yüksek\n",
    "        num_threads=32,\n",
    "        train_extremely_large_corpus=False,\n",
    "\n",
    "        # Normalizasyon\n",
    "        normalization_rule_name='nmt_nfkc_cf',  # Unicode normalizasyonu\n",
    "        remove_extra_whitespaces=True,\n",
    "\n",
    "\n",
    "        # Subword regularization (opsiyonel, eğitimde çeşitlilik için)\n",
    "        # split_by_unicode_script=True,\n",
    "        # byte_fallback=True,  # Bilinmeyen karakterler için\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Tokenizer eğitildi: {model_prefix}.model\")\n",
    "\n",
    "# Corpus oluştur ve tokenizer'ları eğit\n",
    "tr_corpus, en_corpus =  \"data/train_target.txt\", \"data/train_source.txt\"\n",
    "\n",
    "# NOT: Gerçek projede vocab_size=32000, demo için 1000\n",
    "train_tokenizer(tr_corpus, 'tokenizers/tr_bpe_32k', vocab_size=vocab_size)\n",
    "train_tokenizer(en_corpus, 'tokenizers/en_bpe_32k', vocab_size=vocab_size)\n",
    "\n",
    "print(\"\\n✅ Her iki tokenizer da hazır!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer eğitimi başladı: tokenizers/tr_bpe_32k.model\n",
      "✅ Tokenizer eğitildi: tokenizers/tr_bpe_32k.model\n",
      "✅ Tokenizer eğitimi başladı: tokenizers/en_bpe_32k.model\n",
      "✅ Tokenizer eğitildi: tokenizers/en_bpe_32k.model\n",
      "\n",
      "✅ Her iki tokenizer da hazır!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/train_target.txt\n",
      "  input_format: \n",
      "  model_prefix: tokenizers/tr_bpe_32k\n",
      "  model_type: BPE\n",
      "  vocab_size: 800\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 32\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <UNK>\n",
      "  bos_piece: <BOS>\n",
      "  eos_piece: <EOS>\n",
      "  pad_piece: <PAD>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc_cf\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: data/train_target.txt\n",
      "trainer_interface.cc(148) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(125) LOG(WARNING) Too many sentences are loaded! (1082358), which may slow down training.\n",
      "trainer_interface.cc(127) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(130) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 1082358 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <PAD>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <UNK>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <BOS>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <EOS>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=141443890\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9552% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=54\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999552\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 1082358 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 1082358\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 876002\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2440967 min_freq=383\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=862322 size=20 all=2458 active=2023 piece=▁o\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=445710 size=40 all=4016 active=3581 piece=as\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=301389 size=60 all=5721 active=5286 piece=ey\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=221802 size=80 all=7581 active=7146 piece=ıl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=168384 size=100 all=9291 active=8856 piece=ında\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=167636 min_freq=16903\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=143202 size=120 all=11278 active=2942 piece=larak\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=121357 size=140 all=12558 active=4222 piece=bilir\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=105628 size=160 all=14338 active=6002 piece=▁veya\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=92278 size=180 all=16007 active=7671 piece=▁yap\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=83415 size=200 all=17960 active=9624 piece=ğı\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=81108 min_freq=14124\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=72183 size=220 all=19189 active=2203 piece=larını\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66226 size=240 all=21029 active=4043 piece=yor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59910 size=260 all=22644 active=5658 piece=yan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55512 size=280 all=24218 active=7232 piece=▁fazla\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52186 size=300 all=25526 active=8540 piece=▁-\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=52124 min_freq=9954\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49823 size=320 all=27598 active=3329 piece=▁düş\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46519 size=340 all=29170 active=4901 piece=ğu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43904 size=360 all=30297 active=6028 piece=ellik\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41708 size=380 all=31636 active=7367 piece=iyet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39206 size=400 all=32831 active=8562 piece=▁karşı\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=39077 min_freq=6928\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36693 size=420 all=34554 active=3351 piece=▁am\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34273 size=440 all=35630 active=4427 piece=▁dön\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32712 size=460 all=36953 active=5750 piece=▁yaz\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31621 size=480 all=38202 active=6999 piece=▁iz\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29250 size=500 all=39780 active=8577 piece=▁çalışma\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=29187 min_freq=5281\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28349 size=520 all=41160 active=3340 piece=▁ul\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27229 size=540 all=42718 active=4898 piece=▁olmak\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26212 size=560 all=43390 active=5570 piece=va\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25236 size=580 all=44576 active=6756 piece=leştir\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24549 size=600 all=45823 active=8003 piece=han\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24549 min_freq=4199\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23438 size=620 all=47011 active=3278 piece=▁eğitim\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22585 size=640 all=48017 active=4284 piece=▁görün\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21654 size=660 all=49540 active=5807 piece=izi\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20655 size=680 all=51245 active=7512 piece=▁hükümet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20161 size=700 all=52282 active=8549 piece=▁mu\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=20154 min_freq=3440\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19424 size=720 all=53432 active=3709 piece=▁uzun\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18897 size=740 all=54487 active=4764 piece=▁merkez\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: tokenizers/tr_bpe_32k.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: tokenizers/tr_bpe_32k.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/train_source.txt\n",
      "  input_format: \n",
      "  model_prefix: tokenizers/en_bpe_32k\n",
      "  model_type: BPE\n",
      "  vocab_size: 800\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 32\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <UNK>\n",
      "  bos_piece: <BOS>\n",
      "  eos_piece: <EOS>\n",
      "  pad_piece: <PAD>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc_cf\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: data/train_source.txt\n",
      "trainer_interface.cc(148) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(125) LOG(WARNING) Too many sentences are loaded! (1082358), which may slow down training.\n",
      "trainer_interface.cc(127) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(130) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 1082358 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <PAD>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <UNK>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <BOS>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <EOS>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=145629588\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9583% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=51\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999583\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 1082358 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 1082358\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 609319\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3559764 min_freq=64\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=952501 size=20 all=2248 active=1947 piece=or\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=551480 size=40 all=3560 active=3259 piece=ent\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=288259 size=60 all=5148 active=4847 piece=▁st\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=205605 size=80 all=7133 active=6832 piece=ul\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=161596 size=100 all=8852 active=8551 piece=ate\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=161348 min_freq=14373\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=134520 size=120 all=11018 active=3039 piece=ld\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115458 size=140 all=13510 active=5531 piece=os\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=96361 size=160 all=15712 active=7733 piece=art\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=79790 size=180 all=17920 active=9941 piece=00\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=72047 size=200 all=19528 active=11549 piece=iz\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=72027 min_freq=12889\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67493 size=220 all=21462 active=2720 piece=ast\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59420 size=240 all=23479 active=4737 piece=pl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53224 size=260 all=25285 active=6543 piece=ould\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48791 size=280 all=27164 active=8422 piece=▁were\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45202 size=300 all=28888 active=10146 piece=ors\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=45063 min_freq=7648\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42791 size=320 all=30636 active=3053 piece=one\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39822 size=340 all=32132 active=4549 piece=very\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38075 size=360 all=33838 active=6255 piece=▁prov\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35993 size=380 all=35338 active=7755 piece=▁use\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34238 size=400 all=36598 active=9015 piece=ific\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=34163 min_freq=5373\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32142 size=420 all=38392 active=3528 piece=▁through\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30777 size=440 all=40088 active=5224 piece=ced\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29325 size=460 all=41186 active=6322 piece=▁sur\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27370 size=480 all=42404 active=7540 piece=▁like\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26287 size=500 all=43444 active=8580 piece=iss\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=26145 min_freq=4217\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24889 size=520 all=44753 active=3297 piece=▁techn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24190 size=540 all=45910 active=4454 piece=yp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22792 size=560 all=47350 active=5894 piece=▁loc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21992 size=580 all=48842 active=7386 piece=▁exper\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20956 size=600 all=49946 active=8490 piece=▁should\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=20945 min_freq=3394\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20059 size=620 all=50970 active=3514 piece=ives\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19455 size=640 all=52363 active=4907 piece=ruct\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18675 size=660 all=53488 active=6032 piece=▁cre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18102 size=680 all=54407 active=6951 piece=▁fact\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17626 size=700 all=55301 active=7845 piece=▁redu\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=17610 min_freq=2854\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17027 size=720 all=56577 active=4030 piece=▁br\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16367 size=740 all=57340 active=4793 piece=ually\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: tokenizers/en_bpe_32k.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: tokenizers/en_bpe_32k.vocab\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "ca5d8bce428cbf44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.748935Z",
     "start_time": "2026-01-04T11:15:14.743600Z"
    }
   },
   "source": [
    "from tokenizer import SentencePieceTokenizer\n",
    "\n",
    "tr_tokenizer = SentencePieceTokenizer('tokenizers/tr_bpe_32k.model')\n",
    "en_tokenizer = SentencePieceTokenizer('tokenizers/en_bpe_32k.model')\n",
    "\n",
    "\n",
    "# test\n",
    "print_test_heading(\"SentencePiece Test\")\n",
    "tr_tokens = tr_tokenizer.encode(\"Merhaba dünya\")\n",
    "print(tr_tokens)\n",
    "print(tr_tokenizer.decode(tr_tokens))\n",
    "\n",
    "en_tokens = en_tokenizer.encode(\"Hello world\")\n",
    "print(en_tokens)\n",
    "print(en_tokenizer.decode(en_tokens))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "SentencePiece Test\n",
      "\n",
      "[2, 589, 769, 258, 747, 668, 3]\n",
      "merhaba dünya\n",
      "[2, 108, 281, 754, 482, 3]\n",
      "hello world\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "efe008ffa1bd6908",
   "metadata": {},
   "source": [
    "### token embedding"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4044fe005bc10a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.763191Z",
     "start_time": "2026-01-04T11:15:14.761267Z"
    }
   },
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    kelimeleri sayısal vektörlere dönüştürmek için kullanılan bir katman.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,vocab_size:int,d_model:int):\n",
    "        \"\"\"\n",
    "        :param vocab_size: sözlükteki toplam kelime sayisi\n",
    "        :param d_model: her kelimenin vektör boyutu\n",
    "        \"\"\"\n",
    "        super(TokenEmbedding,self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embedding=nn.Embedding(self.vocab_size,self.d_model)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        :param x: token tensorü (batch_size, seq_len)\n",
    "        :return: embedding tensorü (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return self.embedding(x)*math.sqrt(self.d_model)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "2140f69f118d2550",
   "metadata": {},
   "source": [
    "### positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "id": "57e969e63a42895a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.775843Z",
     "start_time": "2026-01-04T11:15:14.773476Z"
    }
   },
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Kelimelerin cümle içindeki konum bilgilerini eklemek için kullanılan bir katman.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,d_model:int,max_seq_len:int=500,dropout:float=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: embedding boyutu\n",
    "        :param max_seq_len: max cümle uzunluğu\n",
    "        :param dropout: regularizasyon için dropout oranı\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        # pozisyonel encoding matrisini oluştur\n",
    "        pe=torch.zeros(self.max_seq_len,self.d_model)\n",
    "        position=torch.arange(0,self.max_seq_len).unsqueeze(1).float()\n",
    "\n",
    "        div_term=torch.exp(torch.arange(0,self.d_model,2).float()*(-math.log(10000.0)/self.d_model))\n",
    "\n",
    "        pe[:,0::2]=torch.sin(position*div_term) # çift indexler sin\n",
    "        pe[:,1::2]=torch.cos(position*div_term) # tek indexler cos\n",
    "\n",
    "        pe=pe.unsqueeze(0) # (1, max_seq_len, d_model)\n",
    "\n",
    "        # model ile kaydet, fakat gradient hesaplanmasın\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        :param x: token embedding tensorü (batch_size, seq_len, d_model)\n",
    "        :return: position-encoded embedding tensorü\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "8531dd0946aafcdb",
   "metadata": {},
   "source": [
    "### sclaed Dot-Product attention"
   ]
  },
  {
   "cell_type": "code",
   "id": "a99e07a58363f3b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.782314Z",
     "start_time": "2026-01-04T11:15:14.779843Z"
    }
   },
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mekanizması, modelin önemli bilgilere odaklanmasını sağlar.\n",
    "    FORMÜL:\n",
    "    ───────\n",
    "    Attention(Q, K, V) = softmax(Q·Kᵀ / √d_k) · V\n",
    "\n",
    "    - Q (Query): \"Ne arıyorum?\"\n",
    "    - K (Key): \"Ben neyim?\"\n",
    "    - V (Value): \"Değerim ne?\"\n",
    "\n",
    "    √d_k ile bölme: Skorların çok büyümesini önler (softmax'ı stabilize eder)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,dropout:float=0.1):\n",
    "        \"\"\"\n",
    "        :param dropout: regularizasyon için dropout oranı\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention,self).__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        \"\"\"\n",
    "        :param query: [batch_size, num_heads, seq_len, d_k]\n",
    "        :param key: [batch_size, num_heads, seq_len, d_k]\n",
    "        :param value: [batch_size, num_heads, seq_len, d_k]\n",
    "        :param mask: opsiyonel maske tensorü\n",
    "\n",
    "        :return: output :Attention uygulanmış tensor\n",
    "        :return :attention_weights : dikkat ağırlıkları, her pozisyonun diğerlerine verdiği önem\n",
    "        \"\"\"\n",
    "        d_k = query.size(-1)  # d_k boyutu\n",
    "        # Q·Kᵀ hesapla\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # softmax uygula\n",
    "        attention_weights = self.dropout(attention_weights)  # dropout uygula\n",
    "\n",
    "        # softmax sonrası dropout yapınca, satırların toplamı 1 olmayabilir, normalize edelim\n",
    "        temp_attention_weights = attention_weights.sum(dim=-1, keepdim=True).clamp(min=1e-9)\n",
    "        attention_weights = attention_weights / temp_attention_weights\n",
    "\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output,attention_weights\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "f5d41870cebf59c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.788997Z",
     "start_time": "2026-01-04T11:15:14.785309Z"
    }
   },
   "source": [
    "print_test_heading(\"Scaled Dot-Product Attention Test\")\n",
    "attention = ScaledDotProductAttention(dropout=0.1)\n",
    "batch_size, num_heads, seq_len, d_k = 1, 1, 4, 64\n",
    "Q = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "K = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "V = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "output, attn_weights = attention(Q, K, V)\n",
    "\n",
    "print(f\"Query shape: {Q.shape}\")\n",
    "print(f\"Attention output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention weights (her kelime diğerlerine ne kadar bakıyor):\")\n",
    "print(attn_weights.squeeze())\n",
    "print(f\"\\nHer satırın toplamı (1 olmalı): {attn_weights.squeeze().sum(dim=-1)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Scaled Dot-Product Attention Test\n",
      "\n",
      "Query shape: torch.Size([1, 1, 4, 64])\n",
      "Attention output shape: torch.Size([1, 1, 4, 64])\n",
      "Attention weights shape: torch.Size([1, 1, 4, 4])\n",
      "\n",
      "Attention weights (her kelime diğerlerine ne kadar bakıyor):\n",
      "tensor([[0.0000, 0.8948, 0.1052, 0.0000],\n",
      "        [0.2312, 0.1593, 0.5557, 0.0538],\n",
      "        [0.0492, 0.6531, 0.1307, 0.1671],\n",
      "        [0.1505, 0.0000, 0.2174, 0.6321]])\n",
      "\n",
      "Her satırın toplamı (1 olmalı): tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "45fde59e510ae67a",
   "metadata": {},
   "source": [
    "### multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "id": "f3b14c722844326d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.796060Z",
     "start_time": "2026-01-04T11:15:14.793092Z"
    }
   },
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    birden fazla attention \"kafası\" ile aynı anda farklı bilgilere odaklanmayı sağlar.\n",
    "\n",
    "    Cümle: \"Bankaya gittim para çekmek için\"\n",
    "\n",
    "    Farklı head'ler farklı şeylere odaklanır:\n",
    "    - Head 1: Sözdizimi ilişkisi → \"gittim\" ← \"için\" (amaç ilişkisi)\n",
    "    - Head 2: Anlamsal ilişki → \"banka\" ← \"para\" (finans bağlamı)\n",
    "    - Head 3: Özne-fiil ilişkisi → \"gittim\" ← (gizli ben)\n",
    "    - Head 4: Nesne ilişkisi → \"çekmek\" ← \"para\"\n",
    "\n",
    "    Tek head tüm bu ilişkileri aynı anda yakalayamaz!\n",
    "    Multi-head ile paralel olarak farklı pattern'ler öğrenilir.\n",
    "\n",
    "    FORMÜL:\n",
    "    ───────\n",
    "    MultiHead(Q, K, V) = Concat(head_1, ..., head_h) · W_O\n",
    "    head_i = Attention(Q·W_Q_i, K·W_K_i, V·W_V_i)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,d_model:int,num_heads:int,dropout:float=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: model boyutu boyutu\n",
    "        :param num_heads: attention kafası sayısı\n",
    "        :param dropout: regularizasyon için dropout oranı\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0 , \"d_model, num_heads ile tam bölünmeli\"\n",
    "\n",
    "        self.d_model=d_model\n",
    "        self.num_heads=num_heads\n",
    "        self.d_k = d_model // num_heads # her head'in boyutu\n",
    "\n",
    "        # Q, K, V ve çıktı için lineer dönüşümler\n",
    "        self.W_q = nn.Linear(d_model,d_model)\n",
    "        self.W_k = nn.Linear(d_model,d_model)\n",
    "        self.W_v = nn.Linear(d_model,d_model)\n",
    "        self.W_o = nn.Linear(d_model,d_model)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        \"\"\"\n",
    "        :param query: [batch_size, seq_len, d_model]\n",
    "        :param key: [batch_size, seq_len, d_model]\n",
    "        :param value: [batch_size, seq_len, d_model]\n",
    "        :param mask: opsiyonel maske tensorü\n",
    "\n",
    "        :return: output :Multi-head attention uygulanmış tensor [batch_size, seq_len, d_model]\n",
    "        :return: attention_weights : dikkat ağırlıkları [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # linear projeksiyonlar\n",
    "        Q = self.W_q(query)  # (batch_size, seq_len, d_model)\n",
    "        K = self.W_k(key)    # (batch_size, seq_len, d_model)\n",
    "        V = self.W_v(value)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # headlere böl\n",
    "        Q = Q.reshape(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (batch, num_heads, seq_len, d_k)\n",
    "        K = K.reshape(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.reshape(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # attention uygula\n",
    "        attn_output, attention_weights = self.attention(Q, K, V, mask)  # (batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "        # headleri birleştir\n",
    "        # [batch, num_heads, seq_len, d_k] → [batch, seq_len, d_model]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # final lineer dönüşüm\n",
    "        output = self.W_o(attn_output)  # (batch_size, seq_len, d_model)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output, attention_weights"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "4da4e539debd5551",
   "metadata": {},
   "source": [
    "### position-wise feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "id": "4fc84fda53d938fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.807064Z",
     "start_time": "2026-01-04T11:15:14.804877Z"
    }
   },
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    her pozisyon için ayrı ayrı uygulanan iki katmanlı tam bağlantılı sinir ağı.\n",
    "    Attention -> hangi kelimeye bakmalıyım? sorusunu cevaplar\n",
    "    Feed-Forward -> o kelimeden ne öğrenmeliyim? sorusunu cevaplar\n",
    "\n",
    "    her kelime vektörü için:\n",
    "    - Genişletme (expansion): Boyutu artırarak daha karmaşık özellikler öğrenilir.\n",
    "    - Aktivasyon: Non-lineerlik eklenir (ReLU gibi).\n",
    "    - Daraltma (projection): Orijinal boyuta geri döner.\n",
    "\n",
    "    FFN(x) = ReLU(x·W₁ + b₁)·W₂ + b₂\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,d_ff,dropout:float=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: model boyutu\n",
    "        :param d_ff: feed-forward katmanının ara boyutu\n",
    "        :param dropout: regularizasyon için dropout oranı\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model,d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff,d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        :param x: input tensorü (batch_size, seq_len, d_model)\n",
    "        :return: output tensorü (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "8b8bc6f8cef22f5f",
   "metadata": {},
   "source": [
    "### layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5ddd1df33e9eb25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.812277Z",
     "start_time": "2026-01-04T11:15:14.810167Z"
    }
   },
   "source": [
    "class AddAndNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual bağlantı + layer normalization\n",
    "\n",
    "    Residual => vanishing gradient problemini azaltır ve eğitim stabilitesini artırır.\n",
    "    Layer Norm => her katmanda aktivasyonları normalize eder, eğitim hızını artırır.\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,dropout:float=0.1):\n",
    "        super(AddAndNorm,self).__init__()\n",
    "        self.layer_norm=nn.LayerNorm(d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,sublayer_output):\n",
    "        \"\"\"\n",
    "        :param x: orjinal girdi tensorü\n",
    "        :param sublayer_output: alt katman çıktısı tensorü (attention veya feed-forward)\n",
    "        :return: normalize edilmiş çıktı tensorü\n",
    "        \"\"\"\n",
    "        return self.layer_norm(x + self.dropout(sublayer_output))\n",
    "\n",
    "\n",
    "class PreNormAndNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-Layer Normalization + Residual bağlantı + Layer Normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,d_model,dropout:float=0.1):\n",
    "        super(PreNormAndNorm,self).__init__()\n",
    "        self.layer_norm=nn.LayerNorm(d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,sublayer):\n",
    "        \"\"\"\n",
    "        Pre-LN: Norm -> Sublayer -> Add\n",
    "        \"\"\"\n",
    "        normalized = self.layer_norm(x)\n",
    "        return x + self.dropout(sublayer(normalized))\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "9d7f0143237c6b8e",
   "metadata": {},
   "source": [
    "### encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9cea06fba28df21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.817183Z",
     "start_time": "2026-01-04T11:15:14.814879Z"
    }
   },
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    tek bir encoder katmanı : Self-attention + Feed-Forward\n",
    "\n",
    "    örnek : Kedi sütü içti.\n",
    "    Encoder layer -> kelimeler birbirlerine bakar..\n",
    "        içti -> kediye bakar (özne-fiil ilişkisi)\n",
    "        içti -> süt'e bakar (nesne ilişkisi)\n",
    "    sonra her kelime için ne öğrenmesi gerektiğine karar verir.\n",
    "    Feed forward -> her kelime için zengin temsil oluşturur\n",
    "        içti : kedi + süt + geçmiş zaman bilgisi\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,d_model:int,num_heads:int,d_ff:int,dropout:float=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: model boyutu\n",
    "        :param num_heads: attention kafası sayısı\n",
    "        :param d_ff: feed-forward katmanının ara boyutu\n",
    "        :param dropout: regularizasyon için dropout oranı\n",
    "        \"\"\"\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        # sublayer 1\n",
    "        self.self_attention=MultiHeadAttention(d_model,num_heads,dropout)\n",
    "        self.norm1=nn.LayerNorm(d_model)\n",
    "\n",
    "        #sublayer 2\n",
    "        self.feed_forward=PositionwiseFeedForward(d_model,d_ff,dropout)\n",
    "        self.norm2=nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,src_mask=None):\n",
    "        \"\"\"\n",
    "        :param x: input tensorü [batch_size, seq_len, d_model]\n",
    "        :param src_mask: opsiyonel kaynak maske tensorü\n",
    "\n",
    "        :return: output tensorü [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Sublayer 1: Self-Attention with residual and Layer Norm\n",
    "        attn_output, _ = self.self_attention(x, x, x, src_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Sublayer 2: Feed-Forward with residual and Layer Norm\n",
    "        ffn_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "4ec14bee1671860f",
   "metadata": {},
   "source": [
    "### decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec018dda18a47c9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.823761Z",
     "start_time": "2026-01-04T11:15:14.821005Z"
    }
   },
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    tek bir decoder katmanı : Masked Self-attention + Cross-Attention + Feed-Forward\n",
    "    örnek : Kedi sütü içti. -> The cat drank the milk.\n",
    "\n",
    "    Decoder \"the cat\" üretmiş, şimdi \"drank\" üretmeli.\n",
    "    - Masked Self-Attention -> sadece \"the\" ve \"cat\" kelimelerine bakabilir. milk e bakamaz, o gelecekte.\n",
    "    - Cross-Attention -> Türkçe encoder çıktısın bakar, \"içti\" kelimesine yüksek attention -> \"drank\"\n",
    "    - Feed-Forward -> tüm bilgiyi işle ve final temsil oluştur\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model:int,num_heads:int,d_ff:int,dropout:float=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # sublayer 1: masked multi-head self-attention\n",
    "        self.masked_self_attention=MultiHeadAttention(d_model,num_heads,dropout)\n",
    "        self.norm1=nn.LayerNorm(d_model)\n",
    "\n",
    "        # sublayer 2: multi-head cross-attention (encoder-decoder attention)\n",
    "        self.cross_attention=MultiHeadAttention(d_model,num_heads,dropout)\n",
    "        self.norm2=nn.LayerNorm(d_model)\n",
    "\n",
    "        #sublayer 3: position-wise feed-forward\n",
    "        self.feed_forward=PositionwiseFeedForward(d_model,d_ff,dropout)\n",
    "        self.norm3=nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,encoder_output, src_mask=None,tgt_mask=None):\n",
    "        \"\"\"\n",
    "        :param x: decoder input tensorü [batch_size, tgt_seq_len, d_model]\n",
    "        :param encoder_output: encoder çıktısı tensorü [batch_size, src_seq_len, d_model]\n",
    "        :param src_mask: source padding mask tensorü\n",
    "        :param tgt_mask: target casual mask tensorü (look-ahead + padding) gelecek kelimelere bakmamak için\n",
    "        :return: [batch_size, tgt_seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Sublayer 1: Masked Self-Attention\n",
    "        # Q=K=V=decoder input, mask ile gelecek görünmez\n",
    "        attn_output, _ = self.masked_self_attention(x, x,x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        #sublayer 2: Cross-Attention\n",
    "        # Q=decoder output, K=V=encoder output\n",
    "        attn_output, _ = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "\n",
    "        #sublayer 3: Feed-Forward\n",
    "        ffn_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ffn_output))\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "ae363979e1b13723",
   "metadata": {},
   "source": [
    "### full encoder"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e84fa9db594b52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.829523Z",
     "start_time": "2026-01-04T11:15:14.827139Z"
    }
   },
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    n tane encoder katmanından oluşan tam encoder.\n",
    "\n",
    "    örnek : bugün hava çok güzel\n",
    "    layer 1 :temel ilişkiler -> güzel -> hava (sıfat-isim ilişkisi)\n",
    "    layer 2: kompozit anlam : güzel hava -> kavram\n",
    "    layer 3: bağlam bilgisi : bugün güzel hava (zaman bilgisi, bugüne özgü durum)\n",
    "    layer 4-6: daha soyut ilişkiler ve anlamlar öğrenir. duygu, niyet vb.\n",
    "    daha fazla katman -> daha derin anlamlar öğrenme, daha fazla parametre, daha fazla hesaplama\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size:int,\n",
    "        d_model:int,\n",
    "        num_heads:int,\n",
    "        num_layers:int,\n",
    "        d_ff:int,\n",
    "        max_seq_len:int,\n",
    "        dropout:float=0.1\n",
    "    ):\n",
    "        super(Encoder,self).__init__()\n",
    "\n",
    "        # token embedding + positional encoding\n",
    "        self.token_embedding=TokenEmbedding(vocab_size,d_model)\n",
    "        self.positional_encoding=PositionalEncoding(d_model,max_seq_len,dropout)\n",
    "\n",
    "        # n adet encoder layer\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model,num_heads,d_ff,dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,src,src_mask=None):\n",
    "        \"\"\"\n",
    "        :param src: kaynak cümle tensorü [batch_size, src_seq_len] source token ids\n",
    "        :param src_mask: opsiyonel kaynak maske tensorü\n",
    "        :return: encoder çıktısı tensorü [batch_size, src_seq_len, d_model\n",
    "        \"\"\"\n",
    "        # token embedding\n",
    "        x = self.token_embedding(src)  # (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        # poisitional encoding\n",
    "        x = self.positional_encoding(x)  # pozisyon bilgisi ekle\n",
    "\n",
    "        # dropout uygula\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # n encoder layer'dan geçir\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        # final normalization\n",
    "        return self.norm(x)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "1cb685abb0ca2713",
   "metadata": {},
   "source": [
    "### decoder"
   ]
  },
  {
   "cell_type": "code",
   "id": "b85d820b8625ebad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.835736Z",
     "start_time": "2026-01-04T11:15:14.833273Z"
    }
   },
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    N adet decoder layer'dan oluşan tam decoder.\n",
    "\n",
    "    Encoder: \"Kedi süt içti\" → [encoded_context]\n",
    "\n",
    "    Decoder adım adım çeviri üretir:\n",
    "\n",
    "    Adım 1: <BOS> → \"The\"\n",
    "    Adım 2: <BOS> The → \"cat\"\n",
    "    Adım 3: <BOS> The cat → \"drank\"\n",
    "    Adım 4: <BOS> The cat drank → \"milk\"\n",
    "    Adım 5: <BOS> The cat drank milk → <EOS>\n",
    "\n",
    "    Her adımda:\n",
    "    1. Şu ana kadar üretilenlere bak (masked self-attention)\n",
    "    2. Encoder'a bak, Türkçe ne diyordu? (cross-attention)\n",
    "    3. Sonraki kelimeyi tahmin et\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size:int,\n",
    "            d_model:int,\n",
    "            num_heads:int,\n",
    "            num_layers:int,\n",
    "            d_ff:int,\n",
    "            max_seq_len:int,\n",
    "            dropout:float=0.1\n",
    "    ):\n",
    "        super(Decoder,self).__init__()\n",
    "\n",
    "        self.token_embedding=TokenEmbedding(vocab_size,d_model)\n",
    "        self.positional_encoding=PositionalEncoding(d_model,max_seq_len,dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model,num_heads,d_ff,dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,tgt,encoder_output, src_mask=None,tgt_mask=None):\n",
    "        \"\"\"\n",
    "        :param tgt: hedef cümle tensorü [batch_size, tgt_seq_len] target token ids\n",
    "        :param encoder_output: encoder çıktısı tensorü [batch_size, src_seq_len,\n",
    "        d_model]\n",
    "        :param src_mask: kaynak maske tensorü\n",
    "        :param tgt_mask: hedef maske tensorü\n",
    "        :return: decoder çıktısı tensorü [batch_size, tgt_seq_len, d_model]\n",
    "        \"\"\"\n",
    "        x = self.token_embedding(tgt)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # dropout uygula\n",
    "        x = self.dropout(x)\n",
    "\n",
    "         # n decoder layer'dan geçir\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,encoder_output,src_mask,tgt_mask)\n",
    "\n",
    "        return self.norm(x)\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "78ec02bf64eb67c1",
   "metadata": {},
   "source": [
    "### complete transformer model"
   ]
  },
  {
   "cell_type": "code",
   "id": "234cdac829d3f44e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.842300Z",
     "start_time": "2026-01-04T11:15:14.838875Z"
    }
   },
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Türkçe-İngilizce çeviri için tam Transformer modeli.\n",
    "    Akış:\n",
    "    Cümle : Merhaba dünya\n",
    "    Tokenizer - > Token IDs : [45, 678, 23]\n",
    "    Encoder -> Türkçenin zengin temsili\n",
    "    Decoder -> <BOS> -> Hello -> world -> <EOS>\n",
    "    Cümle : Hello world\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            src_vocab_size:int, #türkçe sözlük boyutu\n",
    "            tgt_vocab_size:int, #ingilizce sözlük boyutu\n",
    "            d_model:int, #model boyutu\n",
    "            num_heads:int, #attention kafası sayısı\n",
    "            num_encoder_layers:int, #encoder katman sayısı\n",
    "            num_decoder_layers:int, #decoder katman sayısı\n",
    "            d_ff:int, #feed-forward ara boyutu\n",
    "            max_seq_len:int, # max cümle uzunluğu\n",
    "            dropout:float=0.1,\n",
    "            padd_idx:int=0 #padding token id\n",
    "    ):\n",
    "        super(Transformer,self).__init__()\n",
    "\n",
    "        self.pad_idx=padd_idx\n",
    "        self.d_model=d_model\n",
    "\n",
    "        #encoder\n",
    "        self.encoder=Encoder(\n",
    "            src_vocab_size,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            d_ff,\n",
    "            max_seq_len,\n",
    "            dropout\n",
    "        )\n",
    "\n",
    "        #decoder\n",
    "        self.decoder=Decoder(\n",
    "            tgt_vocab_size,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            num_decoder_layers,\n",
    "            d_ff,\n",
    "            max_seq_len,\n",
    "            dropout\n",
    "        )\n",
    "\n",
    "        # final output projeksiyonu : d_model -> tgt_vocab_size\n",
    "        # her pozisyon için kelime olasılıkları\n",
    "        self.output_projections=nn.Linear(d_model,tgt_vocab_size)\n",
    "\n",
    "        self._init_parameters()\n",
    "\n",
    "    def _init_parameters(self):\n",
    "        \"\"\"\n",
    "        model parametrelerini Xavier uniform ile başlat\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim()>1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def make_src_mask(self,src):\n",
    "        \"\"\"\n",
    "        kaynak cümle için padding maskesi oluştur\n",
    "        :param src: kaynak cümle tensorü [batch_size, src_seq_len]\n",
    "        :return: src_mask: [batch_size, 1, 1, src_seq_len]\n",
    "        \"\"\"\n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask  # (batch_size, 1, 1, src_seq_len)\n",
    "\n",
    "    def make_tgt_mask(self,tgt):\n",
    "        \"\"\"\n",
    "        hedef cümle için casual (look-ahead + padding) maske oluştur\n",
    "        :param tgt: hedef cümle tensorü [batch_size, tgt_seq_len]\n",
    "        :return: tgt_mask: [batch_size, 1, tgt_seq_len, tgt_seq_len]\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "\n",
    "        # Padding mask\n",
    "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # [batch, 1, 1, tgt_len]\n",
    "\n",
    "        # Causal mask (üst üçgen sıfır)\n",
    "        causal_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=tgt.device))\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)\n",
    "        # [1, 1, tgt_len, tgt_len]\n",
    "\n",
    "        # İkisini birleştir\n",
    "        tgt_mask = tgt_pad_mask & causal_mask.bool()\n",
    "        return tgt_mask\n",
    "\n",
    "    def forward(self,src,tgt):\n",
    "        \"\"\"\n",
    "        :param src: en - kaynak cümle tensorü [batch_size, src_seq_len]\n",
    "        :param tgt: tr - hedef cümle tensorü [batch_size, tgt_seq_len]\n",
    "        :return: output logits tensorü [batch_size, tgt_seq_len, tgt_vocab_size]\n",
    "        \"\"\"\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "\n",
    "        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        logits = self.output_projections(decoder_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def encode(self,src,src_mask=None):\n",
    "        if src_mask is None:\n",
    "            src_mask = self.make_src_mask(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, tgt, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = self.make_tgt_mask(tgt)\n",
    "        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "        return self.output_projections(decoder_output)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "ba5a2d717a1fb05c",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "id": "c48b965bccd48ca8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:14.847445Z",
     "start_time": "2026-01-04T11:15:14.845361Z"
    }
   },
   "source": [
    "def read_corpus_lines(corpus_path, encoding='utf-8', strip=True, skip_empty=True):\n",
    "    \"\"\"\n",
    "    Dosyayı satır satır okur ve temizlenmiş satırları liste olarak döner.\n",
    "    Args:\n",
    "      - corpus_path: path string (ör. `data/train_target.txt`)\n",
    "      - encoding: dosya encoding'i\n",
    "      - strip: her satırı strip() ile temizle\n",
    "      - skip_empty: boş satırları atla\n",
    "    Returns:\n",
    "      - list of str\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    with open(corpus_path, 'r', encoding=encoding) as f:\n",
    "        for ln in f:\n",
    "            s = ln.rstrip('\\n')\n",
    "            if strip:\n",
    "                s = s.strip()\n",
    "            if skip_empty and not s:\n",
    "                continue\n",
    "            lines.append(s)\n",
    "    return lines\n",
    "\n",
    "def get_corpus_and_tokenizers():\n",
    "    \"\"\"\n",
    "    Eğitim corpuslarını ve tokenizer'ları yükle.\n",
    "    Returns:\n",
    "        tr_corpus: Türkçe eğitim corpus dosyası\n",
    "        en_corpus: İngilizce eğitim corpus dosyası\n",
    "        tr_tokenizer: Türkçe SentencePiece tokenizer\n",
    "        en_tokenizer: İngilizce SentencePiece tokenizer\n",
    "    \"\"\"\n",
    "    tr_corpus = \"data/train_target.txt\"\n",
    "    en_corpus = \"data/train_source.txt\"\n",
    "\n",
    "    tr_tokenizer = SentencePieceTokenizer('tokenizers/tr_bpe_32k.model')\n",
    "    en_tokenizer = SentencePieceTokenizer('tokenizers/en_bpe_32k.model')\n",
    "\n",
    "    return tr_corpus, en_corpus, tr_tokenizer, en_tokenizer"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "88994dfa4a0ef524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:15.510322Z",
     "start_time": "2026-01-04T11:15:14.850092Z"
    }
   },
   "source": [
    "tr_corpus, en_corpus, tr_tokenizer, en_tokenizer = get_corpus_and_tokenizers()\n",
    "\n",
    "tr_sentences = read_corpus_lines(tr_corpus)\n",
    "en_sentences = read_corpus_lines(en_corpus)\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "c5e9385a7d26554f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:15.516008Z",
     "start_time": "2026-01-04T11:15:15.514333Z"
    }
   },
   "source": [
    "print(tr_sentences[0])\n",
    "print(en_sentences[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Çalışmalarının çoğu N.C. Eğitim Araştırma Veri Merkezine dayanmaktadır.\n",
      "Many of their studies rely on the N.C. Education Research Data Center.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "5e3e125e19d32b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:15.522069Z",
     "start_time": "2026-01-04T11:15:15.520243Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "def create_dataloader(src_encoded,tgt_encoded,batch_size=16,val_split=0.1):\n",
    "    dataset = TensorDataset(src_encoded,tgt_encoded)\n",
    "    val_size = int(len(dataset)*val_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset,[train_size,val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
    "    return train_loader, val_loader"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "b7682e4bd6515c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:15.531836Z",
     "start_time": "2026-01-04T11:15:15.529413Z"
    }
   },
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "class TranslationDataSet(Dataset):\n",
    "    \"\"\"\n",
    "    çeviri veri seti için özel Dataset sınıfı.\n",
    "    \"\"\"\n",
    "    def __init__(self,src_sentences,tgt_sentences,src_tokenizer,tgt_tokenizer,max_len=max_seq_len):\n",
    "        \"\"\"\n",
    "        :param src_sentences: kaynak cümleler listesi\n",
    "        :param tgt_sentences: hedef cümleler listesi\n",
    "        :param src_tokenizer: kaynak tokenizer\n",
    "        :param tgt_tokenizer: hedef tokenizer\n",
    "        :param max_len: max cümle uzunluğu\n",
    "        \"\"\"\n",
    "        self.src_sentences=src_sentences\n",
    "        self.tgt_sentences=tgt_sentences\n",
    "        self.src_tokenizer=src_tokenizer\n",
    "        self.tgt_tokenizer=tgt_tokenizer\n",
    "        self.max_len=max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        src = self.src_tokenizer.encode(self.src_sentences[idx])\n",
    "        tgt = self.tgt_tokenizer.encode(self.tgt_sentences[idx])\n",
    "\n",
    "        # max_len'e göre padding/truncation\n",
    "        src = src[:self.max_len]\n",
    "        src = src + [0] * max(0, self.max_len - len(src))\n",
    "\n",
    "        tgt = tgt[:self.max_len]\n",
    "        tgt = tgt + [0] * max(0, self.max_len - len(tgt))\n",
    "\n",
    "        return torch.tensor(src , dtype=torch.long), torch.tensor(tgt, dtype=torch.long)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "69a9fa1352139376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:15.537664Z",
     "start_time": "2026-01-04T11:15:15.535408Z"
    }
   },
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Label Smoothing: Overconfidence'ı önler.\n",
    "\n",
    "    Normal: target = [0, 0, 1, 0, 0] (kesin \"cat\")\n",
    "    Smoothed: target = [0.02, 0.02, 0.92, 0.02, 0.02]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, pad_idx: int = 0, smoothing: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_idx = pad_idx\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1.0 - smoothing\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: [batch * seq_len, vocab_size]\n",
    "            target: [batch * seq_len]\n",
    "        \"\"\"\n",
    "        logits = logits.reshape(-1, self.vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "\n",
    "        # Smooth distribution\n",
    "        smooth_target = torch.zeros_like(logits)\n",
    "        smooth_target.fill_(self.smoothing / (self.vocab_size - 2))\n",
    "        smooth_target.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "        smooth_target[:, self.pad_idx] = 0\n",
    "\n",
    "        # Padding mask\n",
    "        mask = (target != self.pad_idx)\n",
    "\n",
    "        # Cross entropy with smooth targets\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        loss = -smooth_target * log_probs\n",
    "        loss = loss.sum(dim=-1)\n",
    "        loss = loss.masked_select(mask).mean()\n",
    "\n",
    "        return loss"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "af8d96aa2c4bb42b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T11:15:15.541902Z",
     "start_time": "2026-01-04T11:15:15.539829Z"
    }
   },
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"Bir epoch eğitim.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, tgt) in enumerate(dataloader):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        # print(\"SHAPE\",src.shape, tgt.shape)\n",
    "\n",
    "        # Decoder input: <BOS> + target[:-1]\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        # Decoder target: target[1:] + <EOS>\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, tgt_input)\n",
    "\n",
    "        # Loss hesapla\n",
    "        loss = criterion(logits, tgt_output)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % batch_size == 0:\n",
    "            print(f\"  Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "f8c6955616272880",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T15:20:46.092547Z",
     "start_time": "2026-01-04T11:15:15.544599Z"
    }
   },
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "print_test_heading(\"Training Loop\")\n",
    "\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_len=max_seq_len,\n",
    "    dropout=0.1,\n",
    "    padd_idx=0\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "criterion = LabelSmoothingLoss(vocab_size=vocab_size, pad_idx=0, smoothing=0.1)\n",
    "\n",
    "print(f\"\\nModel hazır!\")\n",
    "print(f\"Parametre sayısı: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "dataset=TranslationDataSet(en_sentences,tr_sentences,en_tokenizer,tr_tokenizer)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_epoch(model, dataloader, optimizer, criterion, device)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "end = datetime.now()\n",
    "print(f\"\\nEğitim süresi: {(end - start).total_seconds() / 60} dakika\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Training Loop\n",
      "\n",
      "\n",
      "Model hazır!\n",
      "Parametre sayısı: 17,979,168\n",
      "  Batch 0, Loss: 6.8854\n",
      "  Batch 32, Loss: 6.2551\n",
      "  Batch 64, Loss: 6.1708\n",
      "  Batch 96, Loss: 6.2009\n",
      "  Batch 128, Loss: 6.1361\n",
      "  Batch 160, Loss: 6.1414\n",
      "  Batch 192, Loss: 6.1097\n",
      "  Batch 224, Loss: 6.0539\n",
      "  Batch 256, Loss: 6.0825\n",
      "  Batch 288, Loss: 5.9839\n",
      "  Batch 320, Loss: 6.0479\n",
      "  Batch 352, Loss: 6.0054\n",
      "  Batch 384, Loss: 6.0168\n",
      "  Batch 416, Loss: 5.9650\n",
      "  Batch 448, Loss: 6.0141\n",
      "  Batch 480, Loss: 5.9458\n",
      "  Batch 512, Loss: 5.9948\n",
      "  Batch 544, Loss: 5.8750\n",
      "  Batch 576, Loss: 6.0091\n",
      "  Batch 608, Loss: 5.9095\n",
      "  Batch 640, Loss: 5.8913\n",
      "  Batch 672, Loss: 5.8207\n",
      "  Batch 704, Loss: 5.7467\n",
      "  Batch 736, Loss: 5.7972\n",
      "  Batch 768, Loss: 5.7188\n",
      "  Batch 800, Loss: 5.6573\n",
      "  Batch 832, Loss: 5.6884\n",
      "  Batch 864, Loss: 5.6803\n",
      "  Batch 896, Loss: 5.5872\n",
      "  Batch 928, Loss: 5.6045\n",
      "  Batch 960, Loss: 5.5448\n",
      "  Batch 992, Loss: 5.4936\n",
      "  Batch 1024, Loss: 5.4738\n",
      "  Batch 1056, Loss: 5.5111\n",
      "  Batch 1088, Loss: 5.4326\n",
      "  Batch 1120, Loss: 5.4255\n",
      "  Batch 1152, Loss: 5.3603\n",
      "  Batch 1184, Loss: 5.3703\n",
      "  Batch 1216, Loss: 5.3656\n",
      "  Batch 1248, Loss: 5.2846\n",
      "  Batch 1280, Loss: 5.3579\n",
      "  Batch 1312, Loss: 5.2987\n",
      "  Batch 1344, Loss: 5.2415\n",
      "  Batch 1376, Loss: 5.2338\n",
      "  Batch 1408, Loss: 5.1698\n",
      "  Batch 1440, Loss: 5.2030\n",
      "  Batch 1472, Loss: 5.1414\n",
      "  Batch 1504, Loss: 5.0761\n",
      "  Batch 1536, Loss: 5.1184\n",
      "  Batch 1568, Loss: 5.1131\n",
      "  Batch 1600, Loss: 5.0208\n",
      "  Batch 1632, Loss: 4.9863\n",
      "  Batch 1664, Loss: 5.0321\n",
      "  Batch 1696, Loss: 5.0013\n",
      "  Batch 1728, Loss: 4.9662\n",
      "  Batch 1760, Loss: 4.9305\n",
      "  Batch 1792, Loss: 4.9363\n",
      "  Batch 1824, Loss: 4.8293\n",
      "  Batch 1856, Loss: 4.8441\n",
      "  Batch 1888, Loss: 4.8588\n",
      "  Batch 1920, Loss: 4.8204\n",
      "  Batch 1952, Loss: 4.8432\n",
      "  Batch 1984, Loss: 4.8152\n",
      "  Batch 2016, Loss: 4.8304\n",
      "  Batch 2048, Loss: 4.8175\n",
      "  Batch 2080, Loss: 4.7595\n",
      "  Batch 2112, Loss: 4.7568\n",
      "  Batch 2144, Loss: 4.8264\n",
      "  Batch 2176, Loss: 4.7578\n",
      "  Batch 2208, Loss: 4.6600\n",
      "  Batch 2240, Loss: 4.7625\n",
      "  Batch 2272, Loss: 4.6791\n",
      "  Batch 2304, Loss: 4.7966\n",
      "  Batch 2336, Loss: 4.6815\n",
      "  Batch 2368, Loss: 4.7136\n",
      "  Batch 2400, Loss: 4.6545\n",
      "  Batch 2432, Loss: 4.6451\n",
      "  Batch 2464, Loss: 4.6104\n",
      "  Batch 2496, Loss: 4.6713\n",
      "  Batch 2528, Loss: 4.6470\n",
      "  Batch 2560, Loss: 4.6047\n",
      "  Batch 2592, Loss: 4.5937\n",
      "  Batch 2624, Loss: 4.5742\n",
      "  Batch 2656, Loss: 4.4957\n",
      "  Batch 2688, Loss: 4.5476\n",
      "  Batch 2720, Loss: 4.5687\n",
      "  Batch 2752, Loss: 4.6026\n",
      "  Batch 2784, Loss: 4.5828\n",
      "  Batch 2816, Loss: 4.5499\n",
      "  Batch 2848, Loss: 4.6419\n",
      "  Batch 2880, Loss: 4.4969\n",
      "  Batch 2912, Loss: 4.6215\n",
      "  Batch 2944, Loss: 4.5385\n",
      "  Batch 2976, Loss: 4.5532\n",
      "  Batch 3008, Loss: 4.6104\n",
      "  Batch 3040, Loss: 4.5201\n",
      "  Batch 3072, Loss: 4.5549\n",
      "  Batch 3104, Loss: 4.4423\n",
      "  Batch 3136, Loss: 4.4859\n",
      "  Batch 3168, Loss: 4.3844\n",
      "  Batch 3200, Loss: 4.5289\n",
      "  Batch 3232, Loss: 4.5294\n",
      "  Batch 3264, Loss: 4.4459\n",
      "  Batch 3296, Loss: 4.4672\n",
      "  Batch 3328, Loss: 4.3619\n",
      "  Batch 3360, Loss: 4.4653\n",
      "  Batch 3392, Loss: 4.3901\n",
      "  Batch 3424, Loss: 4.4446\n",
      "  Batch 3456, Loss: 4.4660\n",
      "  Batch 3488, Loss: 4.5353\n",
      "  Batch 3520, Loss: 4.2881\n",
      "  Batch 3552, Loss: 4.4115\n",
      "  Batch 3584, Loss: 4.3095\n",
      "  Batch 3616, Loss: 4.3991\n",
      "  Batch 3648, Loss: 4.4102\n",
      "  Batch 3680, Loss: 4.3841\n",
      "  Batch 3712, Loss: 4.5370\n",
      "  Batch 3744, Loss: 4.3871\n",
      "  Batch 3776, Loss: 4.3306\n",
      "  Batch 3808, Loss: 4.3190\n",
      "  Batch 3840, Loss: 4.3368\n",
      "  Batch 3872, Loss: 4.3096\n",
      "  Batch 3904, Loss: 4.2470\n",
      "  Batch 3936, Loss: 4.4042\n",
      "  Batch 3968, Loss: 4.3227\n",
      "  Batch 4000, Loss: 4.3367\n",
      "  Batch 4032, Loss: 4.2491\n",
      "  Batch 4064, Loss: 4.3672\n",
      "  Batch 4096, Loss: 4.3616\n",
      "  Batch 4128, Loss: 4.2921\n",
      "  Batch 4160, Loss: 4.3566\n",
      "  Batch 4192, Loss: 4.3739\n",
      "  Batch 4224, Loss: 4.4235\n",
      "  Batch 4256, Loss: 4.3340\n",
      "  Batch 4288, Loss: 4.2559\n",
      "  Batch 4320, Loss: 4.2508\n",
      "  Batch 4352, Loss: 4.1727\n",
      "  Batch 4384, Loss: 4.3600\n",
      "  Batch 4416, Loss: 4.2538\n",
      "  Batch 4448, Loss: 4.2863\n",
      "  Batch 4480, Loss: 4.1907\n",
      "  Batch 4512, Loss: 4.3204\n",
      "  Batch 4544, Loss: 4.2904\n",
      "  Batch 4576, Loss: 4.3554\n",
      "  Batch 4608, Loss: 4.2772\n",
      "  Batch 4640, Loss: 4.3367\n",
      "  Batch 4672, Loss: 4.2670\n",
      "  Batch 4704, Loss: 4.1714\n",
      "  Batch 4736, Loss: 4.2254\n",
      "  Batch 4768, Loss: 4.1863\n",
      "  Batch 4800, Loss: 4.2434\n",
      "  Batch 4832, Loss: 4.2199\n",
      "  Batch 4864, Loss: 4.3316\n",
      "  Batch 4896, Loss: 4.2756\n",
      "  Batch 4928, Loss: 4.2815\n",
      "  Batch 4960, Loss: 4.2359\n",
      "  Batch 4992, Loss: 4.2826\n",
      "  Batch 5024, Loss: 4.2557\n",
      "  Batch 5056, Loss: 4.2316\n",
      "  Batch 5088, Loss: 4.3209\n",
      "  Batch 5120, Loss: 4.2361\n",
      "  Batch 5152, Loss: 4.1497\n",
      "  Batch 5184, Loss: 4.1240\n",
      "  Batch 5216, Loss: 4.2742\n",
      "  Batch 5248, Loss: 4.1082\n",
      "  Batch 5280, Loss: 4.1214\n",
      "  Batch 5312, Loss: 4.1579\n",
      "  Batch 5344, Loss: 4.1534\n",
      "  Batch 5376, Loss: 4.1933\n",
      "  Batch 5408, Loss: 4.1008\n",
      "  Batch 5440, Loss: 4.3172\n",
      "  Batch 5472, Loss: 4.1871\n",
      "  Batch 5504, Loss: 4.1567\n",
      "  Batch 5536, Loss: 4.1694\n",
      "  Batch 5568, Loss: 4.1363\n",
      "  Batch 5600, Loss: 4.1622\n",
      "  Batch 5632, Loss: 4.0946\n",
      "  Batch 5664, Loss: 4.2042\n",
      "  Batch 5696, Loss: 4.0527\n",
      "  Batch 5728, Loss: 4.1563\n",
      "  Batch 5760, Loss: 4.1227\n",
      "  Batch 5792, Loss: 4.2034\n",
      "  Batch 5824, Loss: 4.1456\n",
      "  Batch 5856, Loss: 4.0527\n",
      "  Batch 5888, Loss: 4.2557\n",
      "  Batch 5920, Loss: 4.0446\n",
      "  Batch 5952, Loss: 4.1749\n",
      "  Batch 5984, Loss: 4.1995\n",
      "  Batch 6016, Loss: 4.0756\n",
      "  Batch 6048, Loss: 4.0550\n",
      "  Batch 6080, Loss: 4.0338\n",
      "  Batch 6112, Loss: 4.1051\n",
      "  Batch 6144, Loss: 4.2099\n",
      "  Batch 6176, Loss: 4.0859\n",
      "  Batch 6208, Loss: 4.0388\n",
      "  Batch 6240, Loss: 4.2619\n",
      "  Batch 6272, Loss: 3.9686\n",
      "  Batch 6304, Loss: 4.2505\n",
      "  Batch 6336, Loss: 4.0691\n",
      "  Batch 6368, Loss: 4.1741\n",
      "  Batch 6400, Loss: 4.0698\n",
      "  Batch 6432, Loss: 3.9553\n",
      "  Batch 6464, Loss: 4.0822\n",
      "  Batch 6496, Loss: 4.0427\n",
      "  Batch 6528, Loss: 4.0307\n",
      "  Batch 6560, Loss: 4.0314\n",
      "  Batch 6592, Loss: 4.1517\n",
      "  Batch 6624, Loss: 4.0875\n",
      "  Batch 6656, Loss: 3.9819\n",
      "  Batch 6688, Loss: 4.0860\n",
      "  Batch 6720, Loss: 4.1299\n",
      "  Batch 6752, Loss: 3.9404\n",
      "  Batch 6784, Loss: 4.1390\n",
      "  Batch 6816, Loss: 4.0467\n",
      "  Batch 6848, Loss: 4.0150\n",
      "  Batch 6880, Loss: 3.9385\n",
      "  Batch 6912, Loss: 4.1731\n",
      "  Batch 6944, Loss: 3.9348\n",
      "  Batch 6976, Loss: 4.1361\n",
      "  Batch 7008, Loss: 4.1167\n",
      "  Batch 7040, Loss: 4.0892\n",
      "  Batch 7072, Loss: 4.0534\n",
      "  Batch 7104, Loss: 4.0787\n",
      "  Batch 7136, Loss: 4.0409\n",
      "  Batch 7168, Loss: 3.9646\n",
      "  Batch 7200, Loss: 3.8810\n",
      "  Batch 7232, Loss: 4.1272\n",
      "  Batch 7264, Loss: 4.0649\n",
      "  Batch 7296, Loss: 4.0372\n",
      "  Batch 7328, Loss: 4.0657\n",
      "  Batch 7360, Loss: 3.9756\n",
      "  Batch 7392, Loss: 3.9623\n",
      "  Batch 7424, Loss: 3.9526\n",
      "  Batch 7456, Loss: 3.9256\n",
      "  Batch 7488, Loss: 4.0306\n",
      "  Batch 7520, Loss: 3.8901\n",
      "  Batch 7552, Loss: 3.9561\n",
      "  Batch 7584, Loss: 4.0217\n",
      "  Batch 7616, Loss: 3.9929\n",
      "  Batch 7648, Loss: 4.1316\n",
      "  Batch 7680, Loss: 3.9043\n",
      "  Batch 7712, Loss: 3.9610\n",
      "  Batch 7744, Loss: 3.9355\n",
      "  Batch 7776, Loss: 3.8965\n",
      "  Batch 7808, Loss: 4.0546\n",
      "  Batch 7840, Loss: 4.0319\n",
      "  Batch 7872, Loss: 3.9778\n",
      "  Batch 7904, Loss: 3.9019\n",
      "  Batch 7936, Loss: 3.9922\n",
      "  Batch 7968, Loss: 4.0210\n",
      "  Batch 8000, Loss: 4.0245\n",
      "  Batch 8032, Loss: 3.8086\n",
      "  Batch 8064, Loss: 3.9910\n",
      "  Batch 8096, Loss: 3.9689\n",
      "  Batch 8128, Loss: 3.9004\n",
      "  Batch 8160, Loss: 3.9715\n",
      "  Batch 8192, Loss: 3.8486\n",
      "  Batch 8224, Loss: 3.9877\n",
      "  Batch 8256, Loss: 4.0075\n",
      "  Batch 8288, Loss: 3.9995\n",
      "  Batch 8320, Loss: 4.0056\n",
      "  Batch 8352, Loss: 4.0157\n",
      "  Batch 8384, Loss: 3.9217\n",
      "  Batch 8416, Loss: 4.0179\n",
      "  Batch 8448, Loss: 4.1082\n",
      "  Batch 8480, Loss: 4.0466\n",
      "  Batch 8512, Loss: 3.9597\n",
      "  Batch 8544, Loss: 3.9614\n",
      "  Batch 8576, Loss: 3.9932\n",
      "  Batch 8608, Loss: 3.9577\n",
      "  Batch 8640, Loss: 3.8958\n",
      "  Batch 8672, Loss: 3.9387\n",
      "  Batch 8704, Loss: 3.8002\n",
      "  Batch 8736, Loss: 3.9404\n",
      "  Batch 8768, Loss: 3.8400\n",
      "  Batch 8800, Loss: 3.9414\n",
      "  Batch 8832, Loss: 3.8082\n",
      "  Batch 8864, Loss: 3.9918\n",
      "  Batch 8896, Loss: 3.8841\n",
      "  Batch 8928, Loss: 3.9631\n",
      "  Batch 8960, Loss: 3.7573\n",
      "  Batch 8992, Loss: 3.8247\n",
      "  Batch 9024, Loss: 3.8882\n",
      "  Batch 9056, Loss: 3.9452\n",
      "  Batch 9088, Loss: 3.8391\n",
      "  Batch 9120, Loss: 3.8909\n",
      "  Batch 9152, Loss: 3.9763\n",
      "  Batch 9184, Loss: 3.9015\n",
      "  Batch 9216, Loss: 3.9849\n",
      "  Batch 9248, Loss: 3.8737\n",
      "  Batch 9280, Loss: 3.8249\n",
      "  Batch 9312, Loss: 3.9689\n",
      "  Batch 9344, Loss: 4.0274\n",
      "  Batch 9376, Loss: 3.7671\n",
      "  Batch 9408, Loss: 3.8732\n",
      "  Batch 9440, Loss: 3.8265\n",
      "  Batch 9472, Loss: 3.8544\n",
      "  Batch 9504, Loss: 3.9408\n",
      "  Batch 9536, Loss: 3.8527\n",
      "  Batch 9568, Loss: 3.8782\n",
      "  Batch 9600, Loss: 3.7958\n",
      "  Batch 9632, Loss: 3.8974\n",
      "  Batch 9664, Loss: 3.8623\n",
      "  Batch 9696, Loss: 3.9647\n",
      "  Batch 9728, Loss: 3.8805\n",
      "  Batch 9760, Loss: 3.9015\n",
      "  Batch 9792, Loss: 3.8378\n",
      "  Batch 9824, Loss: 3.8723\n",
      "  Batch 9856, Loss: 3.8923\n",
      "  Batch 9888, Loss: 3.8881\n",
      "  Batch 9920, Loss: 3.8403\n",
      "  Batch 9952, Loss: 3.8270\n",
      "  Batch 9984, Loss: 3.8259\n",
      "  Batch 10016, Loss: 3.8945\n",
      "  Batch 10048, Loss: 3.8646\n",
      "  Batch 10080, Loss: 3.8435\n",
      "  Batch 10112, Loss: 3.9319\n",
      "  Batch 10144, Loss: 3.8533\n",
      "  Batch 10176, Loss: 3.8957\n",
      "  Batch 10208, Loss: 3.9681\n",
      "  Batch 10240, Loss: 3.7045\n",
      "  Batch 10272, Loss: 3.8548\n",
      "  Batch 10304, Loss: 3.8186\n",
      "  Batch 10336, Loss: 3.8704\n",
      "  Batch 10368, Loss: 3.8511\n",
      "  Batch 10400, Loss: 3.8033\n",
      "  Batch 10432, Loss: 3.8800\n",
      "  Batch 10464, Loss: 3.8710\n",
      "  Batch 10496, Loss: 3.8252\n",
      "  Batch 10528, Loss: 3.8844\n",
      "  Batch 10560, Loss: 3.6376\n",
      "  Batch 10592, Loss: 3.8056\n",
      "  Batch 10624, Loss: 3.8199\n",
      "  Batch 10656, Loss: 3.8509\n",
      "  Batch 10688, Loss: 3.7727\n",
      "  Batch 10720, Loss: 3.9574\n",
      "  Batch 10752, Loss: 3.8063\n",
      "  Batch 10784, Loss: 3.9370\n",
      "  Batch 10816, Loss: 3.8852\n",
      "  Batch 10848, Loss: 3.7375\n",
      "  Batch 10880, Loss: 3.8260\n",
      "  Batch 10912, Loss: 3.9441\n",
      "  Batch 10944, Loss: 3.8229\n",
      "  Batch 10976, Loss: 3.6650\n",
      "  Batch 11008, Loss: 3.9011\n",
      "  Batch 11040, Loss: 3.8213\n",
      "  Batch 11072, Loss: 3.8783\n",
      "  Batch 11104, Loss: 3.7611\n",
      "  Batch 11136, Loss: 3.8299\n",
      "  Batch 11168, Loss: 3.6674\n",
      "  Batch 11200, Loss: 3.8130\n",
      "  Batch 11232, Loss: 3.7371\n",
      "  Batch 11264, Loss: 3.9214\n",
      "  Batch 11296, Loss: 3.8062\n",
      "  Batch 11328, Loss: 3.7684\n",
      "  Batch 11360, Loss: 3.8887\n",
      "  Batch 11392, Loss: 3.8099\n",
      "  Batch 11424, Loss: 3.9099\n",
      "  Batch 11456, Loss: 3.8707\n",
      "  Batch 11488, Loss: 3.7974\n",
      "  Batch 11520, Loss: 3.6785\n",
      "  Batch 11552, Loss: 3.8983\n",
      "  Batch 11584, Loss: 3.7446\n",
      "  Batch 11616, Loss: 3.8781\n",
      "  Batch 11648, Loss: 3.8896\n",
      "  Batch 11680, Loss: 3.8660\n",
      "  Batch 11712, Loss: 3.7933\n",
      "  Batch 11744, Loss: 3.7772\n",
      "  Batch 11776, Loss: 3.8325\n",
      "  Batch 11808, Loss: 3.8561\n",
      "  Batch 11840, Loss: 3.7957\n",
      "  Batch 11872, Loss: 3.8095\n",
      "  Batch 11904, Loss: 3.8295\n",
      "  Batch 11936, Loss: 3.8359\n",
      "  Batch 11968, Loss: 3.7586\n",
      "  Batch 12000, Loss: 3.7230\n",
      "  Batch 12032, Loss: 3.7313\n",
      "  Batch 12064, Loss: 3.7592\n",
      "  Batch 12096, Loss: 3.6979\n",
      "  Batch 12128, Loss: 3.8787\n",
      "  Batch 12160, Loss: 3.7621\n",
      "  Batch 12192, Loss: 3.7735\n",
      "  Batch 12224, Loss: 3.7526\n",
      "  Batch 12256, Loss: 3.8087\n",
      "  Batch 12288, Loss: 3.8687\n",
      "  Batch 12320, Loss: 3.8876\n",
      "  Batch 12352, Loss: 3.7049\n",
      "  Batch 12384, Loss: 3.8243\n",
      "  Batch 12416, Loss: 3.9146\n",
      "  Batch 12448, Loss: 3.7213\n",
      "  Batch 12480, Loss: 3.7466\n",
      "  Batch 12512, Loss: 3.7782\n",
      "  Batch 12544, Loss: 3.6215\n",
      "  Batch 12576, Loss: 3.8403\n",
      "  Batch 12608, Loss: 3.8019\n",
      "  Batch 12640, Loss: 3.6422\n",
      "  Batch 12672, Loss: 3.8949\n",
      "  Batch 12704, Loss: 3.7977\n",
      "  Batch 12736, Loss: 3.8206\n",
      "  Batch 12768, Loss: 3.8186\n",
      "  Batch 12800, Loss: 3.7755\n",
      "  Batch 12832, Loss: 3.8194\n",
      "  Batch 12864, Loss: 3.7835\n",
      "  Batch 12896, Loss: 3.5602\n",
      "  Batch 12928, Loss: 3.7449\n",
      "  Batch 12960, Loss: 3.7315\n",
      "  Batch 12992, Loss: 3.7987\n",
      "  Batch 13024, Loss: 3.8244\n",
      "  Batch 13056, Loss: 3.7739\n",
      "  Batch 13088, Loss: 3.8009\n",
      "  Batch 13120, Loss: 3.6506\n",
      "  Batch 13152, Loss: 3.7266\n",
      "  Batch 13184, Loss: 3.7331\n",
      "  Batch 13216, Loss: 3.8964\n",
      "  Batch 13248, Loss: 3.6726\n",
      "  Batch 13280, Loss: 3.8958\n",
      "  Batch 13312, Loss: 3.7044\n",
      "  Batch 13344, Loss: 3.7478\n",
      "  Batch 13376, Loss: 3.6922\n",
      "  Batch 13408, Loss: 3.7228\n",
      "  Batch 13440, Loss: 3.7828\n",
      "  Batch 13472, Loss: 3.8722\n",
      "  Batch 13504, Loss: 3.9020\n",
      "  Batch 13536, Loss: 3.7502\n",
      "  Batch 13568, Loss: 3.8166\n",
      "  Batch 13600, Loss: 3.6130\n",
      "  Batch 13632, Loss: 3.7959\n",
      "  Batch 13664, Loss: 3.7966\n",
      "  Batch 13696, Loss: 3.7584\n",
      "  Batch 13728, Loss: 3.7924\n",
      "  Batch 13760, Loss: 3.7050\n",
      "  Batch 13792, Loss: 3.7875\n",
      "  Batch 13824, Loss: 3.6742\n",
      "  Batch 13856, Loss: 3.6674\n",
      "  Batch 13888, Loss: 3.7441\n",
      "  Batch 13920, Loss: 3.7573\n",
      "  Batch 13952, Loss: 3.6956\n",
      "  Batch 13984, Loss: 3.8218\n",
      "  Batch 14016, Loss: 3.6570\n",
      "  Batch 14048, Loss: 3.5960\n",
      "  Batch 14080, Loss: 3.6092\n",
      "  Batch 14112, Loss: 3.6977\n",
      "  Batch 14144, Loss: 3.6862\n",
      "  Batch 14176, Loss: 3.7969\n",
      "  Batch 14208, Loss: 3.7747\n",
      "  Batch 14240, Loss: 3.7375\n",
      "  Batch 14272, Loss: 3.6583\n",
      "  Batch 14304, Loss: 3.6450\n",
      "  Batch 14336, Loss: 3.8683\n",
      "  Batch 14368, Loss: 3.7476\n",
      "  Batch 14400, Loss: 3.7454\n",
      "  Batch 14432, Loss: 3.7948\n",
      "  Batch 14464, Loss: 3.6947\n",
      "  Batch 14496, Loss: 3.7542\n",
      "  Batch 14528, Loss: 3.6965\n",
      "  Batch 14560, Loss: 3.7060\n",
      "  Batch 14592, Loss: 3.6974\n",
      "  Batch 14624, Loss: 3.7006\n",
      "  Batch 14656, Loss: 3.6243\n",
      "  Batch 14688, Loss: 3.7794\n",
      "  Batch 14720, Loss: 3.7404\n",
      "  Batch 14752, Loss: 3.7821\n",
      "  Batch 14784, Loss: 3.6664\n",
      "  Batch 14816, Loss: 3.7224\n",
      "  Batch 14848, Loss: 3.7370\n",
      "  Batch 14880, Loss: 3.7149\n",
      "  Batch 14912, Loss: 3.6361\n",
      "  Batch 14944, Loss: 3.6809\n",
      "  Batch 14976, Loss: 3.5562\n",
      "  Batch 15008, Loss: 3.6274\n",
      "  Batch 15040, Loss: 3.7328\n",
      "  Batch 15072, Loss: 3.6987\n",
      "  Batch 15104, Loss: 3.6787\n",
      "  Batch 15136, Loss: 3.6515\n",
      "  Batch 15168, Loss: 3.6924\n",
      "  Batch 15200, Loss: 3.7998\n",
      "  Batch 15232, Loss: 3.8341\n",
      "  Batch 15264, Loss: 3.7214\n",
      "  Batch 15296, Loss: 3.6545\n",
      "  Batch 15328, Loss: 3.7171\n",
      "  Batch 15360, Loss: 3.6916\n",
      "  Batch 15392, Loss: 3.7113\n",
      "  Batch 15424, Loss: 3.7335\n",
      "  Batch 15456, Loss: 3.8149\n",
      "  Batch 15488, Loss: 3.6652\n",
      "  Batch 15520, Loss: 3.6749\n",
      "  Batch 15552, Loss: 3.7810\n",
      "  Batch 15584, Loss: 3.6012\n",
      "  Batch 15616, Loss: 3.7156\n",
      "  Batch 15648, Loss: 3.7452\n",
      "  Batch 15680, Loss: 3.7899\n",
      "  Batch 15712, Loss: 3.6682\n",
      "  Batch 15744, Loss: 3.5834\n",
      "  Batch 15776, Loss: 3.7387\n",
      "  Batch 15808, Loss: 3.7690\n",
      "  Batch 15840, Loss: 3.6387\n",
      "  Batch 15872, Loss: 3.7554\n",
      "  Batch 15904, Loss: 3.6205\n",
      "  Batch 15936, Loss: 3.6954\n",
      "  Batch 15968, Loss: 3.7871\n",
      "  Batch 16000, Loss: 3.7053\n",
      "  Batch 16032, Loss: 3.6882\n",
      "  Batch 16064, Loss: 3.6220\n",
      "  Batch 16096, Loss: 3.6201\n",
      "  Batch 16128, Loss: 3.7446\n",
      "  Batch 16160, Loss: 3.7941\n",
      "  Batch 16192, Loss: 3.5771\n",
      "  Batch 16224, Loss: 3.7467\n",
      "  Batch 16256, Loss: 3.6976\n",
      "  Batch 16288, Loss: 3.6791\n",
      "  Batch 16320, Loss: 3.7919\n",
      "  Batch 16352, Loss: 3.6919\n",
      "  Batch 16384, Loss: 3.6855\n",
      "  Batch 16416, Loss: 3.6524\n",
      "  Batch 16448, Loss: 3.6307\n",
      "  Batch 16480, Loss: 3.6175\n",
      "  Batch 16512, Loss: 3.6441\n",
      "  Batch 16544, Loss: 3.7156\n",
      "  Batch 16576, Loss: 3.5867\n",
      "  Batch 16608, Loss: 3.5895\n",
      "  Batch 16640, Loss: 3.7085\n",
      "  Batch 16672, Loss: 3.5697\n",
      "  Batch 16704, Loss: 3.6434\n",
      "  Batch 16736, Loss: 3.6619\n",
      "  Batch 16768, Loss: 3.5929\n",
      "  Batch 16800, Loss: 3.7011\n",
      "  Batch 16832, Loss: 3.5973\n",
      "  Batch 16864, Loss: 3.7087\n",
      "  Batch 16896, Loss: 3.5761\n",
      "  Batch 16928, Loss: 3.6109\n",
      "  Batch 16960, Loss: 3.5665\n",
      "  Batch 16992, Loss: 3.6597\n",
      "  Batch 17024, Loss: 3.5878\n",
      "  Batch 17056, Loss: 3.5437\n",
      "  Batch 17088, Loss: 3.7055\n",
      "  Batch 17120, Loss: 3.6176\n",
      "  Batch 17152, Loss: 3.6538\n",
      "  Batch 17184, Loss: 3.5916\n",
      "  Batch 17216, Loss: 3.6719\n",
      "  Batch 17248, Loss: 3.6848\n",
      "  Batch 17280, Loss: 3.5397\n",
      "  Batch 17312, Loss: 3.5958\n",
      "  Batch 17344, Loss: 3.7398\n",
      "  Batch 17376, Loss: 3.7153\n",
      "  Batch 17408, Loss: 3.6211\n",
      "  Batch 17440, Loss: 3.7041\n",
      "  Batch 17472, Loss: 3.6936\n",
      "  Batch 17504, Loss: 3.6030\n",
      "  Batch 17536, Loss: 3.5889\n",
      "  Batch 17568, Loss: 3.6273\n",
      "  Batch 17600, Loss: 3.7921\n",
      "  Batch 17632, Loss: 3.6900\n",
      "  Batch 17664, Loss: 3.6316\n",
      "  Batch 17696, Loss: 3.6953\n",
      "  Batch 17728, Loss: 3.6404\n",
      "  Batch 17760, Loss: 3.5917\n",
      "  Batch 17792, Loss: 3.7151\n",
      "  Batch 17824, Loss: 3.6915\n",
      "  Batch 17856, Loss: 3.5768\n",
      "  Batch 17888, Loss: 3.5464\n",
      "  Batch 17920, Loss: 3.6235\n",
      "  Batch 17952, Loss: 3.5903\n",
      "  Batch 17984, Loss: 3.6850\n",
      "  Batch 18016, Loss: 3.7298\n",
      "  Batch 18048, Loss: 3.6217\n",
      "  Batch 18080, Loss: 3.7003\n",
      "  Batch 18112, Loss: 3.6620\n",
      "  Batch 18144, Loss: 3.6596\n",
      "  Batch 18176, Loss: 3.5733\n",
      "  Batch 18208, Loss: 3.5328\n",
      "  Batch 18240, Loss: 3.5982\n",
      "  Batch 18272, Loss: 3.6575\n",
      "  Batch 18304, Loss: 3.6623\n",
      "  Batch 18336, Loss: 3.6250\n",
      "  Batch 18368, Loss: 3.5195\n",
      "  Batch 18400, Loss: 3.6154\n",
      "  Batch 18432, Loss: 3.5774\n",
      "  Batch 18464, Loss: 3.6018\n",
      "  Batch 18496, Loss: 3.5895\n",
      "  Batch 18528, Loss: 3.6039\n",
      "  Batch 18560, Loss: 3.5776\n",
      "  Batch 18592, Loss: 3.5844\n",
      "  Batch 18624, Loss: 3.6383\n",
      "  Batch 18656, Loss: 3.7122\n",
      "  Batch 18688, Loss: 3.6654\n",
      "  Batch 18720, Loss: 3.6923\n",
      "  Batch 18752, Loss: 3.5462\n",
      "  Batch 18784, Loss: 3.6212\n",
      "  Batch 18816, Loss: 3.5363\n",
      "  Batch 18848, Loss: 3.6352\n",
      "  Batch 18880, Loss: 3.5671\n",
      "  Batch 18912, Loss: 3.5488\n",
      "  Batch 18944, Loss: 3.6837\n",
      "  Batch 18976, Loss: 3.5850\n",
      "  Batch 19008, Loss: 3.6653\n",
      "  Batch 19040, Loss: 3.6040\n",
      "  Batch 19072, Loss: 3.5982\n",
      "  Batch 19104, Loss: 3.6525\n",
      "  Batch 19136, Loss: 3.5784\n",
      "  Batch 19168, Loss: 3.6136\n",
      "  Batch 19200, Loss: 3.6578\n",
      "  Batch 19232, Loss: 3.5966\n",
      "  Batch 19264, Loss: 3.6017\n",
      "  Batch 19296, Loss: 3.6120\n",
      "  Batch 19328, Loss: 3.5272\n",
      "  Batch 19360, Loss: 3.4639\n",
      "  Batch 19392, Loss: 3.6327\n",
      "  Batch 19424, Loss: 3.6217\n",
      "  Batch 19456, Loss: 3.6177\n",
      "  Batch 19488, Loss: 3.4858\n",
      "  Batch 19520, Loss: 3.6587\n",
      "  Batch 19552, Loss: 3.5639\n",
      "  Batch 19584, Loss: 3.7192\n",
      "  Batch 19616, Loss: 3.5696\n",
      "  Batch 19648, Loss: 3.5607\n",
      "  Batch 19680, Loss: 3.5972\n",
      "  Batch 19712, Loss: 3.6169\n",
      "  Batch 19744, Loss: 3.6915\n",
      "  Batch 19776, Loss: 3.6152\n",
      "  Batch 19808, Loss: 3.5570\n",
      "  Batch 19840, Loss: 3.6558\n",
      "  Batch 19872, Loss: 3.5606\n",
      "  Batch 19904, Loss: 3.5732\n",
      "  Batch 19936, Loss: 3.4739\n",
      "  Batch 19968, Loss: 3.6030\n",
      "  Batch 20000, Loss: 3.5986\n",
      "  Batch 20032, Loss: 3.5211\n",
      "  Batch 20064, Loss: 3.4747\n",
      "  Batch 20096, Loss: 3.5059\n",
      "  Batch 20128, Loss: 3.6168\n",
      "  Batch 20160, Loss: 3.7197\n",
      "  Batch 20192, Loss: 3.6085\n",
      "  Batch 20224, Loss: 3.6623\n",
      "  Batch 20256, Loss: 3.6478\n",
      "  Batch 20288, Loss: 3.7137\n",
      "  Batch 20320, Loss: 3.5932\n",
      "  Batch 20352, Loss: 3.6255\n",
      "  Batch 20384, Loss: 3.5093\n",
      "  Batch 20416, Loss: 3.6804\n",
      "  Batch 20448, Loss: 3.5608\n",
      "  Batch 20480, Loss: 3.6374\n",
      "  Batch 20512, Loss: 3.5555\n",
      "  Batch 20544, Loss: 3.5985\n",
      "  Batch 20576, Loss: 3.5954\n",
      "  Batch 20608, Loss: 3.5322\n",
      "  Batch 20640, Loss: 3.5243\n",
      "  Batch 20672, Loss: 3.5692\n",
      "  Batch 20704, Loss: 3.5801\n",
      "  Batch 20736, Loss: 3.5081\n",
      "  Batch 20768, Loss: 3.7045\n",
      "  Batch 20800, Loss: 3.5567\n",
      "  Batch 20832, Loss: 3.5387\n",
      "  Batch 20864, Loss: 3.5051\n",
      "  Batch 20896, Loss: 3.5135\n",
      "  Batch 20928, Loss: 3.4263\n",
      "  Batch 20960, Loss: 3.5944\n",
      "  Batch 20992, Loss: 3.4892\n",
      "  Batch 21024, Loss: 3.6579\n",
      "  Batch 21056, Loss: 3.5026\n",
      "  Batch 21088, Loss: 3.5211\n",
      "  Batch 21120, Loss: 3.5745\n",
      "  Batch 21152, Loss: 3.5856\n",
      "  Batch 21184, Loss: 3.6525\n",
      "  Batch 21216, Loss: 3.6356\n",
      "  Batch 21248, Loss: 3.5259\n",
      "  Batch 21280, Loss: 3.5309\n",
      "  Batch 21312, Loss: 3.4957\n",
      "  Batch 21344, Loss: 3.4228\n",
      "  Batch 21376, Loss: 3.5649\n",
      "  Batch 21408, Loss: 3.6086\n",
      "  Batch 21440, Loss: 3.4760\n",
      "  Batch 21472, Loss: 3.5215\n",
      "  Batch 21504, Loss: 3.5343\n",
      "  Batch 21536, Loss: 3.6152\n",
      "  Batch 21568, Loss: 3.5481\n",
      "  Batch 21600, Loss: 3.5544\n",
      "  Batch 21632, Loss: 3.5502\n",
      "  Batch 21664, Loss: 3.5037\n",
      "  Batch 21696, Loss: 3.4835\n",
      "  Batch 21728, Loss: 3.6018\n",
      "  Batch 21760, Loss: 3.5736\n",
      "  Batch 21792, Loss: 3.6167\n",
      "  Batch 21824, Loss: 3.6318\n",
      "  Batch 21856, Loss: 3.4979\n",
      "  Batch 21888, Loss: 3.4675\n",
      "  Batch 21920, Loss: 3.4615\n",
      "  Batch 21952, Loss: 3.5379\n",
      "  Batch 21984, Loss: 3.6057\n",
      "  Batch 22016, Loss: 3.6744\n",
      "  Batch 22048, Loss: 3.5402\n",
      "  Batch 22080, Loss: 3.4943\n",
      "  Batch 22112, Loss: 3.6498\n",
      "  Batch 22144, Loss: 3.4956\n",
      "  Batch 22176, Loss: 3.4988\n",
      "  Batch 22208, Loss: 3.4892\n",
      "  Batch 22240, Loss: 3.5120\n",
      "  Batch 22272, Loss: 3.6451\n",
      "  Batch 22304, Loss: 3.5402\n",
      "  Batch 22336, Loss: 3.5357\n",
      "  Batch 22368, Loss: 3.5807\n",
      "  Batch 22400, Loss: 3.4333\n",
      "  Batch 22432, Loss: 3.5605\n",
      "  Batch 22464, Loss: 3.5212\n",
      "  Batch 22496, Loss: 3.4567\n",
      "  Batch 22528, Loss: 3.4914\n",
      "  Batch 22560, Loss: 3.6571\n",
      "  Batch 22592, Loss: 3.4360\n",
      "  Batch 22624, Loss: 3.6276\n",
      "  Batch 22656, Loss: 3.5368\n",
      "  Batch 22688, Loss: 3.5701\n",
      "  Batch 22720, Loss: 3.5910\n",
      "  Batch 22752, Loss: 3.5915\n",
      "  Batch 22784, Loss: 3.5251\n",
      "  Batch 22816, Loss: 3.6260\n",
      "  Batch 22848, Loss: 3.5931\n",
      "  Batch 22880, Loss: 3.5586\n",
      "  Batch 22912, Loss: 3.5058\n",
      "  Batch 22944, Loss: 3.5134\n",
      "  Batch 22976, Loss: 3.5027\n",
      "  Batch 23008, Loss: 3.5348\n",
      "  Batch 23040, Loss: 3.5122\n",
      "  Batch 23072, Loss: 3.4725\n",
      "  Batch 23104, Loss: 3.4676\n",
      "  Batch 23136, Loss: 3.5728\n",
      "  Batch 23168, Loss: 3.7290\n",
      "  Batch 23200, Loss: 3.4669\n",
      "  Batch 23232, Loss: 3.5215\n",
      "  Batch 23264, Loss: 3.4971\n",
      "  Batch 23296, Loss: 3.5542\n",
      "  Batch 23328, Loss: 3.5548\n",
      "  Batch 23360, Loss: 3.3880\n",
      "  Batch 23392, Loss: 3.4369\n",
      "  Batch 23424, Loss: 3.4006\n",
      "  Batch 23456, Loss: 3.6537\n",
      "  Batch 23488, Loss: 3.5783\n",
      "  Batch 23520, Loss: 3.4845\n",
      "  Batch 23552, Loss: 3.6384\n",
      "  Batch 23584, Loss: 3.7029\n",
      "  Batch 23616, Loss: 3.5693\n",
      "  Batch 23648, Loss: 3.5082\n",
      "  Batch 23680, Loss: 3.5138\n",
      "  Batch 23712, Loss: 3.4382\n",
      "  Batch 23744, Loss: 3.5212\n",
      "  Batch 23776, Loss: 3.4980\n",
      "  Batch 23808, Loss: 3.5419\n",
      "  Batch 23840, Loss: 3.5584\n",
      "  Batch 23872, Loss: 3.6465\n",
      "  Batch 23904, Loss: 3.5315\n",
      "  Batch 23936, Loss: 3.5173\n",
      "  Batch 23968, Loss: 3.4105\n",
      "  Batch 24000, Loss: 3.4162\n",
      "  Batch 24032, Loss: 3.4366\n",
      "  Batch 24064, Loss: 3.5586\n",
      "  Batch 24096, Loss: 3.5478\n",
      "  Batch 24128, Loss: 3.4940\n",
      "  Batch 24160, Loss: 3.4938\n",
      "  Batch 24192, Loss: 3.5015\n",
      "  Batch 24224, Loss: 3.5490\n",
      "  Batch 24256, Loss: 3.5799\n",
      "  Batch 24288, Loss: 3.6393\n",
      "  Batch 24320, Loss: 3.4494\n",
      "  Batch 24352, Loss: 3.5618\n",
      "  Batch 24384, Loss: 3.5152\n",
      "  Batch 24416, Loss: 3.4857\n",
      "  Batch 24448, Loss: 3.5373\n",
      "  Batch 24480, Loss: 3.4668\n",
      "  Batch 24512, Loss: 3.5410\n",
      "  Batch 24544, Loss: 3.5977\n",
      "  Batch 24576, Loss: 3.6663\n",
      "  Batch 24608, Loss: 3.4428\n",
      "  Batch 24640, Loss: 3.5531\n",
      "  Batch 24672, Loss: 3.4635\n",
      "  Batch 24704, Loss: 3.7014\n",
      "  Batch 24736, Loss: 3.5295\n",
      "  Batch 24768, Loss: 3.5144\n",
      "  Batch 24800, Loss: 3.4400\n",
      "  Batch 24832, Loss: 3.5629\n",
      "  Batch 24864, Loss: 3.6489\n",
      "  Batch 24896, Loss: 3.5299\n",
      "  Batch 24928, Loss: 3.5978\n",
      "  Batch 24960, Loss: 3.4713\n",
      "  Batch 24992, Loss: 3.5095\n",
      "  Batch 25024, Loss: 3.5913\n",
      "  Batch 25056, Loss: 3.4851\n",
      "  Batch 25088, Loss: 3.5550\n",
      "  Batch 25120, Loss: 3.4860\n",
      "  Batch 25152, Loss: 3.5101\n",
      "  Batch 25184, Loss: 3.4424\n",
      "  Batch 25216, Loss: 3.4842\n",
      "  Batch 25248, Loss: 3.4926\n",
      "  Batch 25280, Loss: 3.4593\n",
      "  Batch 25312, Loss: 3.6101\n",
      "  Batch 25344, Loss: 3.5240\n",
      "  Batch 25376, Loss: 3.4457\n",
      "  Batch 25408, Loss: 3.6388\n",
      "  Batch 25440, Loss: 3.4803\n",
      "  Batch 25472, Loss: 3.3732\n",
      "  Batch 25504, Loss: 3.5042\n",
      "  Batch 25536, Loss: 3.5476\n",
      "  Batch 25568, Loss: 3.5842\n",
      "  Batch 25600, Loss: 3.4170\n",
      "  Batch 25632, Loss: 3.5135\n",
      "  Batch 25664, Loss: 3.5138\n",
      "  Batch 25696, Loss: 3.4296\n",
      "  Batch 25728, Loss: 3.5886\n",
      "  Batch 25760, Loss: 3.6021\n",
      "  Batch 25792, Loss: 3.5066\n",
      "  Batch 25824, Loss: 3.5337\n",
      "  Batch 25856, Loss: 3.5909\n",
      "  Batch 25888, Loss: 3.4148\n",
      "  Batch 25920, Loss: 3.5462\n",
      "  Batch 25952, Loss: 3.4014\n",
      "  Batch 25984, Loss: 3.3498\n",
      "  Batch 26016, Loss: 3.5242\n",
      "  Batch 26048, Loss: 3.4329\n",
      "  Batch 26080, Loss: 3.4615\n",
      "  Batch 26112, Loss: 3.4782\n",
      "  Batch 26144, Loss: 3.4900\n",
      "  Batch 26176, Loss: 3.5266\n",
      "  Batch 26208, Loss: 3.5461\n",
      "  Batch 26240, Loss: 3.4884\n",
      "  Batch 26272, Loss: 3.4177\n",
      "  Batch 26304, Loss: 3.4750\n",
      "  Batch 26336, Loss: 3.4395\n",
      "  Batch 26368, Loss: 3.4577\n",
      "  Batch 26400, Loss: 3.4521\n",
      "  Batch 26432, Loss: 3.4875\n",
      "  Batch 26464, Loss: 3.4426\n",
      "  Batch 26496, Loss: 3.5309\n",
      "  Batch 26528, Loss: 3.4237\n",
      "  Batch 26560, Loss: 3.5481\n",
      "  Batch 26592, Loss: 3.4989\n",
      "  Batch 26624, Loss: 3.5467\n",
      "  Batch 26656, Loss: 3.4850\n",
      "  Batch 26688, Loss: 3.4697\n",
      "  Batch 26720, Loss: 3.4658\n",
      "  Batch 26752, Loss: 3.4484\n",
      "  Batch 26784, Loss: 3.5874\n",
      "  Batch 26816, Loss: 3.4342\n",
      "  Batch 26848, Loss: 3.5845\n",
      "  Batch 26880, Loss: 3.4391\n",
      "  Batch 26912, Loss: 3.4170\n",
      "  Batch 26944, Loss: 3.4369\n",
      "  Batch 26976, Loss: 3.3724\n",
      "  Batch 27008, Loss: 3.4602\n",
      "  Batch 27040, Loss: 3.4002\n",
      "  Batch 27072, Loss: 3.4266\n",
      "  Batch 27104, Loss: 3.4708\n",
      "  Batch 27136, Loss: 3.5316\n",
      "  Batch 27168, Loss: 3.5112\n",
      "  Batch 27200, Loss: 3.6396\n",
      "  Batch 27232, Loss: 3.4938\n",
      "  Batch 27264, Loss: 3.5101\n",
      "  Batch 27296, Loss: 3.5103\n",
      "  Batch 27328, Loss: 3.4806\n",
      "  Batch 27360, Loss: 3.4868\n",
      "  Batch 27392, Loss: 3.5240\n",
      "  Batch 27424, Loss: 3.3459\n",
      "  Batch 27456, Loss: 3.4965\n",
      "  Batch 27488, Loss: 3.6160\n",
      "  Batch 27520, Loss: 3.4150\n",
      "  Batch 27552, Loss: 3.4836\n",
      "  Batch 27584, Loss: 3.4723\n",
      "  Batch 27616, Loss: 3.3554\n",
      "  Batch 27648, Loss: 3.5025\n",
      "  Batch 27680, Loss: 3.5338\n",
      "  Batch 27712, Loss: 3.3709\n",
      "  Batch 27744, Loss: 3.4536\n",
      "  Batch 27776, Loss: 3.3748\n",
      "  Batch 27808, Loss: 3.4126\n",
      "  Batch 27840, Loss: 3.4714\n",
      "  Batch 27872, Loss: 3.4894\n",
      "  Batch 27904, Loss: 3.3624\n",
      "  Batch 27936, Loss: 3.4435\n",
      "  Batch 27968, Loss: 3.4503\n",
      "  Batch 28000, Loss: 3.3058\n",
      "  Batch 28032, Loss: 3.4525\n",
      "  Batch 28064, Loss: 3.3522\n",
      "  Batch 28096, Loss: 3.4881\n",
      "  Batch 28128, Loss: 3.4690\n",
      "  Batch 28160, Loss: 3.3611\n",
      "  Batch 28192, Loss: 3.4667\n",
      "  Batch 28224, Loss: 3.5419\n",
      "  Batch 28256, Loss: 3.3540\n",
      "  Batch 28288, Loss: 3.3875\n",
      "  Batch 28320, Loss: 3.4693\n",
      "  Batch 28352, Loss: 3.4619\n",
      "  Batch 28384, Loss: 3.3402\n",
      "  Batch 28416, Loss: 3.4340\n",
      "  Batch 28448, Loss: 3.3914\n",
      "  Batch 28480, Loss: 3.3648\n",
      "  Batch 28512, Loss: 3.5282\n",
      "  Batch 28544, Loss: 3.4384\n",
      "  Batch 28576, Loss: 3.4411\n",
      "  Batch 28608, Loss: 3.4287\n",
      "  Batch 28640, Loss: 3.5094\n",
      "  Batch 28672, Loss: 3.4035\n",
      "  Batch 28704, Loss: 3.4599\n",
      "  Batch 28736, Loss: 3.3487\n",
      "  Batch 28768, Loss: 3.4582\n",
      "  Batch 28800, Loss: 3.4059\n",
      "  Batch 28832, Loss: 3.5358\n",
      "  Batch 28864, Loss: 3.4493\n",
      "  Batch 28896, Loss: 3.3548\n",
      "  Batch 28928, Loss: 3.2717\n",
      "  Batch 28960, Loss: 3.4855\n",
      "  Batch 28992, Loss: 3.4580\n",
      "  Batch 29024, Loss: 3.3613\n",
      "  Batch 29056, Loss: 3.3952\n",
      "  Batch 29088, Loss: 3.4524\n",
      "  Batch 29120, Loss: 3.3770\n",
      "  Batch 29152, Loss: 3.4615\n",
      "  Batch 29184, Loss: 3.2879\n",
      "  Batch 29216, Loss: 3.3867\n",
      "  Batch 29248, Loss: 3.5110\n",
      "  Batch 29280, Loss: 3.4244\n",
      "  Batch 29312, Loss: 3.5069\n",
      "  Batch 29344, Loss: 3.2490\n",
      "  Batch 29376, Loss: 3.5112\n",
      "  Batch 29408, Loss: 3.4140\n",
      "  Batch 29440, Loss: 3.4494\n",
      "  Batch 29472, Loss: 3.4449\n",
      "  Batch 29504, Loss: 3.4011\n",
      "  Batch 29536, Loss: 3.5302\n",
      "  Batch 29568, Loss: 3.4314\n",
      "  Batch 29600, Loss: 3.3942\n",
      "  Batch 29632, Loss: 3.4428\n",
      "  Batch 29664, Loss: 3.3891\n",
      "  Batch 29696, Loss: 3.4378\n",
      "  Batch 29728, Loss: 3.4357\n",
      "  Batch 29760, Loss: 3.5086\n",
      "  Batch 29792, Loss: 3.4456\n",
      "  Batch 29824, Loss: 3.5595\n",
      "  Batch 29856, Loss: 3.3206\n",
      "  Batch 29888, Loss: 3.3501\n",
      "  Batch 29920, Loss: 3.4429\n",
      "  Batch 29952, Loss: 3.4310\n",
      "  Batch 29984, Loss: 3.3548\n",
      "  Batch 30016, Loss: 3.3738\n",
      "  Batch 30048, Loss: 3.5066\n",
      "  Batch 30080, Loss: 3.4603\n",
      "  Batch 30112, Loss: 3.3118\n",
      "  Batch 30144, Loss: 3.6085\n",
      "  Batch 30176, Loss: 3.4070\n",
      "  Batch 30208, Loss: 3.4976\n",
      "  Batch 30240, Loss: 3.4184\n",
      "  Batch 30272, Loss: 3.3634\n",
      "  Batch 30304, Loss: 3.3358\n",
      "  Batch 30336, Loss: 3.4508\n",
      "  Batch 30368, Loss: 3.4886\n",
      "  Batch 30400, Loss: 3.3557\n",
      "  Batch 30432, Loss: 3.5256\n",
      "  Batch 30464, Loss: 3.4993\n",
      "  Batch 30496, Loss: 3.4305\n",
      "  Batch 30528, Loss: 3.4216\n",
      "  Batch 30560, Loss: 3.4269\n",
      "  Batch 30592, Loss: 3.3568\n",
      "  Batch 30624, Loss: 3.3662\n",
      "  Batch 30656, Loss: 3.5387\n",
      "  Batch 30688, Loss: 3.3945\n",
      "  Batch 30720, Loss: 3.3831\n",
      "  Batch 30752, Loss: 3.4042\n",
      "  Batch 30784, Loss: 3.4160\n",
      "  Batch 30816, Loss: 3.2508\n",
      "  Batch 30848, Loss: 3.4055\n",
      "  Batch 30880, Loss: 3.3657\n",
      "  Batch 30912, Loss: 3.3747\n",
      "  Batch 30944, Loss: 3.2838\n",
      "  Batch 30976, Loss: 3.4150\n",
      "  Batch 31008, Loss: 3.4615\n",
      "  Batch 31040, Loss: 3.4459\n",
      "  Batch 31072, Loss: 3.3973\n",
      "  Batch 31104, Loss: 3.5286\n",
      "  Batch 31136, Loss: 3.5077\n",
      "  Batch 31168, Loss: 3.3096\n",
      "  Batch 31200, Loss: 3.3554\n",
      "  Batch 31232, Loss: 3.4573\n",
      "  Batch 31264, Loss: 3.5604\n",
      "  Batch 31296, Loss: 3.4896\n",
      "  Batch 31328, Loss: 3.5137\n",
      "  Batch 31360, Loss: 3.3477\n",
      "  Batch 31392, Loss: 3.4505\n",
      "  Batch 31424, Loss: 3.4531\n",
      "  Batch 31456, Loss: 3.3157\n",
      "  Batch 31488, Loss: 3.3672\n",
      "  Batch 31520, Loss: 3.4001\n",
      "  Batch 31552, Loss: 3.2837\n",
      "  Batch 31584, Loss: 3.4664\n",
      "  Batch 31616, Loss: 3.3937\n",
      "  Batch 31648, Loss: 3.3186\n",
      "  Batch 31680, Loss: 3.3462\n",
      "  Batch 31712, Loss: 3.4362\n",
      "  Batch 31744, Loss: 3.4456\n",
      "  Batch 31776, Loss: 3.2957\n",
      "  Batch 31808, Loss: 3.4046\n",
      "  Batch 31840, Loss: 3.5784\n",
      "  Batch 31872, Loss: 3.3817\n",
      "  Batch 31904, Loss: 3.3620\n",
      "  Batch 31936, Loss: 3.3718\n",
      "  Batch 31968, Loss: 3.3527\n",
      "  Batch 32000, Loss: 3.4588\n",
      "  Batch 32032, Loss: 3.4040\n",
      "  Batch 32064, Loss: 3.3222\n",
      "  Batch 32096, Loss: 3.4569\n",
      "  Batch 32128, Loss: 3.3555\n",
      "  Batch 32160, Loss: 3.2844\n",
      "  Batch 32192, Loss: 3.3767\n",
      "  Batch 32224, Loss: 3.3744\n",
      "  Batch 32256, Loss: 3.2047\n",
      "  Batch 32288, Loss: 3.4815\n",
      "  Batch 32320, Loss: 3.4101\n",
      "  Batch 32352, Loss: 3.4144\n",
      "  Batch 32384, Loss: 3.4356\n",
      "  Batch 32416, Loss: 3.4744\n",
      "  Batch 32448, Loss: 3.5227\n",
      "  Batch 32480, Loss: 3.3945\n",
      "  Batch 32512, Loss: 3.3500\n",
      "  Batch 32544, Loss: 3.3015\n",
      "  Batch 32576, Loss: 3.3316\n",
      "  Batch 32608, Loss: 3.4166\n",
      "  Batch 32640, Loss: 3.4541\n",
      "  Batch 32672, Loss: 3.3858\n",
      "  Batch 32704, Loss: 3.4517\n",
      "  Batch 32736, Loss: 3.4478\n",
      "  Batch 32768, Loss: 3.3230\n",
      "  Batch 32800, Loss: 3.3714\n",
      "  Batch 32832, Loss: 3.3025\n",
      "  Batch 32864, Loss: 3.4996\n",
      "  Batch 32896, Loss: 3.4289\n",
      "  Batch 32928, Loss: 3.3825\n",
      "  Batch 32960, Loss: 3.3870\n",
      "  Batch 32992, Loss: 3.3983\n",
      "  Batch 33024, Loss: 3.3876\n",
      "  Batch 33056, Loss: 3.4355\n",
      "  Batch 33088, Loss: 3.3557\n",
      "  Batch 33120, Loss: 3.4871\n",
      "  Batch 33152, Loss: 3.4014\n",
      "  Batch 33184, Loss: 3.2988\n",
      "  Batch 33216, Loss: 3.3926\n",
      "  Batch 33248, Loss: 3.4021\n",
      "  Batch 33280, Loss: 3.4152\n",
      "  Batch 33312, Loss: 3.4612\n",
      "  Batch 33344, Loss: 3.3910\n",
      "  Batch 33376, Loss: 3.2857\n",
      "  Batch 33408, Loss: 3.4287\n",
      "  Batch 33440, Loss: 3.4346\n",
      "  Batch 33472, Loss: 3.4721\n",
      "  Batch 33504, Loss: 3.5113\n",
      "  Batch 33536, Loss: 3.2725\n",
      "  Batch 33568, Loss: 3.3300\n",
      "  Batch 33600, Loss: 3.4517\n",
      "  Batch 33632, Loss: 3.3803\n",
      "  Batch 33664, Loss: 3.3480\n",
      "  Batch 33696, Loss: 3.3908\n",
      "  Batch 33728, Loss: 3.2864\n",
      "  Batch 33760, Loss: 3.3400\n",
      "  Batch 33792, Loss: 3.4601\n",
      "Epoch 1/3, Loss: 3.8324\n",
      "  Batch 0, Loss: 3.3650\n",
      "  Batch 32, Loss: 3.4711\n",
      "  Batch 64, Loss: 3.3236\n",
      "  Batch 96, Loss: 3.4163\n",
      "  Batch 128, Loss: 3.4088\n",
      "  Batch 160, Loss: 3.3769\n",
      "  Batch 192, Loss: 3.4647\n",
      "  Batch 224, Loss: 3.3526\n",
      "  Batch 256, Loss: 3.3609\n",
      "  Batch 288, Loss: 3.3405\n",
      "  Batch 320, Loss: 3.4398\n",
      "  Batch 352, Loss: 3.3528\n",
      "  Batch 384, Loss: 3.2042\n",
      "  Batch 416, Loss: 3.3049\n",
      "  Batch 448, Loss: 3.2811\n",
      "  Batch 480, Loss: 3.1996\n",
      "  Batch 512, Loss: 3.3372\n",
      "  Batch 544, Loss: 3.4593\n",
      "  Batch 576, Loss: 3.3106\n",
      "  Batch 608, Loss: 3.3708\n",
      "  Batch 640, Loss: 3.3916\n",
      "  Batch 672, Loss: 3.4118\n",
      "  Batch 704, Loss: 3.4062\n",
      "  Batch 736, Loss: 3.3788\n",
      "  Batch 768, Loss: 3.2480\n",
      "  Batch 800, Loss: 3.3394\n",
      "  Batch 832, Loss: 3.2507\n",
      "  Batch 864, Loss: 3.3545\n",
      "  Batch 896, Loss: 3.3977\n",
      "  Batch 928, Loss: 3.3589\n",
      "  Batch 960, Loss: 3.3885\n",
      "  Batch 992, Loss: 3.4399\n",
      "  Batch 1024, Loss: 3.4468\n",
      "  Batch 1056, Loss: 3.2910\n",
      "  Batch 1088, Loss: 3.2815\n",
      "  Batch 1120, Loss: 3.4247\n",
      "  Batch 1152, Loss: 3.4697\n",
      "  Batch 1184, Loss: 3.3024\n",
      "  Batch 1216, Loss: 3.4087\n",
      "  Batch 1248, Loss: 3.2882\n",
      "  Batch 1280, Loss: 3.4241\n",
      "  Batch 1312, Loss: 3.2837\n",
      "  Batch 1344, Loss: 3.2740\n",
      "  Batch 1376, Loss: 3.3209\n",
      "  Batch 1408, Loss: 3.3979\n",
      "  Batch 1440, Loss: 3.2533\n",
      "  Batch 1472, Loss: 3.3964\n",
      "  Batch 1504, Loss: 3.4936\n",
      "  Batch 1536, Loss: 3.3983\n",
      "  Batch 1568, Loss: 3.4676\n",
      "  Batch 1600, Loss: 3.3960\n",
      "  Batch 1632, Loss: 3.3150\n",
      "  Batch 1664, Loss: 3.3783\n",
      "  Batch 1696, Loss: 3.3542\n",
      "  Batch 1728, Loss: 3.3096\n",
      "  Batch 1760, Loss: 3.3918\n",
      "  Batch 1792, Loss: 3.2799\n",
      "  Batch 1824, Loss: 3.2828\n",
      "  Batch 1856, Loss: 3.3015\n",
      "  Batch 1888, Loss: 3.3441\n",
      "  Batch 1920, Loss: 3.4139\n",
      "  Batch 1952, Loss: 3.2720\n",
      "  Batch 1984, Loss: 3.1964\n",
      "  Batch 2016, Loss: 3.3203\n",
      "  Batch 2048, Loss: 3.3503\n",
      "  Batch 2080, Loss: 3.3729\n",
      "  Batch 2112, Loss: 3.4460\n",
      "  Batch 2144, Loss: 3.3573\n",
      "  Batch 2176, Loss: 3.3689\n",
      "  Batch 2208, Loss: 3.3249\n",
      "  Batch 2240, Loss: 3.2960\n",
      "  Batch 2272, Loss: 3.3292\n",
      "  Batch 2304, Loss: 3.3193\n",
      "  Batch 2336, Loss: 3.2967\n",
      "  Batch 2368, Loss: 3.3988\n",
      "  Batch 2400, Loss: 3.3233\n",
      "  Batch 2432, Loss: 3.2998\n",
      "  Batch 2464, Loss: 3.3445\n",
      "  Batch 2496, Loss: 3.4244\n",
      "  Batch 2528, Loss: 3.2873\n",
      "  Batch 2560, Loss: 3.2649\n",
      "  Batch 2592, Loss: 3.2824\n",
      "  Batch 2624, Loss: 3.2675\n",
      "  Batch 2656, Loss: 3.2976\n",
      "  Batch 2688, Loss: 3.3393\n",
      "  Batch 2720, Loss: 3.3325\n",
      "  Batch 2752, Loss: 3.3867\n",
      "  Batch 2784, Loss: 3.3453\n",
      "  Batch 2816, Loss: 3.4547\n",
      "  Batch 2848, Loss: 3.2880\n",
      "  Batch 2880, Loss: 3.2336\n",
      "  Batch 2912, Loss: 3.4302\n",
      "  Batch 2944, Loss: 3.2953\n",
      "  Batch 2976, Loss: 3.4298\n",
      "  Batch 3008, Loss: 3.3096\n",
      "  Batch 3040, Loss: 3.3932\n",
      "  Batch 3072, Loss: 3.2308\n",
      "  Batch 3104, Loss: 3.3179\n",
      "  Batch 3136, Loss: 3.4308\n",
      "  Batch 3168, Loss: 3.1889\n",
      "  Batch 3200, Loss: 3.2942\n",
      "  Batch 3232, Loss: 3.2905\n",
      "  Batch 3264, Loss: 3.2399\n",
      "  Batch 3296, Loss: 3.3252\n",
      "  Batch 3328, Loss: 3.4200\n",
      "  Batch 3360, Loss: 3.3676\n",
      "  Batch 3392, Loss: 3.3097\n",
      "  Batch 3424, Loss: 3.3969\n",
      "  Batch 3456, Loss: 3.3927\n",
      "  Batch 3488, Loss: 3.3183\n",
      "  Batch 3520, Loss: 3.2970\n",
      "  Batch 3552, Loss: 3.4174\n",
      "  Batch 3584, Loss: 3.1920\n",
      "  Batch 3616, Loss: 3.3763\n",
      "  Batch 3648, Loss: 3.3641\n",
      "  Batch 3680, Loss: 3.2504\n",
      "  Batch 3712, Loss: 3.2897\n",
      "  Batch 3744, Loss: 3.3086\n",
      "  Batch 3776, Loss: 3.3563\n",
      "  Batch 3808, Loss: 3.4897\n",
      "  Batch 3840, Loss: 3.2822\n",
      "  Batch 3872, Loss: 3.4035\n",
      "  Batch 3904, Loss: 3.2935\n",
      "  Batch 3936, Loss: 3.3284\n",
      "  Batch 3968, Loss: 3.2944\n",
      "  Batch 4000, Loss: 3.3357\n",
      "  Batch 4032, Loss: 3.2603\n",
      "  Batch 4064, Loss: 3.4830\n",
      "  Batch 4096, Loss: 3.2600\n",
      "  Batch 4128, Loss: 3.3219\n",
      "  Batch 4160, Loss: 3.2663\n",
      "  Batch 4192, Loss: 3.3884\n",
      "  Batch 4224, Loss: 3.2808\n",
      "  Batch 4256, Loss: 3.2667\n",
      "  Batch 4288, Loss: 3.4518\n",
      "  Batch 4320, Loss: 3.3389\n",
      "  Batch 4352, Loss: 3.2845\n",
      "  Batch 4384, Loss: 3.3858\n",
      "  Batch 4416, Loss: 3.2112\n",
      "  Batch 4448, Loss: 3.2650\n",
      "  Batch 4480, Loss: 3.3396\n",
      "  Batch 4512, Loss: 3.4051\n",
      "  Batch 4544, Loss: 3.2698\n",
      "  Batch 4576, Loss: 3.2980\n",
      "  Batch 4608, Loss: 3.1689\n",
      "  Batch 4640, Loss: 3.2836\n",
      "  Batch 4672, Loss: 3.5282\n",
      "  Batch 4704, Loss: 3.3700\n",
      "  Batch 4736, Loss: 3.3507\n",
      "  Batch 4768, Loss: 3.4441\n",
      "  Batch 4800, Loss: 3.3260\n",
      "  Batch 4832, Loss: 3.3938\n",
      "  Batch 4864, Loss: 3.3004\n",
      "  Batch 4896, Loss: 3.3440\n",
      "  Batch 4928, Loss: 3.2645\n",
      "  Batch 4960, Loss: 3.3186\n",
      "  Batch 4992, Loss: 3.2704\n",
      "  Batch 5024, Loss: 3.3683\n",
      "  Batch 5056, Loss: 3.3697\n",
      "  Batch 5088, Loss: 3.2634\n",
      "  Batch 5120, Loss: 3.3542\n",
      "  Batch 5152, Loss: 3.3589\n",
      "  Batch 5184, Loss: 3.3728\n",
      "  Batch 5216, Loss: 3.1744\n",
      "  Batch 5248, Loss: 3.3365\n",
      "  Batch 5280, Loss: 3.3433\n",
      "  Batch 5312, Loss: 3.2629\n",
      "  Batch 5344, Loss: 3.3338\n",
      "  Batch 5376, Loss: 3.2985\n",
      "  Batch 5408, Loss: 3.2409\n",
      "  Batch 5440, Loss: 3.3104\n",
      "  Batch 5472, Loss: 3.3528\n",
      "  Batch 5504, Loss: 3.2869\n",
      "  Batch 5536, Loss: 3.1949\n",
      "  Batch 5568, Loss: 3.4562\n",
      "  Batch 5600, Loss: 3.2275\n",
      "  Batch 5632, Loss: 3.1915\n",
      "  Batch 5664, Loss: 3.3111\n",
      "  Batch 5696, Loss: 3.1577\n",
      "  Batch 5728, Loss: 3.2340\n",
      "  Batch 5760, Loss: 3.2951\n",
      "  Batch 5792, Loss: 3.3083\n",
      "  Batch 5824, Loss: 3.2142\n",
      "  Batch 5856, Loss: 3.2591\n",
      "  Batch 5888, Loss: 3.2890\n",
      "  Batch 5920, Loss: 3.2053\n",
      "  Batch 5952, Loss: 3.3885\n",
      "  Batch 5984, Loss: 3.1570\n",
      "  Batch 6016, Loss: 3.3172\n",
      "  Batch 6048, Loss: 3.3443\n",
      "  Batch 6080, Loss: 3.3255\n",
      "  Batch 6112, Loss: 3.2689\n",
      "  Batch 6144, Loss: 3.3650\n",
      "  Batch 6176, Loss: 3.3115\n",
      "  Batch 6208, Loss: 3.2390\n",
      "  Batch 6240, Loss: 3.2989\n",
      "  Batch 6272, Loss: 3.2276\n",
      "  Batch 6304, Loss: 3.2811\n",
      "  Batch 6336, Loss: 3.1342\n",
      "  Batch 6368, Loss: 3.3471\n",
      "  Batch 6400, Loss: 3.3400\n",
      "  Batch 6432, Loss: 3.3385\n",
      "  Batch 6464, Loss: 3.3158\n",
      "  Batch 6496, Loss: 3.3250\n",
      "  Batch 6528, Loss: 3.2295\n",
      "  Batch 6560, Loss: 3.3348\n",
      "  Batch 6592, Loss: 3.2275\n",
      "  Batch 6624, Loss: 3.3291\n",
      "  Batch 6656, Loss: 3.3225\n",
      "  Batch 6688, Loss: 3.2213\n",
      "  Batch 6720, Loss: 3.3228\n",
      "  Batch 6752, Loss: 3.3409\n",
      "  Batch 6784, Loss: 3.3367\n",
      "  Batch 6816, Loss: 3.1980\n",
      "  Batch 6848, Loss: 3.2406\n",
      "  Batch 6880, Loss: 3.2481\n",
      "  Batch 6912, Loss: 3.2595\n",
      "  Batch 6944, Loss: 3.2233\n",
      "  Batch 6976, Loss: 3.2353\n",
      "  Batch 7008, Loss: 3.2651\n",
      "  Batch 7040, Loss: 3.3417\n",
      "  Batch 7072, Loss: 3.1892\n",
      "  Batch 7104, Loss: 3.2504\n",
      "  Batch 7136, Loss: 3.3841\n",
      "  Batch 7168, Loss: 3.1879\n",
      "  Batch 7200, Loss: 3.2145\n",
      "  Batch 7232, Loss: 3.1988\n",
      "  Batch 7264, Loss: 3.2976\n",
      "  Batch 7296, Loss: 3.4054\n",
      "  Batch 7328, Loss: 3.2414\n",
      "  Batch 7360, Loss: 3.2495\n",
      "  Batch 7392, Loss: 3.1032\n",
      "  Batch 7424, Loss: 3.3508\n",
      "  Batch 7456, Loss: 3.2862\n",
      "  Batch 7488, Loss: 3.2074\n",
      "  Batch 7520, Loss: 3.2790\n",
      "  Batch 7552, Loss: 3.1778\n",
      "  Batch 7584, Loss: 3.1984\n",
      "  Batch 7616, Loss: 3.1945\n",
      "  Batch 7648, Loss: 3.2029\n",
      "  Batch 7680, Loss: 3.1979\n",
      "  Batch 7712, Loss: 3.1421\n",
      "  Batch 7744, Loss: 3.2139\n",
      "  Batch 7776, Loss: 3.1823\n",
      "  Batch 7808, Loss: 3.2202\n",
      "  Batch 7840, Loss: 3.2741\n",
      "  Batch 7872, Loss: 3.3800\n",
      "  Batch 7904, Loss: 3.1341\n",
      "  Batch 7936, Loss: 3.3604\n",
      "  Batch 7968, Loss: 3.1493\n",
      "  Batch 8000, Loss: 3.3113\n",
      "  Batch 8032, Loss: 3.3381\n",
      "  Batch 8064, Loss: 3.2273\n",
      "  Batch 8096, Loss: 3.1540\n",
      "  Batch 8128, Loss: 3.2429\n",
      "  Batch 8160, Loss: 3.1847\n",
      "  Batch 8192, Loss: 3.1646\n",
      "  Batch 8224, Loss: 3.1050\n",
      "  Batch 8256, Loss: 3.1276\n",
      "  Batch 8288, Loss: 3.1974\n",
      "  Batch 8320, Loss: 3.1985\n",
      "  Batch 8352, Loss: 3.2544\n",
      "  Batch 8384, Loss: 3.1847\n",
      "  Batch 8416, Loss: 3.2484\n",
      "  Batch 8448, Loss: 3.2409\n",
      "  Batch 8480, Loss: 3.1556\n",
      "  Batch 8512, Loss: 3.2754\n",
      "  Batch 8544, Loss: 3.1855\n",
      "  Batch 8576, Loss: 3.2481\n",
      "  Batch 8608, Loss: 3.2633\n",
      "  Batch 8640, Loss: 3.1835\n",
      "  Batch 8672, Loss: 3.1813\n",
      "  Batch 8704, Loss: 3.2043\n",
      "  Batch 8736, Loss: 3.2014\n",
      "  Batch 8768, Loss: 3.4126\n",
      "  Batch 8800, Loss: 3.2176\n",
      "  Batch 8832, Loss: 3.3055\n",
      "  Batch 8864, Loss: 3.0549\n",
      "  Batch 8896, Loss: 3.3661\n",
      "  Batch 8928, Loss: 3.2650\n",
      "  Batch 8960, Loss: 3.2512\n",
      "  Batch 8992, Loss: 3.1216\n",
      "  Batch 9024, Loss: 3.3143\n",
      "  Batch 9056, Loss: 3.3016\n",
      "  Batch 9088, Loss: 3.2678\n",
      "  Batch 9120, Loss: 3.1444\n",
      "  Batch 9152, Loss: 3.3514\n",
      "  Batch 9184, Loss: 3.2501\n",
      "  Batch 9216, Loss: 3.1480\n",
      "  Batch 9248, Loss: 3.2794\n",
      "  Batch 9280, Loss: 3.2936\n",
      "  Batch 9312, Loss: 3.2330\n",
      "  Batch 9344, Loss: 3.1632\n",
      "  Batch 9376, Loss: 3.1826\n",
      "  Batch 9408, Loss: 3.1074\n",
      "  Batch 9440, Loss: 3.2952\n",
      "  Batch 9472, Loss: 3.3683\n",
      "  Batch 9504, Loss: 3.0794\n",
      "  Batch 9536, Loss: 3.2575\n",
      "  Batch 9568, Loss: 3.2274\n",
      "  Batch 9600, Loss: 3.2464\n",
      "  Batch 9632, Loss: 3.2713\n",
      "  Batch 9664, Loss: 3.2413\n",
      "  Batch 9696, Loss: 3.2250\n",
      "  Batch 9728, Loss: 3.2432\n",
      "  Batch 9760, Loss: 3.1297\n",
      "  Batch 9792, Loss: 3.2141\n",
      "  Batch 9824, Loss: 3.0897\n",
      "  Batch 9856, Loss: 3.2019\n",
      "  Batch 9888, Loss: 3.1255\n",
      "  Batch 9920, Loss: 3.2136\n",
      "  Batch 9952, Loss: 3.1778\n",
      "  Batch 9984, Loss: 3.1736\n",
      "  Batch 10016, Loss: 3.0553\n",
      "  Batch 10048, Loss: 3.0911\n",
      "  Batch 10080, Loss: 3.1712\n",
      "  Batch 10112, Loss: 3.1797\n",
      "  Batch 10144, Loss: 3.2404\n",
      "  Batch 10176, Loss: 3.0772\n",
      "  Batch 10208, Loss: 3.2064\n",
      "  Batch 10240, Loss: 3.1846\n",
      "  Batch 10272, Loss: 3.1461\n",
      "  Batch 10304, Loss: 3.2262\n",
      "  Batch 10336, Loss: 3.2324\n",
      "  Batch 10368, Loss: 3.2684\n",
      "  Batch 10400, Loss: 3.0976\n",
      "  Batch 10432, Loss: 3.1427\n",
      "  Batch 10464, Loss: 3.1519\n",
      "  Batch 10496, Loss: 3.1520\n",
      "  Batch 10528, Loss: 3.2336\n",
      "  Batch 10560, Loss: 3.2590\n",
      "  Batch 10592, Loss: 3.1451\n",
      "  Batch 10624, Loss: 3.2658\n",
      "  Batch 10656, Loss: 3.1834\n",
      "  Batch 10688, Loss: 3.1634\n",
      "  Batch 10720, Loss: 3.1437\n",
      "  Batch 10752, Loss: 2.9753\n",
      "  Batch 10784, Loss: 3.0740\n",
      "  Batch 10816, Loss: 3.1814\n",
      "  Batch 10848, Loss: 3.0851\n",
      "  Batch 10880, Loss: 3.1573\n",
      "  Batch 10912, Loss: 3.0550\n",
      "  Batch 10944, Loss: 3.2652\n",
      "  Batch 10976, Loss: 3.1010\n",
      "  Batch 11008, Loss: 3.3308\n",
      "  Batch 11040, Loss: 3.0790\n",
      "  Batch 11072, Loss: 3.1358\n",
      "  Batch 11104, Loss: 3.1217\n",
      "  Batch 11136, Loss: 3.1449\n",
      "  Batch 11168, Loss: 3.2832\n",
      "  Batch 11200, Loss: 3.2899\n",
      "  Batch 11232, Loss: 3.1549\n",
      "  Batch 11264, Loss: 3.1433\n",
      "  Batch 11296, Loss: 3.0314\n",
      "  Batch 11328, Loss: 3.1158\n",
      "  Batch 11360, Loss: 3.0227\n",
      "  Batch 11392, Loss: 3.0945\n",
      "  Batch 11424, Loss: 3.0564\n",
      "  Batch 11456, Loss: 3.3512\n",
      "  Batch 11488, Loss: 3.0648\n",
      "  Batch 11520, Loss: 3.1604\n",
      "  Batch 11552, Loss: 3.0443\n",
      "  Batch 11584, Loss: 3.0796\n",
      "  Batch 11616, Loss: 3.1184\n",
      "  Batch 11648, Loss: 3.1239\n",
      "  Batch 11680, Loss: 3.0262\n",
      "  Batch 11712, Loss: 3.0538\n",
      "  Batch 11744, Loss: 3.2165\n",
      "  Batch 11776, Loss: 3.1395\n",
      "  Batch 11808, Loss: 3.1985\n",
      "  Batch 11840, Loss: 3.1352\n",
      "  Batch 11872, Loss: 3.0945\n",
      "  Batch 11904, Loss: 3.0979\n",
      "  Batch 11936, Loss: 3.0040\n",
      "  Batch 11968, Loss: 3.1092\n",
      "  Batch 12000, Loss: 3.1553\n",
      "  Batch 12032, Loss: 3.2048\n",
      "  Batch 12064, Loss: 3.0967\n",
      "  Batch 12096, Loss: 3.1623\n",
      "  Batch 12128, Loss: 3.0853\n",
      "  Batch 12160, Loss: 3.2261\n",
      "  Batch 12192, Loss: 3.0609\n",
      "  Batch 12224, Loss: 3.2903\n",
      "  Batch 12256, Loss: 3.0553\n",
      "  Batch 12288, Loss: 3.0135\n",
      "  Batch 12320, Loss: 3.1329\n",
      "  Batch 12352, Loss: 3.0303\n",
      "  Batch 12384, Loss: 3.1692\n",
      "  Batch 12416, Loss: 3.0411\n",
      "  Batch 12448, Loss: 3.1007\n",
      "  Batch 12480, Loss: 3.0092\n",
      "  Batch 12512, Loss: 3.1196\n",
      "  Batch 12544, Loss: 3.1512\n",
      "  Batch 12576, Loss: 3.1675\n",
      "  Batch 12608, Loss: 3.1733\n",
      "  Batch 12640, Loss: 3.1782\n",
      "  Batch 12672, Loss: 3.1142\n",
      "  Batch 12704, Loss: 3.1093\n",
      "  Batch 12736, Loss: 3.0910\n",
      "  Batch 12768, Loss: 3.1773\n",
      "  Batch 12800, Loss: 3.1302\n",
      "  Batch 12832, Loss: 3.0138\n",
      "  Batch 12864, Loss: 3.2247\n",
      "  Batch 12896, Loss: 3.0701\n",
      "  Batch 12928, Loss: 2.9730\n",
      "  Batch 12960, Loss: 3.1169\n",
      "  Batch 12992, Loss: 3.0725\n",
      "  Batch 13024, Loss: 3.1423\n",
      "  Batch 13056, Loss: 3.1365\n",
      "  Batch 13088, Loss: 3.0883\n",
      "  Batch 13120, Loss: 3.1498\n",
      "  Batch 13152, Loss: 3.0387\n",
      "  Batch 13184, Loss: 3.1552\n",
      "  Batch 13216, Loss: 3.0743\n",
      "  Batch 13248, Loss: 3.0514\n",
      "  Batch 13280, Loss: 3.1999\n",
      "  Batch 13312, Loss: 3.1946\n",
      "  Batch 13344, Loss: 2.9903\n",
      "  Batch 13376, Loss: 3.0337\n",
      "  Batch 13408, Loss: 3.0023\n",
      "  Batch 13440, Loss: 3.1768\n",
      "  Batch 13472, Loss: 3.1333\n",
      "  Batch 13504, Loss: 3.1348\n",
      "  Batch 13536, Loss: 3.0513\n",
      "  Batch 13568, Loss: 3.1050\n",
      "  Batch 13600, Loss: 3.0857\n",
      "  Batch 13632, Loss: 3.0720\n",
      "  Batch 13664, Loss: 2.9886\n",
      "  Batch 13696, Loss: 3.1861\n",
      "  Batch 13728, Loss: 2.9473\n",
      "  Batch 13760, Loss: 3.1129\n",
      "  Batch 13792, Loss: 3.1101\n",
      "  Batch 13824, Loss: 2.9049\n",
      "  Batch 13856, Loss: 3.1545\n",
      "  Batch 13888, Loss: 3.1413\n",
      "  Batch 13920, Loss: 3.0648\n",
      "  Batch 13952, Loss: 3.0399\n",
      "  Batch 13984, Loss: 3.0718\n",
      "  Batch 14016, Loss: 3.1216\n",
      "  Batch 14048, Loss: 3.1269\n",
      "  Batch 14080, Loss: 3.0838\n",
      "  Batch 14112, Loss: 2.9917\n",
      "  Batch 14144, Loss: 2.9937\n",
      "  Batch 14176, Loss: 2.9303\n",
      "  Batch 14208, Loss: 3.1060\n",
      "  Batch 14240, Loss: 3.0034\n",
      "  Batch 14272, Loss: 3.1320\n",
      "  Batch 14304, Loss: 3.1097\n",
      "  Batch 14336, Loss: 3.0055\n",
      "  Batch 14368, Loss: 2.9193\n",
      "  Batch 14400, Loss: 3.0769\n",
      "  Batch 14432, Loss: 2.9975\n",
      "  Batch 14464, Loss: 3.0398\n",
      "  Batch 14496, Loss: 3.0438\n",
      "  Batch 14528, Loss: 3.0554\n",
      "  Batch 14560, Loss: 3.0251\n",
      "  Batch 14592, Loss: 3.0469\n",
      "  Batch 14624, Loss: 2.9899\n",
      "  Batch 14656, Loss: 3.0219\n",
      "  Batch 14688, Loss: 3.1700\n",
      "  Batch 14720, Loss: 3.0000\n",
      "  Batch 14752, Loss: 3.0205\n",
      "  Batch 14784, Loss: 3.0705\n",
      "  Batch 14816, Loss: 2.9469\n",
      "  Batch 14848, Loss: 3.0859\n",
      "  Batch 14880, Loss: 3.0636\n",
      "  Batch 14912, Loss: 3.0970\n",
      "  Batch 14944, Loss: 2.8893\n",
      "  Batch 14976, Loss: 3.0566\n",
      "  Batch 15008, Loss: 3.1224\n",
      "  Batch 15040, Loss: 3.1337\n",
      "  Batch 15072, Loss: 2.9941\n",
      "  Batch 15104, Loss: 3.0031\n",
      "  Batch 15136, Loss: 3.0593\n",
      "  Batch 15168, Loss: 3.0885\n",
      "  Batch 15200, Loss: 3.0073\n",
      "  Batch 15232, Loss: 3.0649\n",
      "  Batch 15264, Loss: 3.0796\n",
      "  Batch 15296, Loss: 3.1042\n",
      "  Batch 15328, Loss: 3.0345\n",
      "  Batch 15360, Loss: 3.1533\n",
      "  Batch 15392, Loss: 3.0011\n",
      "  Batch 15424, Loss: 3.0945\n",
      "  Batch 15456, Loss: 2.9441\n",
      "  Batch 15488, Loss: 3.0791\n",
      "  Batch 15520, Loss: 2.9764\n",
      "  Batch 15552, Loss: 3.0183\n",
      "  Batch 15584, Loss: 3.0576\n",
      "  Batch 15616, Loss: 2.9652\n",
      "  Batch 15648, Loss: 3.0309\n",
      "  Batch 15680, Loss: 3.0697\n",
      "  Batch 15712, Loss: 2.8690\n",
      "  Batch 15744, Loss: 3.0262\n",
      "  Batch 15776, Loss: 3.1145\n",
      "  Batch 15808, Loss: 3.0628\n",
      "  Batch 15840, Loss: 2.9850\n",
      "  Batch 15872, Loss: 3.0129\n",
      "  Batch 15904, Loss: 2.9782\n",
      "  Batch 15936, Loss: 3.1523\n",
      "  Batch 15968, Loss: 3.0120\n",
      "  Batch 16000, Loss: 3.0906\n",
      "  Batch 16032, Loss: 3.0361\n",
      "  Batch 16064, Loss: 3.0810\n",
      "  Batch 16096, Loss: 3.0462\n",
      "  Batch 16128, Loss: 3.1337\n",
      "  Batch 16160, Loss: 2.9433\n",
      "  Batch 16192, Loss: 3.2784\n",
      "  Batch 16224, Loss: 3.1003\n",
      "  Batch 16256, Loss: 3.0761\n",
      "  Batch 16288, Loss: 2.9668\n",
      "  Batch 16320, Loss: 2.8838\n",
      "  Batch 16352, Loss: 3.1185\n",
      "  Batch 16384, Loss: 2.9518\n",
      "  Batch 16416, Loss: 3.0257\n",
      "  Batch 16448, Loss: 3.0468\n",
      "  Batch 16480, Loss: 2.9463\n",
      "  Batch 16512, Loss: 3.0083\n",
      "  Batch 16544, Loss: 2.9906\n",
      "  Batch 16576, Loss: 2.9689\n",
      "  Batch 16608, Loss: 3.0095\n",
      "  Batch 16640, Loss: 2.9405\n",
      "  Batch 16672, Loss: 2.9350\n",
      "  Batch 16704, Loss: 2.9347\n",
      "  Batch 16736, Loss: 3.1244\n",
      "  Batch 16768, Loss: 2.8212\n",
      "  Batch 16800, Loss: 2.9901\n",
      "  Batch 16832, Loss: 2.9918\n",
      "  Batch 16864, Loss: 3.0179\n",
      "  Batch 16896, Loss: 2.9667\n",
      "  Batch 16928, Loss: 3.1253\n",
      "  Batch 16960, Loss: 3.0257\n",
      "  Batch 16992, Loss: 3.0386\n",
      "  Batch 17024, Loss: 2.9510\n",
      "  Batch 17056, Loss: 2.9263\n",
      "  Batch 17088, Loss: 3.1585\n",
      "  Batch 17120, Loss: 2.9775\n",
      "  Batch 17152, Loss: 2.9264\n",
      "  Batch 17184, Loss: 3.0696\n",
      "  Batch 17216, Loss: 3.0676\n",
      "  Batch 17248, Loss: 3.0176\n",
      "  Batch 17280, Loss: 3.0160\n",
      "  Batch 17312, Loss: 2.9851\n",
      "  Batch 17344, Loss: 3.0221\n",
      "  Batch 17376, Loss: 2.9675\n",
      "  Batch 17408, Loss: 3.0114\n",
      "  Batch 17440, Loss: 2.9831\n",
      "  Batch 17472, Loss: 2.9918\n",
      "  Batch 17504, Loss: 3.0655\n",
      "  Batch 17536, Loss: 3.0454\n",
      "  Batch 17568, Loss: 2.9747\n",
      "  Batch 17600, Loss: 3.0506\n",
      "  Batch 17632, Loss: 3.0811\n",
      "  Batch 17664, Loss: 3.1042\n",
      "  Batch 17696, Loss: 3.0935\n",
      "  Batch 17728, Loss: 3.0654\n",
      "  Batch 17760, Loss: 2.9326\n",
      "  Batch 17792, Loss: 2.9568\n",
      "  Batch 17824, Loss: 3.1513\n",
      "  Batch 17856, Loss: 2.9635\n",
      "  Batch 17888, Loss: 2.9490\n",
      "  Batch 17920, Loss: 2.9162\n",
      "  Batch 17952, Loss: 2.9903\n",
      "  Batch 17984, Loss: 2.9803\n",
      "  Batch 18016, Loss: 2.9652\n",
      "  Batch 18048, Loss: 2.9543\n",
      "  Batch 18080, Loss: 3.0974\n",
      "  Batch 18112, Loss: 2.9857\n",
      "  Batch 18144, Loss: 2.9953\n",
      "  Batch 18176, Loss: 2.9102\n",
      "  Batch 18208, Loss: 3.0239\n",
      "  Batch 18240, Loss: 2.9164\n",
      "  Batch 18272, Loss: 3.0031\n",
      "  Batch 18304, Loss: 2.9374\n",
      "  Batch 18336, Loss: 3.0361\n",
      "  Batch 18368, Loss: 2.9401\n",
      "  Batch 18400, Loss: 2.8643\n",
      "  Batch 18432, Loss: 2.8672\n",
      "  Batch 18464, Loss: 2.9724\n",
      "  Batch 18496, Loss: 3.0152\n",
      "  Batch 18528, Loss: 2.8963\n",
      "  Batch 18560, Loss: 2.9059\n",
      "  Batch 18592, Loss: 2.8838\n",
      "  Batch 18624, Loss: 3.0792\n",
      "  Batch 18656, Loss: 3.1030\n",
      "  Batch 18688, Loss: 2.9723\n",
      "  Batch 18720, Loss: 2.9452\n",
      "  Batch 18752, Loss: 2.8864\n",
      "  Batch 18784, Loss: 3.0013\n",
      "  Batch 18816, Loss: 2.9376\n",
      "  Batch 18848, Loss: 2.9647\n",
      "  Batch 18880, Loss: 2.8412\n",
      "  Batch 18912, Loss: 2.8728\n",
      "  Batch 18944, Loss: 2.9646\n",
      "  Batch 18976, Loss: 2.8778\n",
      "  Batch 19008, Loss: 2.8444\n",
      "  Batch 19040, Loss: 3.0134\n",
      "  Batch 19072, Loss: 2.9826\n",
      "  Batch 19104, Loss: 2.8975\n",
      "  Batch 19136, Loss: 3.0596\n",
      "  Batch 19168, Loss: 3.0237\n",
      "  Batch 19200, Loss: 3.0818\n",
      "  Batch 19232, Loss: 2.8617\n",
      "  Batch 19264, Loss: 2.9033\n",
      "  Batch 19296, Loss: 2.8196\n",
      "  Batch 19328, Loss: 2.9072\n",
      "  Batch 19360, Loss: 2.9311\n",
      "  Batch 19392, Loss: 2.9025\n",
      "  Batch 19424, Loss: 2.9375\n",
      "  Batch 19456, Loss: 2.8823\n",
      "  Batch 19488, Loss: 2.9458\n",
      "  Batch 19520, Loss: 2.9634\n",
      "  Batch 19552, Loss: 2.9190\n",
      "  Batch 19584, Loss: 2.9216\n",
      "  Batch 19616, Loss: 2.9343\n",
      "  Batch 19648, Loss: 3.0497\n",
      "  Batch 19680, Loss: 2.9090\n",
      "  Batch 19712, Loss: 2.9484\n",
      "  Batch 19744, Loss: 2.9914\n",
      "  Batch 19776, Loss: 3.0487\n",
      "  Batch 19808, Loss: 2.9319\n",
      "  Batch 19840, Loss: 2.8389\n",
      "  Batch 19872, Loss: 2.7305\n",
      "  Batch 19904, Loss: 3.0723\n",
      "  Batch 19936, Loss: 3.1052\n",
      "  Batch 19968, Loss: 2.9257\n",
      "  Batch 20000, Loss: 2.8521\n",
      "  Batch 20032, Loss: 2.8919\n",
      "  Batch 20064, Loss: 2.9652\n",
      "  Batch 20096, Loss: 3.0750\n",
      "  Batch 20128, Loss: 2.7330\n",
      "  Batch 20160, Loss: 2.8900\n",
      "  Batch 20192, Loss: 2.9508\n",
      "  Batch 20224, Loss: 2.8080\n",
      "  Batch 20256, Loss: 2.8328\n",
      "  Batch 20288, Loss: 2.8155\n",
      "  Batch 20320, Loss: 2.9046\n",
      "  Batch 20352, Loss: 2.9123\n",
      "  Batch 20384, Loss: 2.8726\n",
      "  Batch 20416, Loss: 2.9007\n",
      "  Batch 20448, Loss: 2.9145\n",
      "  Batch 20480, Loss: 2.9172\n",
      "  Batch 20512, Loss: 2.8002\n",
      "  Batch 20544, Loss: 2.8238\n",
      "  Batch 20576, Loss: 2.9534\n",
      "  Batch 20608, Loss: 3.0455\n",
      "  Batch 20640, Loss: 2.9058\n",
      "  Batch 20672, Loss: 2.7398\n",
      "  Batch 20704, Loss: 3.0066\n",
      "  Batch 20736, Loss: 2.9320\n",
      "  Batch 20768, Loss: 2.7732\n",
      "  Batch 20800, Loss: 2.9208\n",
      "  Batch 20832, Loss: 2.7944\n",
      "  Batch 20864, Loss: 2.9332\n",
      "  Batch 20896, Loss: 3.0080\n",
      "  Batch 20928, Loss: 2.9113\n",
      "  Batch 20960, Loss: 2.9979\n",
      "  Batch 20992, Loss: 2.7831\n",
      "  Batch 21024, Loss: 2.8388\n",
      "  Batch 21056, Loss: 2.8500\n",
      "  Batch 21088, Loss: 2.8460\n",
      "  Batch 21120, Loss: 2.9083\n",
      "  Batch 21152, Loss: 2.7537\n",
      "  Batch 21184, Loss: 2.9672\n",
      "  Batch 21216, Loss: 2.9115\n",
      "  Batch 21248, Loss: 2.9426\n",
      "  Batch 21280, Loss: 2.9297\n",
      "  Batch 21312, Loss: 2.8085\n",
      "  Batch 21344, Loss: 2.7689\n",
      "  Batch 21376, Loss: 2.8767\n",
      "  Batch 21408, Loss: 2.9011\n",
      "  Batch 21440, Loss: 2.7979\n",
      "  Batch 21472, Loss: 2.7998\n",
      "  Batch 21504, Loss: 2.8715\n",
      "  Batch 21536, Loss: 2.8982\n",
      "  Batch 21568, Loss: 2.8449\n",
      "  Batch 21600, Loss: 2.7601\n",
      "  Batch 21632, Loss: 2.8633\n",
      "  Batch 21664, Loss: 2.9096\n",
      "  Batch 21696, Loss: 2.8642\n",
      "  Batch 21728, Loss: 2.7954\n",
      "  Batch 21760, Loss: 2.8209\n",
      "  Batch 21792, Loss: 2.9292\n",
      "  Batch 21824, Loss: 2.8372\n",
      "  Batch 21856, Loss: 2.6894\n",
      "  Batch 21888, Loss: 2.7332\n",
      "  Batch 21920, Loss: 2.8876\n",
      "  Batch 21952, Loss: 2.8455\n",
      "  Batch 21984, Loss: 2.8527\n",
      "  Batch 22016, Loss: 2.8518\n",
      "  Batch 22048, Loss: 2.9749\n",
      "  Batch 22080, Loss: 2.8422\n",
      "  Batch 22112, Loss: 2.8726\n",
      "  Batch 22144, Loss: 2.8104\n",
      "  Batch 22176, Loss: 2.8820\n",
      "  Batch 22208, Loss: 2.7795\n",
      "  Batch 22240, Loss: 2.9241\n",
      "  Batch 22272, Loss: 2.8519\n",
      "  Batch 22304, Loss: 2.7963\n",
      "  Batch 22336, Loss: 2.8996\n",
      "  Batch 22368, Loss: 3.0301\n",
      "  Batch 22400, Loss: 2.9556\n",
      "  Batch 22432, Loss: 2.8237\n",
      "  Batch 22464, Loss: 2.8893\n",
      "  Batch 22496, Loss: 2.8204\n",
      "  Batch 22528, Loss: 2.7002\n",
      "  Batch 22560, Loss: 2.8097\n",
      "  Batch 22592, Loss: 2.7133\n",
      "  Batch 22624, Loss: 2.7173\n",
      "  Batch 22656, Loss: 2.7914\n",
      "  Batch 22688, Loss: 2.9031\n",
      "  Batch 22720, Loss: 2.8023\n",
      "  Batch 22752, Loss: 2.8264\n",
      "  Batch 22784, Loss: 2.8344\n",
      "  Batch 22816, Loss: 2.8623\n",
      "  Batch 22848, Loss: 2.8739\n",
      "  Batch 22880, Loss: 2.8979\n",
      "  Batch 22912, Loss: 2.9206\n",
      "  Batch 22944, Loss: 2.7824\n",
      "  Batch 22976, Loss: 2.8536\n",
      "  Batch 23008, Loss: 2.8383\n",
      "  Batch 23040, Loss: 2.9797\n",
      "  Batch 23072, Loss: 2.9239\n",
      "  Batch 23104, Loss: 2.8570\n",
      "  Batch 23136, Loss: 2.8261\n",
      "  Batch 23168, Loss: 2.7369\n",
      "  Batch 23200, Loss: 2.8900\n",
      "  Batch 23232, Loss: 2.8853\n",
      "  Batch 23264, Loss: 2.7794\n",
      "  Batch 23296, Loss: 2.9191\n",
      "  Batch 23328, Loss: 2.8620\n",
      "  Batch 23360, Loss: 2.9098\n",
      "  Batch 23392, Loss: 2.9490\n",
      "  Batch 23424, Loss: 2.7843\n",
      "  Batch 23456, Loss: 2.7204\n",
      "  Batch 23488, Loss: 2.8114\n",
      "  Batch 23520, Loss: 2.7232\n",
      "  Batch 23552, Loss: 2.7741\n",
      "  Batch 23584, Loss: 2.7510\n",
      "  Batch 23616, Loss: 2.7122\n",
      "  Batch 23648, Loss: 2.8260\n",
      "  Batch 23680, Loss: 2.7934\n",
      "  Batch 23712, Loss: 2.7800\n",
      "  Batch 23744, Loss: 2.8510\n",
      "  Batch 23776, Loss: 2.9366\n",
      "  Batch 23808, Loss: 2.7751\n",
      "  Batch 23840, Loss: 2.9008\n",
      "  Batch 23872, Loss: 2.8905\n",
      "  Batch 23904, Loss: 2.7678\n",
      "  Batch 23936, Loss: 2.7148\n",
      "  Batch 23968, Loss: 2.7688\n",
      "  Batch 24000, Loss: 2.8936\n",
      "  Batch 24032, Loss: 2.7899\n",
      "  Batch 24064, Loss: 2.8862\n",
      "  Batch 24096, Loss: 2.7794\n",
      "  Batch 24128, Loss: 2.8682\n",
      "  Batch 24160, Loss: 2.8328\n",
      "  Batch 24192, Loss: 2.7598\n",
      "  Batch 24224, Loss: 2.8247\n",
      "  Batch 24256, Loss: 2.7443\n",
      "  Batch 24288, Loss: 2.7453\n",
      "  Batch 24320, Loss: 2.8677\n",
      "  Batch 24352, Loss: 2.9247\n",
      "  Batch 24384, Loss: 2.7256\n",
      "  Batch 24416, Loss: 2.7766\n",
      "  Batch 24448, Loss: 2.8008\n",
      "  Batch 24480, Loss: 2.7654\n",
      "  Batch 24512, Loss: 2.8761\n",
      "  Batch 24544, Loss: 2.8017\n",
      "  Batch 24576, Loss: 2.6839\n",
      "  Batch 24608, Loss: 2.8555\n",
      "  Batch 24640, Loss: 2.8393\n",
      "  Batch 24672, Loss: 2.9834\n",
      "  Batch 24704, Loss: 2.7852\n",
      "  Batch 24736, Loss: 2.7861\n",
      "  Batch 24768, Loss: 2.7775\n",
      "  Batch 24800, Loss: 2.5878\n",
      "  Batch 24832, Loss: 2.7721\n",
      "  Batch 24864, Loss: 2.8252\n",
      "  Batch 24896, Loss: 2.8062\n",
      "  Batch 24928, Loss: 2.9668\n",
      "  Batch 24960, Loss: 2.6707\n",
      "  Batch 24992, Loss: 2.7470\n",
      "  Batch 25024, Loss: 2.8403\n",
      "  Batch 25056, Loss: 2.7202\n",
      "  Batch 25088, Loss: 2.7206\n",
      "  Batch 25120, Loss: 2.8078\n",
      "  Batch 25152, Loss: 2.7278\n",
      "  Batch 25184, Loss: 2.7614\n",
      "  Batch 25216, Loss: 2.8317\n",
      "  Batch 25248, Loss: 2.7709\n",
      "  Batch 25280, Loss: 2.7532\n",
      "  Batch 25312, Loss: 2.7694\n",
      "  Batch 25344, Loss: 2.8175\n",
      "  Batch 25376, Loss: 2.8274\n",
      "  Batch 25408, Loss: 2.6397\n",
      "  Batch 25440, Loss: 2.7171\n",
      "  Batch 25472, Loss: 2.8948\n",
      "  Batch 25504, Loss: 2.7462\n",
      "  Batch 25536, Loss: 2.8070\n",
      "  Batch 25568, Loss: 2.7744\n",
      "  Batch 25600, Loss: 2.7513\n",
      "  Batch 25632, Loss: 2.6597\n",
      "  Batch 25664, Loss: 2.7993\n",
      "  Batch 25696, Loss: 2.7327\n",
      "  Batch 25728, Loss: 2.8052\n",
      "  Batch 25760, Loss: 2.8371\n",
      "  Batch 25792, Loss: 2.7582\n",
      "  Batch 25824, Loss: 2.6823\n",
      "  Batch 25856, Loss: 2.8094\n",
      "  Batch 25888, Loss: 2.7758\n",
      "  Batch 25920, Loss: 2.7008\n",
      "  Batch 25952, Loss: 2.9908\n",
      "  Batch 25984, Loss: 2.8532\n",
      "  Batch 26016, Loss: 2.8129\n",
      "  Batch 26048, Loss: 2.6789\n",
      "  Batch 26080, Loss: 2.7907\n",
      "  Batch 26112, Loss: 2.7053\n",
      "  Batch 26144, Loss: 2.8214\n",
      "  Batch 26176, Loss: 2.6749\n",
      "  Batch 26208, Loss: 2.7441\n",
      "  Batch 26240, Loss: 2.6730\n",
      "  Batch 26272, Loss: 2.7689\n",
      "  Batch 26304, Loss: 2.7189\n",
      "  Batch 26336, Loss: 2.5907\n",
      "  Batch 26368, Loss: 2.6922\n",
      "  Batch 26400, Loss: 2.5415\n",
      "  Batch 26432, Loss: 2.6711\n",
      "  Batch 26464, Loss: 2.8164\n",
      "  Batch 26496, Loss: 2.6771\n",
      "  Batch 26528, Loss: 2.8131\n",
      "  Batch 26560, Loss: 2.7380\n",
      "  Batch 26592, Loss: 2.8121\n",
      "  Batch 26624, Loss: 2.7538\n",
      "  Batch 26656, Loss: 2.7259\n",
      "  Batch 26688, Loss: 2.5380\n",
      "  Batch 26720, Loss: 2.8688\n",
      "  Batch 26752, Loss: 2.7070\n",
      "  Batch 26784, Loss: 2.7224\n",
      "  Batch 26816, Loss: 2.7069\n",
      "  Batch 26848, Loss: 2.7087\n",
      "  Batch 26880, Loss: 2.6476\n",
      "  Batch 26912, Loss: 2.6582\n",
      "  Batch 26944, Loss: 2.7249\n",
      "  Batch 26976, Loss: 2.6756\n",
      "  Batch 27008, Loss: 2.7438\n",
      "  Batch 27040, Loss: 2.8129\n",
      "  Batch 27072, Loss: 2.6978\n",
      "  Batch 27104, Loss: 2.7861\n",
      "  Batch 27136, Loss: 2.6400\n",
      "  Batch 27168, Loss: 2.5728\n",
      "  Batch 27200, Loss: 2.7141\n",
      "  Batch 27232, Loss: 2.5842\n",
      "  Batch 27264, Loss: 2.6319\n",
      "  Batch 27296, Loss: 2.7143\n",
      "  Batch 27328, Loss: 2.6376\n",
      "  Batch 27360, Loss: 2.6991\n",
      "  Batch 27392, Loss: 2.8447\n",
      "  Batch 27424, Loss: 2.6104\n",
      "  Batch 27456, Loss: 2.6788\n",
      "  Batch 27488, Loss: 2.6659\n",
      "  Batch 27520, Loss: 2.9099\n",
      "  Batch 27552, Loss: 2.6864\n",
      "  Batch 27584, Loss: 2.7773\n",
      "  Batch 27616, Loss: 2.6930\n",
      "  Batch 27648, Loss: 2.7007\n",
      "  Batch 27680, Loss: 2.7047\n",
      "  Batch 27712, Loss: 2.5551\n",
      "  Batch 27744, Loss: 2.7572\n",
      "  Batch 27776, Loss: 2.6316\n",
      "  Batch 27808, Loss: 2.7723\n",
      "  Batch 27840, Loss: 2.7403\n",
      "  Batch 27872, Loss: 2.7856\n",
      "  Batch 27904, Loss: 2.7639\n",
      "  Batch 27936, Loss: 2.6592\n",
      "  Batch 27968, Loss: 2.6301\n",
      "  Batch 28000, Loss: 2.7296\n",
      "  Batch 28032, Loss: 2.7119\n",
      "  Batch 28064, Loss: 2.6493\n",
      "  Batch 28096, Loss: 2.6312\n",
      "  Batch 28128, Loss: 2.5588\n",
      "  Batch 28160, Loss: 2.6974\n",
      "  Batch 28192, Loss: 2.6361\n",
      "  Batch 28224, Loss: 2.5785\n",
      "  Batch 28256, Loss: 2.8634\n",
      "  Batch 28288, Loss: 2.5842\n",
      "  Batch 28320, Loss: 2.6340\n",
      "  Batch 28352, Loss: 2.6912\n",
      "  Batch 28384, Loss: 2.6022\n",
      "  Batch 28416, Loss: 2.4818\n",
      "  Batch 28448, Loss: 2.6535\n",
      "  Batch 28480, Loss: 2.6322\n",
      "  Batch 28512, Loss: 2.6396\n",
      "  Batch 28544, Loss: 2.6921\n",
      "  Batch 28576, Loss: 2.6972\n",
      "  Batch 28608, Loss: 2.5639\n",
      "  Batch 28640, Loss: 2.6629\n",
      "  Batch 28672, Loss: 2.5720\n",
      "  Batch 28704, Loss: 2.6491\n",
      "  Batch 28736, Loss: 2.6249\n",
      "  Batch 28768, Loss: 2.6052\n",
      "  Batch 28800, Loss: 2.5121\n",
      "  Batch 28832, Loss: 2.8171\n",
      "  Batch 28864, Loss: 2.6169\n",
      "  Batch 28896, Loss: 2.5524\n",
      "  Batch 28928, Loss: 2.7734\n",
      "  Batch 28960, Loss: 2.7764\n",
      "  Batch 28992, Loss: 2.5591\n",
      "  Batch 29024, Loss: 2.6821\n",
      "  Batch 29056, Loss: 2.4867\n",
      "  Batch 29088, Loss: 2.6171\n",
      "  Batch 29120, Loss: 2.7705\n",
      "  Batch 29152, Loss: 2.6977\n",
      "  Batch 29184, Loss: 2.7492\n",
      "  Batch 29216, Loss: 2.6465\n",
      "  Batch 29248, Loss: 2.7350\n",
      "  Batch 29280, Loss: 2.4592\n",
      "  Batch 29312, Loss: 2.5153\n",
      "  Batch 29344, Loss: 2.5597\n",
      "  Batch 29376, Loss: 2.7201\n",
      "  Batch 29408, Loss: 2.7232\n",
      "  Batch 29440, Loss: 2.5031\n",
      "  Batch 29472, Loss: 2.7502\n",
      "  Batch 29504, Loss: 2.6423\n",
      "  Batch 29536, Loss: 2.6344\n",
      "  Batch 29568, Loss: 2.5701\n",
      "  Batch 29600, Loss: 2.7016\n",
      "  Batch 29632, Loss: 2.4662\n",
      "  Batch 29664, Loss: 2.5346\n",
      "  Batch 29696, Loss: 2.5781\n",
      "  Batch 29728, Loss: 2.7226\n",
      "  Batch 29760, Loss: 2.7666\n",
      "  Batch 29792, Loss: 2.7115\n",
      "  Batch 29824, Loss: 2.6740\n",
      "  Batch 29856, Loss: 2.5928\n",
      "  Batch 29888, Loss: 2.6576\n",
      "  Batch 29920, Loss: 2.7663\n",
      "  Batch 29952, Loss: 2.7188\n",
      "  Batch 29984, Loss: 2.6559\n",
      "  Batch 30016, Loss: 2.6783\n",
      "  Batch 30048, Loss: 2.5911\n",
      "  Batch 30080, Loss: 2.7194\n",
      "  Batch 30112, Loss: 2.7324\n",
      "  Batch 30144, Loss: 2.6137\n",
      "  Batch 30176, Loss: 2.5868\n",
      "  Batch 30208, Loss: 2.6740\n",
      "  Batch 30240, Loss: 2.6803\n",
      "  Batch 30272, Loss: 2.5738\n",
      "  Batch 30304, Loss: 2.5608\n",
      "  Batch 30336, Loss: 2.6054\n",
      "  Batch 30368, Loss: 2.5938\n",
      "  Batch 30400, Loss: 2.5657\n",
      "  Batch 30432, Loss: 2.6608\n",
      "  Batch 30464, Loss: 2.4981\n",
      "  Batch 30496, Loss: 2.5552\n",
      "  Batch 30528, Loss: 2.6589\n",
      "  Batch 30560, Loss: 2.4464\n",
      "  Batch 30592, Loss: 2.5506\n",
      "  Batch 30624, Loss: 2.5314\n",
      "  Batch 30656, Loss: 2.7386\n",
      "  Batch 30688, Loss: 2.4905\n",
      "  Batch 30720, Loss: 2.5537\n",
      "  Batch 30752, Loss: 2.6315\n",
      "  Batch 30784, Loss: 2.5437\n",
      "  Batch 30816, Loss: 2.6306\n",
      "  Batch 30848, Loss: 2.6632\n",
      "  Batch 30880, Loss: 2.5832\n",
      "  Batch 30912, Loss: 2.7186\n",
      "  Batch 30944, Loss: 2.6706\n",
      "  Batch 30976, Loss: 2.6184\n",
      "  Batch 31008, Loss: 2.5705\n",
      "  Batch 31040, Loss: 2.6080\n",
      "  Batch 31072, Loss: 2.6541\n",
      "  Batch 31104, Loss: 2.4871\n",
      "  Batch 31136, Loss: 2.6392\n",
      "  Batch 31168, Loss: 2.5070\n",
      "  Batch 31200, Loss: 2.6033\n",
      "  Batch 31232, Loss: 2.7033\n",
      "  Batch 31264, Loss: 2.6235\n",
      "  Batch 31296, Loss: 2.6020\n",
      "  Batch 31328, Loss: 2.5458\n",
      "  Batch 31360, Loss: 2.6102\n",
      "  Batch 31392, Loss: 2.5644\n",
      "  Batch 31424, Loss: 2.6719\n",
      "  Batch 31456, Loss: 2.6638\n",
      "  Batch 31488, Loss: 2.4395\n",
      "  Batch 31520, Loss: 2.4660\n",
      "  Batch 31552, Loss: 2.5967\n",
      "  Batch 31584, Loss: 2.6245\n",
      "  Batch 31616, Loss: 2.5190\n",
      "  Batch 31648, Loss: 2.5388\n",
      "  Batch 31680, Loss: 2.6582\n",
      "  Batch 31712, Loss: 2.5526\n",
      "  Batch 31744, Loss: 2.5805\n",
      "  Batch 31776, Loss: 2.6061\n",
      "  Batch 31808, Loss: 2.6482\n",
      "  Batch 31840, Loss: 2.5961\n",
      "  Batch 31872, Loss: 2.6392\n",
      "  Batch 31904, Loss: 2.5087\n",
      "  Batch 31936, Loss: 2.5024\n",
      "  Batch 31968, Loss: 2.5317\n",
      "  Batch 32000, Loss: 2.6059\n",
      "  Batch 32032, Loss: 2.5621\n",
      "  Batch 32064, Loss: 2.5773\n",
      "  Batch 32096, Loss: 2.5602\n",
      "  Batch 32128, Loss: 2.6378\n",
      "  Batch 32160, Loss: 2.6590\n",
      "  Batch 32192, Loss: 2.4959\n",
      "  Batch 32224, Loss: 2.6922\n",
      "  Batch 32256, Loss: 2.6053\n",
      "  Batch 32288, Loss: 2.5374\n",
      "  Batch 32320, Loss: 2.5143\n",
      "  Batch 32352, Loss: 2.5878\n",
      "  Batch 32384, Loss: 2.5627\n",
      "  Batch 32416, Loss: 2.6338\n",
      "  Batch 32448, Loss: 2.6122\n",
      "  Batch 32480, Loss: 2.7052\n",
      "  Batch 32512, Loss: 2.5530\n",
      "  Batch 32544, Loss: 2.5797\n",
      "  Batch 32576, Loss: 2.4892\n",
      "  Batch 32608, Loss: 2.5191\n",
      "  Batch 32640, Loss: 2.5445\n",
      "  Batch 32672, Loss: 2.5227\n",
      "  Batch 32704, Loss: 2.5365\n",
      "  Batch 32736, Loss: 2.4375\n",
      "  Batch 32768, Loss: 2.5826\n",
      "  Batch 32800, Loss: 2.4876\n",
      "  Batch 32832, Loss: 2.6668\n",
      "  Batch 32864, Loss: 2.4308\n",
      "  Batch 32896, Loss: 2.5936\n",
      "  Batch 32928, Loss: 2.6081\n",
      "  Batch 32960, Loss: 2.6607\n",
      "  Batch 32992, Loss: 2.5037\n",
      "  Batch 33024, Loss: 2.4583\n",
      "  Batch 33056, Loss: 2.5154\n",
      "  Batch 33088, Loss: 2.6515\n",
      "  Batch 33120, Loss: 2.3873\n",
      "  Batch 33152, Loss: 2.5457\n",
      "  Batch 33184, Loss: 2.5113\n",
      "  Batch 33216, Loss: 2.5176\n",
      "  Batch 33248, Loss: 2.5510\n",
      "  Batch 33280, Loss: 2.6401\n",
      "  Batch 33312, Loss: 2.4952\n",
      "  Batch 33344, Loss: 2.7507\n",
      "  Batch 33376, Loss: 2.4977\n",
      "  Batch 33408, Loss: 2.4906\n",
      "  Batch 33440, Loss: 2.6450\n",
      "  Batch 33472, Loss: 2.5921\n",
      "  Batch 33504, Loss: 2.5185\n",
      "  Batch 33536, Loss: 2.5414\n",
      "  Batch 33568, Loss: 2.5551\n",
      "  Batch 33600, Loss: 2.6746\n",
      "  Batch 33632, Loss: 2.5453\n",
      "  Batch 33664, Loss: 2.4215\n",
      "  Batch 33696, Loss: 2.6552\n",
      "  Batch 33728, Loss: 2.5176\n",
      "  Batch 33760, Loss: 2.5256\n",
      "  Batch 33792, Loss: 2.3898\n",
      "Epoch 2/3, Loss: 2.9891\n",
      "  Batch 0, Loss: 2.3996\n",
      "  Batch 32, Loss: 2.5031\n",
      "  Batch 64, Loss: 2.4614\n",
      "  Batch 96, Loss: 2.4485\n",
      "  Batch 128, Loss: 2.5193\n",
      "  Batch 160, Loss: 2.5003\n",
      "  Batch 192, Loss: 2.5901\n",
      "  Batch 224, Loss: 2.4142\n",
      "  Batch 256, Loss: 2.4415\n",
      "  Batch 288, Loss: 2.5036\n",
      "  Batch 320, Loss: 2.4615\n",
      "  Batch 352, Loss: 2.3942\n",
      "  Batch 384, Loss: 2.5717\n",
      "  Batch 416, Loss: 2.4216\n",
      "  Batch 448, Loss: 2.5094\n",
      "  Batch 480, Loss: 2.5238\n",
      "  Batch 512, Loss: 2.5290\n",
      "  Batch 544, Loss: 2.4983\n",
      "  Batch 576, Loss: 2.5624\n",
      "  Batch 608, Loss: 2.4489\n",
      "  Batch 640, Loss: 2.5105\n",
      "  Batch 672, Loss: 2.5682\n",
      "  Batch 704, Loss: 2.4688\n",
      "  Batch 736, Loss: 2.6378\n",
      "  Batch 768, Loss: 2.5185\n",
      "  Batch 800, Loss: 2.6880\n",
      "  Batch 832, Loss: 2.6499\n",
      "  Batch 864, Loss: 2.6538\n",
      "  Batch 896, Loss: 2.4827\n",
      "  Batch 928, Loss: 2.4761\n",
      "  Batch 960, Loss: 2.3144\n",
      "  Batch 992, Loss: 2.6569\n",
      "  Batch 1024, Loss: 2.4050\n",
      "  Batch 1056, Loss: 2.5397\n",
      "  Batch 1088, Loss: 2.3349\n",
      "  Batch 1120, Loss: 2.6693\n",
      "  Batch 1152, Loss: 2.5303\n",
      "  Batch 1184, Loss: 2.3701\n",
      "  Batch 1216, Loss: 2.4086\n",
      "  Batch 1248, Loss: 2.4975\n",
      "  Batch 1280, Loss: 2.4909\n",
      "  Batch 1312, Loss: 2.5287\n",
      "  Batch 1344, Loss: 2.5144\n",
      "  Batch 1376, Loss: 2.5169\n",
      "  Batch 1408, Loss: 2.4322\n",
      "  Batch 1440, Loss: 2.5369\n",
      "  Batch 1472, Loss: 2.4203\n",
      "  Batch 1504, Loss: 2.5552\n",
      "  Batch 1536, Loss: 2.6901\n",
      "  Batch 1568, Loss: 2.5105\n",
      "  Batch 1600, Loss: 2.4487\n",
      "  Batch 1632, Loss: 2.4407\n",
      "  Batch 1664, Loss: 2.3905\n",
      "  Batch 1696, Loss: 2.5549\n",
      "  Batch 1728, Loss: 2.5775\n",
      "  Batch 1760, Loss: 2.4820\n",
      "  Batch 1792, Loss: 2.4987\n",
      "  Batch 1824, Loss: 2.4279\n",
      "  Batch 1856, Loss: 2.4133\n",
      "  Batch 1888, Loss: 2.5819\n",
      "  Batch 1920, Loss: 2.3841\n",
      "  Batch 1952, Loss: 2.6716\n",
      "  Batch 1984, Loss: 2.5579\n",
      "  Batch 2016, Loss: 2.4464\n",
      "  Batch 2048, Loss: 2.4565\n",
      "  Batch 2080, Loss: 2.6286\n",
      "  Batch 2112, Loss: 2.4950\n",
      "  Batch 2144, Loss: 2.4881\n",
      "  Batch 2176, Loss: 2.5192\n",
      "  Batch 2208, Loss: 2.5284\n",
      "  Batch 2240, Loss: 2.5067\n",
      "  Batch 2272, Loss: 2.4509\n",
      "  Batch 2304, Loss: 2.4195\n",
      "  Batch 2336, Loss: 2.3264\n",
      "  Batch 2368, Loss: 2.3984\n",
      "  Batch 2400, Loss: 2.5043\n",
      "  Batch 2432, Loss: 2.3799\n",
      "  Batch 2464, Loss: 2.3633\n",
      "  Batch 2496, Loss: 2.4494\n",
      "  Batch 2528, Loss: 2.3990\n",
      "  Batch 2560, Loss: 2.4421\n",
      "  Batch 2592, Loss: 2.5733\n",
      "  Batch 2624, Loss: 2.4963\n",
      "  Batch 2656, Loss: 2.3915\n",
      "  Batch 2688, Loss: 2.4106\n",
      "  Batch 2720, Loss: 2.4957\n",
      "  Batch 2752, Loss: 2.5431\n",
      "  Batch 2784, Loss: 2.4744\n",
      "  Batch 2816, Loss: 2.6278\n",
      "  Batch 2848, Loss: 2.4989\n",
      "  Batch 2880, Loss: 2.4751\n",
      "  Batch 2912, Loss: 2.6422\n",
      "  Batch 2944, Loss: 2.5035\n",
      "  Batch 2976, Loss: 2.4229\n",
      "  Batch 3008, Loss: 2.4908\n",
      "  Batch 3040, Loss: 2.4412\n",
      "  Batch 3072, Loss: 2.3780\n",
      "  Batch 3104, Loss: 2.4495\n",
      "  Batch 3136, Loss: 2.4882\n",
      "  Batch 3168, Loss: 2.4194\n",
      "  Batch 3200, Loss: 2.3690\n",
      "  Batch 3232, Loss: 2.4164\n",
      "  Batch 3264, Loss: 2.4818\n",
      "  Batch 3296, Loss: 2.4409\n",
      "  Batch 3328, Loss: 2.4717\n",
      "  Batch 3360, Loss: 2.4201\n",
      "  Batch 3392, Loss: 2.3583\n",
      "  Batch 3424, Loss: 2.4332\n",
      "  Batch 3456, Loss: 2.4169\n",
      "  Batch 3488, Loss: 2.5087\n",
      "  Batch 3520, Loss: 2.4617\n",
      "  Batch 3552, Loss: 2.4828\n",
      "  Batch 3584, Loss: 2.5222\n",
      "  Batch 3616, Loss: 2.4837\n",
      "  Batch 3648, Loss: 2.3740\n",
      "  Batch 3680, Loss: 2.4507\n",
      "  Batch 3712, Loss: 2.3970\n",
      "  Batch 3744, Loss: 2.4562\n",
      "  Batch 3776, Loss: 2.4690\n",
      "  Batch 3808, Loss: 2.3329\n",
      "  Batch 3840, Loss: 2.3673\n",
      "  Batch 3872, Loss: 2.4375\n",
      "  Batch 3904, Loss: 2.4201\n",
      "  Batch 3936, Loss: 2.5907\n",
      "  Batch 3968, Loss: 2.4491\n",
      "  Batch 4000, Loss: 2.4569\n",
      "  Batch 4032, Loss: 2.4742\n",
      "  Batch 4064, Loss: 2.5344\n",
      "  Batch 4096, Loss: 2.3376\n",
      "  Batch 4128, Loss: 2.3852\n",
      "  Batch 4160, Loss: 2.2618\n",
      "  Batch 4192, Loss: 2.5862\n",
      "  Batch 4224, Loss: 2.4812\n",
      "  Batch 4256, Loss: 2.3338\n",
      "  Batch 4288, Loss: 2.3901\n",
      "  Batch 4320, Loss: 2.3168\n",
      "  Batch 4352, Loss: 2.3013\n",
      "  Batch 4384, Loss: 2.4027\n",
      "  Batch 4416, Loss: 2.3839\n",
      "  Batch 4448, Loss: 2.3475\n",
      "  Batch 4480, Loss: 2.6554\n",
      "  Batch 4512, Loss: 2.4004\n",
      "  Batch 4544, Loss: 2.4160\n",
      "  Batch 4576, Loss: 2.4701\n",
      "  Batch 4608, Loss: 2.4402\n",
      "  Batch 4640, Loss: 2.3586\n",
      "  Batch 4672, Loss: 2.4146\n",
      "  Batch 4704, Loss: 2.4450\n",
      "  Batch 4736, Loss: 2.4130\n",
      "  Batch 4768, Loss: 2.4978\n",
      "  Batch 4800, Loss: 2.3847\n",
      "  Batch 4832, Loss: 2.5445\n",
      "  Batch 4864, Loss: 2.3008\n",
      "  Batch 4896, Loss: 2.3964\n",
      "  Batch 4928, Loss: 2.4325\n",
      "  Batch 4960, Loss: 2.4265\n",
      "  Batch 4992, Loss: 2.4276\n",
      "  Batch 5024, Loss: 2.4223\n",
      "  Batch 5056, Loss: 2.3924\n",
      "  Batch 5088, Loss: 2.3539\n",
      "  Batch 5120, Loss: 2.3716\n",
      "  Batch 5152, Loss: 2.4589\n",
      "  Batch 5184, Loss: 2.5843\n",
      "  Batch 5216, Loss: 2.5551\n",
      "  Batch 5248, Loss: 2.5841\n",
      "  Batch 5280, Loss: 2.4469\n",
      "  Batch 5312, Loss: 2.4100\n",
      "  Batch 5344, Loss: 2.3868\n",
      "  Batch 5376, Loss: 2.3576\n",
      "  Batch 5408, Loss: 2.5092\n",
      "  Batch 5440, Loss: 2.3485\n",
      "  Batch 5472, Loss: 2.3569\n",
      "  Batch 5504, Loss: 2.3387\n",
      "  Batch 5536, Loss: 2.4612\n",
      "  Batch 5568, Loss: 2.3435\n",
      "  Batch 5600, Loss: 2.3739\n",
      "  Batch 5632, Loss: 2.4097\n",
      "  Batch 5664, Loss: 2.3769\n",
      "  Batch 5696, Loss: 2.4469\n",
      "  Batch 5728, Loss: 2.5385\n",
      "  Batch 5760, Loss: 2.4681\n",
      "  Batch 5792, Loss: 2.3691\n",
      "  Batch 5824, Loss: 2.5533\n",
      "  Batch 5856, Loss: 2.4457\n",
      "  Batch 5888, Loss: 2.3522\n",
      "  Batch 5920, Loss: 2.5042\n",
      "  Batch 5952, Loss: 2.4298\n",
      "  Batch 5984, Loss: 2.4225\n",
      "  Batch 6016, Loss: 2.2863\n",
      "  Batch 6048, Loss: 2.3899\n",
      "  Batch 6080, Loss: 2.4864\n",
      "  Batch 6112, Loss: 2.3480\n",
      "  Batch 6144, Loss: 2.3842\n",
      "  Batch 6176, Loss: 2.4164\n",
      "  Batch 6208, Loss: 2.3526\n",
      "  Batch 6240, Loss: 2.2278\n",
      "  Batch 6272, Loss: 2.4530\n",
      "  Batch 6304, Loss: 2.2538\n",
      "  Batch 6336, Loss: 2.3891\n",
      "  Batch 6368, Loss: 2.2862\n",
      "  Batch 6400, Loss: 2.3999\n",
      "  Batch 6432, Loss: 2.3967\n",
      "  Batch 6464, Loss: 2.3528\n",
      "  Batch 6496, Loss: 2.6043\n",
      "  Batch 6528, Loss: 2.3687\n",
      "  Batch 6560, Loss: 2.3343\n",
      "  Batch 6592, Loss: 2.3055\n",
      "  Batch 6624, Loss: 2.4167\n",
      "  Batch 6656, Loss: 2.4212\n",
      "  Batch 6688, Loss: 2.4805\n",
      "  Batch 6720, Loss: 2.5401\n",
      "  Batch 6752, Loss: 2.5094\n",
      "  Batch 6784, Loss: 2.4660\n",
      "  Batch 6816, Loss: 2.3435\n",
      "  Batch 6848, Loss: 2.3201\n",
      "  Batch 6880, Loss: 2.2816\n",
      "  Batch 6912, Loss: 2.4062\n",
      "  Batch 6944, Loss: 2.2739\n",
      "  Batch 6976, Loss: 2.3384\n",
      "  Batch 7008, Loss: 2.4693\n",
      "  Batch 7040, Loss: 2.5242\n",
      "  Batch 7072, Loss: 2.3695\n",
      "  Batch 7104, Loss: 2.3352\n",
      "  Batch 7136, Loss: 2.2879\n",
      "  Batch 7168, Loss: 2.3369\n",
      "  Batch 7200, Loss: 2.3185\n",
      "  Batch 7232, Loss: 2.3344\n",
      "  Batch 7264, Loss: 2.3534\n",
      "  Batch 7296, Loss: 2.4844\n",
      "  Batch 7328, Loss: 2.2701\n",
      "  Batch 7360, Loss: 2.3748\n",
      "  Batch 7392, Loss: 2.2835\n",
      "  Batch 7424, Loss: 2.4534\n",
      "  Batch 7456, Loss: 2.4300\n",
      "  Batch 7488, Loss: 2.4493\n",
      "  Batch 7520, Loss: 2.2883\n",
      "  Batch 7552, Loss: 2.3534\n",
      "  Batch 7584, Loss: 2.3184\n",
      "  Batch 7616, Loss: 2.4509\n",
      "  Batch 7648, Loss: 2.2295\n",
      "  Batch 7680, Loss: 2.4001\n",
      "  Batch 7712, Loss: 2.1764\n",
      "  Batch 7744, Loss: 2.5001\n",
      "  Batch 7776, Loss: 2.4510\n",
      "  Batch 7808, Loss: 2.4289\n",
      "  Batch 7840, Loss: 2.4065\n",
      "  Batch 7872, Loss: 2.3467\n",
      "  Batch 7904, Loss: 2.3304\n",
      "  Batch 7936, Loss: 2.2119\n",
      "  Batch 7968, Loss: 2.3769\n",
      "  Batch 8000, Loss: 2.3081\n",
      "  Batch 8032, Loss: 2.2398\n",
      "  Batch 8064, Loss: 2.3028\n",
      "  Batch 8096, Loss: 2.3421\n",
      "  Batch 8128, Loss: 2.2470\n",
      "  Batch 8160, Loss: 2.2405\n",
      "  Batch 8192, Loss: 2.4433\n",
      "  Batch 8224, Loss: 2.3900\n",
      "  Batch 8256, Loss: 2.2667\n",
      "  Batch 8288, Loss: 2.4786\n",
      "  Batch 8320, Loss: 2.4954\n",
      "  Batch 8352, Loss: 2.4055\n",
      "  Batch 8384, Loss: 2.4157\n",
      "  Batch 8416, Loss: 2.4603\n",
      "  Batch 8448, Loss: 2.3972\n",
      "  Batch 8480, Loss: 2.4432\n",
      "  Batch 8512, Loss: 2.3503\n",
      "  Batch 8544, Loss: 2.3497\n",
      "  Batch 8576, Loss: 2.3002\n",
      "  Batch 8608, Loss: 2.5175\n",
      "  Batch 8640, Loss: 2.4915\n",
      "  Batch 8672, Loss: 2.2938\n",
      "  Batch 8704, Loss: 2.3762\n",
      "  Batch 8736, Loss: 2.3527\n",
      "  Batch 8768, Loss: 2.2201\n",
      "  Batch 8800, Loss: 2.3318\n",
      "  Batch 8832, Loss: 2.3451\n",
      "  Batch 8864, Loss: 2.3705\n",
      "  Batch 8896, Loss: 2.3763\n",
      "  Batch 8928, Loss: 2.3793\n",
      "  Batch 8960, Loss: 2.3026\n",
      "  Batch 8992, Loss: 2.3563\n",
      "  Batch 9024, Loss: 2.4149\n",
      "  Batch 9056, Loss: 2.3681\n",
      "  Batch 9088, Loss: 2.3843\n",
      "  Batch 9120, Loss: 2.2863\n",
      "  Batch 9152, Loss: 2.3170\n",
      "  Batch 9184, Loss: 2.2838\n",
      "  Batch 9216, Loss: 2.2815\n",
      "  Batch 9248, Loss: 2.4417\n",
      "  Batch 9280, Loss: 2.3367\n",
      "  Batch 9312, Loss: 2.4067\n",
      "  Batch 9344, Loss: 2.3201\n",
      "  Batch 9376, Loss: 2.3404\n",
      "  Batch 9408, Loss: 2.3563\n",
      "  Batch 9440, Loss: 2.2677\n",
      "  Batch 9472, Loss: 2.2333\n",
      "  Batch 9504, Loss: 2.2759\n",
      "  Batch 9536, Loss: 2.3418\n",
      "  Batch 9568, Loss: 2.2499\n",
      "  Batch 9600, Loss: 2.2970\n",
      "  Batch 9632, Loss: 2.3484\n",
      "  Batch 9664, Loss: 2.2689\n",
      "  Batch 9696, Loss: 2.2345\n",
      "  Batch 9728, Loss: 2.2563\n",
      "  Batch 9760, Loss: 2.3421\n",
      "  Batch 9792, Loss: 2.3928\n",
      "  Batch 9824, Loss: 2.2328\n",
      "  Batch 9856, Loss: 2.2655\n",
      "  Batch 9888, Loss: 2.2767\n",
      "  Batch 9920, Loss: 2.4332\n",
      "  Batch 9952, Loss: 2.3341\n",
      "  Batch 9984, Loss: 2.3341\n",
      "  Batch 10016, Loss: 2.2974\n",
      "  Batch 10048, Loss: 2.3549\n",
      "  Batch 10080, Loss: 2.4749\n",
      "  Batch 10112, Loss: 2.2458\n",
      "  Batch 10144, Loss: 2.2940\n",
      "  Batch 10176, Loss: 2.3521\n",
      "  Batch 10208, Loss: 2.2701\n",
      "  Batch 10240, Loss: 2.2147\n",
      "  Batch 10272, Loss: 2.3180\n",
      "  Batch 10304, Loss: 2.4005\n",
      "  Batch 10336, Loss: 2.2947\n",
      "  Batch 10368, Loss: 2.2309\n",
      "  Batch 10400, Loss: 2.3300\n",
      "  Batch 10432, Loss: 2.3584\n",
      "  Batch 10464, Loss: 2.3613\n",
      "  Batch 10496, Loss: 2.3970\n",
      "  Batch 10528, Loss: 2.3476\n",
      "  Batch 10560, Loss: 2.3876\n",
      "  Batch 10592, Loss: 2.3188\n",
      "  Batch 10624, Loss: 2.3393\n",
      "  Batch 10656, Loss: 2.2911\n",
      "  Batch 10688, Loss: 2.3718\n",
      "  Batch 10720, Loss: 2.2595\n",
      "  Batch 10752, Loss: 2.5000\n",
      "  Batch 10784, Loss: 2.2433\n",
      "  Batch 10816, Loss: 2.3633\n",
      "  Batch 10848, Loss: 2.4274\n",
      "  Batch 10880, Loss: 2.3018\n",
      "  Batch 10912, Loss: 2.4609\n",
      "  Batch 10944, Loss: 2.4110\n",
      "  Batch 10976, Loss: 2.2694\n",
      "  Batch 11008, Loss: 2.2525\n",
      "  Batch 11040, Loss: 2.4466\n",
      "  Batch 11072, Loss: 2.3070\n",
      "  Batch 11104, Loss: 2.3327\n",
      "  Batch 11136, Loss: 2.3513\n",
      "  Batch 11168, Loss: 2.4560\n",
      "  Batch 11200, Loss: 2.3437\n",
      "  Batch 11232, Loss: 2.3945\n",
      "  Batch 11264, Loss: 2.3833\n",
      "  Batch 11296, Loss: 2.2503\n",
      "  Batch 11328, Loss: 2.1614\n",
      "  Batch 11360, Loss: 2.3956\n",
      "  Batch 11392, Loss: 2.3088\n",
      "  Batch 11424, Loss: 2.2588\n",
      "  Batch 11456, Loss: 2.3112\n",
      "  Batch 11488, Loss: 2.3349\n",
      "  Batch 11520, Loss: 2.2376\n",
      "  Batch 11552, Loss: 2.2966\n",
      "  Batch 11584, Loss: 2.3066\n",
      "  Batch 11616, Loss: 2.4440\n",
      "  Batch 11648, Loss: 2.2471\n",
      "  Batch 11680, Loss: 2.3332\n",
      "  Batch 11712, Loss: 2.3344\n",
      "  Batch 11744, Loss: 2.2671\n",
      "  Batch 11776, Loss: 2.2203\n",
      "  Batch 11808, Loss: 2.2857\n",
      "  Batch 11840, Loss: 2.3146\n",
      "  Batch 11872, Loss: 2.3878\n",
      "  Batch 11904, Loss: 2.3489\n",
      "  Batch 11936, Loss: 2.3175\n",
      "  Batch 11968, Loss: 2.3164\n",
      "  Batch 12000, Loss: 2.2046\n",
      "  Batch 12032, Loss: 2.2779\n",
      "  Batch 12064, Loss: 2.5765\n",
      "  Batch 12096, Loss: 2.2243\n",
      "  Batch 12128, Loss: 2.3514\n",
      "  Batch 12160, Loss: 2.3794\n",
      "  Batch 12192, Loss: 2.4099\n",
      "  Batch 12224, Loss: 2.1836\n",
      "  Batch 12256, Loss: 2.3094\n",
      "  Batch 12288, Loss: 2.3237\n",
      "  Batch 12320, Loss: 2.2365\n",
      "  Batch 12352, Loss: 2.2552\n",
      "  Batch 12384, Loss: 2.4091\n",
      "  Batch 12416, Loss: 2.3785\n",
      "  Batch 12448, Loss: 2.2271\n",
      "  Batch 12480, Loss: 2.4065\n",
      "  Batch 12512, Loss: 2.3161\n",
      "  Batch 12544, Loss: 2.2858\n",
      "  Batch 12576, Loss: 2.2211\n",
      "  Batch 12608, Loss: 2.3623\n",
      "  Batch 12640, Loss: 2.3251\n",
      "  Batch 12672, Loss: 2.3458\n",
      "  Batch 12704, Loss: 2.1612\n",
      "  Batch 12736, Loss: 2.2693\n",
      "  Batch 12768, Loss: 2.2009\n",
      "  Batch 12800, Loss: 2.2847\n",
      "  Batch 12832, Loss: 2.5156\n",
      "  Batch 12864, Loss: 2.3207\n",
      "  Batch 12896, Loss: 2.2564\n",
      "  Batch 12928, Loss: 2.2392\n",
      "  Batch 12960, Loss: 2.2978\n",
      "  Batch 12992, Loss: 2.3212\n",
      "  Batch 13024, Loss: 2.2883\n",
      "  Batch 13056, Loss: 2.1358\n",
      "  Batch 13088, Loss: 2.3135\n",
      "  Batch 13120, Loss: 2.1996\n",
      "  Batch 13152, Loss: 2.3450\n",
      "  Batch 13184, Loss: 2.2971\n",
      "  Batch 13216, Loss: 2.3138\n",
      "  Batch 13248, Loss: 2.2386\n",
      "  Batch 13280, Loss: 2.3606\n",
      "  Batch 13312, Loss: 2.4125\n",
      "  Batch 13344, Loss: 2.3116\n",
      "  Batch 13376, Loss: 2.2711\n",
      "  Batch 13408, Loss: 2.3400\n",
      "  Batch 13440, Loss: 2.3607\n",
      "  Batch 13472, Loss: 2.2687\n",
      "  Batch 13504, Loss: 2.2699\n",
      "  Batch 13536, Loss: 2.3278\n",
      "  Batch 13568, Loss: 2.2949\n",
      "  Batch 13600, Loss: 2.2970\n",
      "  Batch 13632, Loss: 2.3746\n",
      "  Batch 13664, Loss: 2.3135\n",
      "  Batch 13696, Loss: 2.2105\n",
      "  Batch 13728, Loss: 2.3751\n",
      "  Batch 13760, Loss: 2.3365\n",
      "  Batch 13792, Loss: 2.3063\n",
      "  Batch 13824, Loss: 2.3090\n",
      "  Batch 13856, Loss: 2.3855\n",
      "  Batch 13888, Loss: 2.1856\n",
      "  Batch 13920, Loss: 2.3745\n",
      "  Batch 13952, Loss: 2.3151\n",
      "  Batch 13984, Loss: 2.2701\n",
      "  Batch 14016, Loss: 2.1740\n",
      "  Batch 14048, Loss: 2.1810\n",
      "  Batch 14080, Loss: 2.2371\n",
      "  Batch 14112, Loss: 2.2843\n",
      "  Batch 14144, Loss: 2.3974\n",
      "  Batch 14176, Loss: 2.3395\n",
      "  Batch 14208, Loss: 2.2181\n",
      "  Batch 14240, Loss: 2.2767\n",
      "  Batch 14272, Loss: 2.1978\n",
      "  Batch 14304, Loss: 2.3140\n",
      "  Batch 14336, Loss: 2.2636\n",
      "  Batch 14368, Loss: 2.3072\n",
      "  Batch 14400, Loss: 2.3717\n",
      "  Batch 14432, Loss: 2.1548\n",
      "  Batch 14464, Loss: 2.2583\n",
      "  Batch 14496, Loss: 2.2786\n",
      "  Batch 14528, Loss: 2.3658\n",
      "  Batch 14560, Loss: 2.3010\n",
      "  Batch 14592, Loss: 2.3037\n",
      "  Batch 14624, Loss: 2.4465\n",
      "  Batch 14656, Loss: 2.2618\n",
      "  Batch 14688, Loss: 2.2394\n",
      "  Batch 14720, Loss: 2.4118\n",
      "  Batch 14752, Loss: 2.1498\n",
      "  Batch 14784, Loss: 2.2620\n",
      "  Batch 14816, Loss: 2.3568\n",
      "  Batch 14848, Loss: 2.1935\n",
      "  Batch 14880, Loss: 2.1726\n",
      "  Batch 14912, Loss: 2.5042\n",
      "  Batch 14944, Loss: 2.2155\n",
      "  Batch 14976, Loss: 2.1192\n",
      "  Batch 15008, Loss: 2.2541\n",
      "  Batch 15040, Loss: 2.3362\n",
      "  Batch 15072, Loss: 2.2598\n",
      "  Batch 15104, Loss: 2.2181\n",
      "  Batch 15136, Loss: 2.2754\n",
      "  Batch 15168, Loss: 2.0854\n",
      "  Batch 15200, Loss: 2.1900\n",
      "  Batch 15232, Loss: 2.2168\n",
      "  Batch 15264, Loss: 2.2832\n",
      "  Batch 15296, Loss: 2.1973\n",
      "  Batch 15328, Loss: 2.4816\n",
      "  Batch 15360, Loss: 2.2058\n",
      "  Batch 15392, Loss: 2.3190\n",
      "  Batch 15424, Loss: 2.2163\n",
      "  Batch 15456, Loss: 2.1687\n",
      "  Batch 15488, Loss: 2.3145\n",
      "  Batch 15520, Loss: 2.3033\n",
      "  Batch 15552, Loss: 2.2607\n",
      "  Batch 15584, Loss: 2.3518\n",
      "  Batch 15616, Loss: 2.2532\n",
      "  Batch 15648, Loss: 2.2421\n",
      "  Batch 15680, Loss: 2.2850\n",
      "  Batch 15712, Loss: 2.0371\n",
      "  Batch 15744, Loss: 2.2982\n",
      "  Batch 15776, Loss: 2.2469\n",
      "  Batch 15808, Loss: 2.1603\n",
      "  Batch 15840, Loss: 2.1980\n",
      "  Batch 15872, Loss: 2.2944\n",
      "  Batch 15904, Loss: 2.2844\n",
      "  Batch 15936, Loss: 2.1906\n",
      "  Batch 15968, Loss: 2.1516\n",
      "  Batch 16000, Loss: 2.3378\n",
      "  Batch 16032, Loss: 2.3189\n",
      "  Batch 16064, Loss: 2.2454\n",
      "  Batch 16096, Loss: 2.2397\n",
      "  Batch 16128, Loss: 2.3375\n",
      "  Batch 16160, Loss: 2.3028\n",
      "  Batch 16192, Loss: 2.2088\n",
      "  Batch 16224, Loss: 2.4266\n",
      "  Batch 16256, Loss: 2.2404\n",
      "  Batch 16288, Loss: 2.2642\n",
      "  Batch 16320, Loss: 2.2165\n",
      "  Batch 16352, Loss: 2.3315\n",
      "  Batch 16384, Loss: 2.3591\n",
      "  Batch 16416, Loss: 2.2243\n",
      "  Batch 16448, Loss: 2.2925\n",
      "  Batch 16480, Loss: 2.2235\n",
      "  Batch 16512, Loss: 2.2281\n",
      "  Batch 16544, Loss: 2.2122\n",
      "  Batch 16576, Loss: 2.2746\n",
      "  Batch 16608, Loss: 2.3402\n",
      "  Batch 16640, Loss: 2.1579\n",
      "  Batch 16672, Loss: 2.3635\n",
      "  Batch 16704, Loss: 2.2316\n",
      "  Batch 16736, Loss: 2.2877\n",
      "  Batch 16768, Loss: 2.2366\n",
      "  Batch 16800, Loss: 2.3261\n",
      "  Batch 16832, Loss: 2.2409\n",
      "  Batch 16864, Loss: 2.3272\n",
      "  Batch 16896, Loss: 2.3013\n",
      "  Batch 16928, Loss: 2.1887\n",
      "  Batch 16960, Loss: 2.2509\n",
      "  Batch 16992, Loss: 2.2781\n",
      "  Batch 17024, Loss: 2.2247\n",
      "  Batch 17056, Loss: 2.2682\n",
      "  Batch 17088, Loss: 2.2684\n",
      "  Batch 17120, Loss: 2.4763\n",
      "  Batch 17152, Loss: 2.2570\n",
      "  Batch 17184, Loss: 2.2762\n",
      "  Batch 17216, Loss: 2.2204\n",
      "  Batch 17248, Loss: 2.1396\n",
      "  Batch 17280, Loss: 2.2631\n",
      "  Batch 17312, Loss: 2.2934\n",
      "  Batch 17344, Loss: 2.2478\n",
      "  Batch 17376, Loss: 2.2554\n",
      "  Batch 17408, Loss: 2.2335\n",
      "  Batch 17440, Loss: 2.1224\n",
      "  Batch 17472, Loss: 2.2524\n",
      "  Batch 17504, Loss: 2.1245\n",
      "  Batch 17536, Loss: 2.2694\n",
      "  Batch 17568, Loss: 2.1949\n",
      "  Batch 17600, Loss: 2.2471\n",
      "  Batch 17632, Loss: 2.1390\n",
      "  Batch 17664, Loss: 2.2096\n",
      "  Batch 17696, Loss: 2.1426\n",
      "  Batch 17728, Loss: 2.2974\n",
      "  Batch 17760, Loss: 2.1556\n",
      "  Batch 17792, Loss: 2.1663\n",
      "  Batch 17824, Loss: 2.1940\n",
      "  Batch 17856, Loss: 2.1161\n",
      "  Batch 17888, Loss: 2.3714\n",
      "  Batch 17920, Loss: 2.1831\n",
      "  Batch 17952, Loss: 2.2170\n",
      "  Batch 17984, Loss: 2.2369\n",
      "  Batch 18016, Loss: 2.1044\n",
      "  Batch 18048, Loss: 2.2092\n",
      "  Batch 18080, Loss: 2.2214\n",
      "  Batch 18112, Loss: 2.1475\n",
      "  Batch 18144, Loss: 2.1052\n",
      "  Batch 18176, Loss: 2.2903\n",
      "  Batch 18208, Loss: 2.2259\n",
      "  Batch 18240, Loss: 2.2082\n",
      "  Batch 18272, Loss: 2.1496\n",
      "  Batch 18304, Loss: 2.3051\n",
      "  Batch 18336, Loss: 2.2221\n",
      "  Batch 18368, Loss: 2.3395\n",
      "  Batch 18400, Loss: 2.2489\n",
      "  Batch 18432, Loss: 2.1616\n",
      "  Batch 18464, Loss: 2.2491\n",
      "  Batch 18496, Loss: 2.3201\n",
      "  Batch 18528, Loss: 2.2871\n",
      "  Batch 18560, Loss: 2.2215\n",
      "  Batch 18592, Loss: 2.2909\n",
      "  Batch 18624, Loss: 2.4198\n",
      "  Batch 18656, Loss: 2.1651\n",
      "  Batch 18688, Loss: 2.4122\n",
      "  Batch 18720, Loss: 2.3479\n",
      "  Batch 18752, Loss: 2.1874\n",
      "  Batch 18784, Loss: 2.2489\n",
      "  Batch 18816, Loss: 2.1874\n",
      "  Batch 18848, Loss: 2.3544\n",
      "  Batch 18880, Loss: 2.0856\n",
      "  Batch 18912, Loss: 2.3100\n",
      "  Batch 18944, Loss: 2.3628\n",
      "  Batch 18976, Loss: 2.2033\n",
      "  Batch 19008, Loss: 2.2666\n",
      "  Batch 19040, Loss: 2.1579\n",
      "  Batch 19072, Loss: 2.1834\n",
      "  Batch 19104, Loss: 2.2421\n",
      "  Batch 19136, Loss: 2.1973\n",
      "  Batch 19168, Loss: 2.2236\n",
      "  Batch 19200, Loss: 2.1780\n",
      "  Batch 19232, Loss: 2.2227\n",
      "  Batch 19264, Loss: 2.2206\n",
      "  Batch 19296, Loss: 2.2608\n",
      "  Batch 19328, Loss: 2.2567\n",
      "  Batch 19360, Loss: 2.3109\n",
      "  Batch 19392, Loss: 2.3521\n",
      "  Batch 19424, Loss: 2.2558\n",
      "  Batch 19456, Loss: 2.1029\n",
      "  Batch 19488, Loss: 2.2637\n",
      "  Batch 19520, Loss: 2.1602\n",
      "  Batch 19552, Loss: 2.1941\n",
      "  Batch 19584, Loss: 2.1665\n",
      "  Batch 19616, Loss: 2.3512\n",
      "  Batch 19648, Loss: 2.3713\n",
      "  Batch 19680, Loss: 2.1280\n",
      "  Batch 19712, Loss: 2.4808\n",
      "  Batch 19744, Loss: 2.2017\n",
      "  Batch 19776, Loss: 2.2299\n",
      "  Batch 19808, Loss: 2.2498\n",
      "  Batch 19840, Loss: 2.4189\n",
      "  Batch 19872, Loss: 2.0878\n",
      "  Batch 19904, Loss: 2.2364\n",
      "  Batch 19936, Loss: 2.3098\n",
      "  Batch 19968, Loss: 2.1373\n",
      "  Batch 20000, Loss: 2.2597\n",
      "  Batch 20032, Loss: 2.1278\n",
      "  Batch 20064, Loss: 2.2129\n",
      "  Batch 20096, Loss: 2.2642\n",
      "  Batch 20128, Loss: 2.1118\n",
      "  Batch 20160, Loss: 2.1316\n",
      "  Batch 20192, Loss: 2.1209\n",
      "  Batch 20224, Loss: 2.2503\n",
      "  Batch 20256, Loss: 2.2806\n",
      "  Batch 20288, Loss: 2.2505\n",
      "  Batch 20320, Loss: 2.0830\n",
      "  Batch 20352, Loss: 2.2193\n",
      "  Batch 20384, Loss: 2.1305\n",
      "  Batch 20416, Loss: 2.1646\n",
      "  Batch 20448, Loss: 2.1167\n",
      "  Batch 20480, Loss: 2.2435\n",
      "  Batch 20512, Loss: 2.1320\n",
      "  Batch 20544, Loss: 2.2226\n",
      "  Batch 20576, Loss: 2.3174\n",
      "  Batch 20608, Loss: 2.1104\n",
      "  Batch 20640, Loss: 2.2425\n",
      "  Batch 20672, Loss: 2.2801\n",
      "  Batch 20704, Loss: 2.1615\n",
      "  Batch 20736, Loss: 2.1638\n",
      "  Batch 20768, Loss: 2.1983\n",
      "  Batch 20800, Loss: 2.1254\n",
      "  Batch 20832, Loss: 2.1112\n",
      "  Batch 20864, Loss: 2.2582\n",
      "  Batch 20896, Loss: 2.2268\n",
      "  Batch 20928, Loss: 2.2655\n",
      "  Batch 20960, Loss: 2.1657\n",
      "  Batch 20992, Loss: 2.2877\n",
      "  Batch 21024, Loss: 2.0636\n",
      "  Batch 21056, Loss: 2.2322\n",
      "  Batch 21088, Loss: 2.3500\n",
      "  Batch 21120, Loss: 2.0830\n",
      "  Batch 21152, Loss: 2.1077\n",
      "  Batch 21184, Loss: 2.2307\n",
      "  Batch 21216, Loss: 2.1242\n",
      "  Batch 21248, Loss: 2.2250\n",
      "  Batch 21280, Loss: 2.2050\n",
      "  Batch 21312, Loss: 2.2259\n",
      "  Batch 21344, Loss: 2.1019\n",
      "  Batch 21376, Loss: 2.2115\n",
      "  Batch 21408, Loss: 2.1428\n",
      "  Batch 21440, Loss: 2.2735\n",
      "  Batch 21472, Loss: 2.1242\n",
      "  Batch 21504, Loss: 2.3067\n",
      "  Batch 21536, Loss: 2.1619\n",
      "  Batch 21568, Loss: 2.2694\n",
      "  Batch 21600, Loss: 2.1199\n",
      "  Batch 21632, Loss: 2.1683\n",
      "  Batch 21664, Loss: 2.1360\n",
      "  Batch 21696, Loss: 2.1584\n",
      "  Batch 21728, Loss: 2.0778\n",
      "  Batch 21760, Loss: 2.1389\n",
      "  Batch 21792, Loss: 2.0979\n",
      "  Batch 21824, Loss: 2.1189\n",
      "  Batch 21856, Loss: 2.1782\n",
      "  Batch 21888, Loss: 2.1894\n",
      "  Batch 21920, Loss: 2.1207\n",
      "  Batch 21952, Loss: 2.1580\n",
      "  Batch 21984, Loss: 2.2118\n",
      "  Batch 22016, Loss: 2.3233\n",
      "  Batch 22048, Loss: 2.1246\n",
      "  Batch 22080, Loss: 2.2509\n",
      "  Batch 22112, Loss: 2.2201\n",
      "  Batch 22144, Loss: 2.1618\n",
      "  Batch 22176, Loss: 2.1685\n",
      "  Batch 22208, Loss: 2.1863\n",
      "  Batch 22240, Loss: 2.3185\n",
      "  Batch 22272, Loss: 2.1362\n",
      "  Batch 22304, Loss: 2.3024\n",
      "  Batch 22336, Loss: 2.4041\n",
      "  Batch 22368, Loss: 2.1121\n",
      "  Batch 22400, Loss: 2.2165\n",
      "  Batch 22432, Loss: 2.1551\n",
      "  Batch 22464, Loss: 2.1547\n",
      "  Batch 22496, Loss: 2.2364\n",
      "  Batch 22528, Loss: 2.0551\n",
      "  Batch 22560, Loss: 2.1526\n",
      "  Batch 22592, Loss: 2.1672\n",
      "  Batch 22624, Loss: 2.3037\n",
      "  Batch 22656, Loss: 2.2416\n",
      "  Batch 22688, Loss: 2.2366\n",
      "  Batch 22720, Loss: 2.1514\n",
      "  Batch 22752, Loss: 2.0250\n",
      "  Batch 22784, Loss: 2.2501\n",
      "  Batch 22816, Loss: 2.2661\n",
      "  Batch 22848, Loss: 2.1426\n",
      "  Batch 22880, Loss: 2.1697\n",
      "  Batch 22912, Loss: 2.1157\n",
      "  Batch 22944, Loss: 2.1521\n",
      "  Batch 22976, Loss: 2.2245\n",
      "  Batch 23008, Loss: 2.1937\n",
      "  Batch 23040, Loss: 2.3613\n",
      "  Batch 23072, Loss: 2.1345\n",
      "  Batch 23104, Loss: 2.3712\n",
      "  Batch 23136, Loss: 2.1853\n",
      "  Batch 23168, Loss: 2.2918\n",
      "  Batch 23200, Loss: 2.2287\n",
      "  Batch 23232, Loss: 2.1340\n",
      "  Batch 23264, Loss: 2.1857\n",
      "  Batch 23296, Loss: 2.1828\n",
      "  Batch 23328, Loss: 2.1523\n",
      "  Batch 23360, Loss: 2.0369\n",
      "  Batch 23392, Loss: 2.0843\n",
      "  Batch 23424, Loss: 2.1907\n",
      "  Batch 23456, Loss: 2.2008\n",
      "  Batch 23488, Loss: 2.1331\n",
      "  Batch 23520, Loss: 2.2756\n",
      "  Batch 23552, Loss: 2.2884\n",
      "  Batch 23584, Loss: 2.0331\n",
      "  Batch 23616, Loss: 2.1206\n",
      "  Batch 23648, Loss: 2.1928\n",
      "  Batch 23680, Loss: 2.1226\n",
      "  Batch 23712, Loss: 2.1089\n",
      "  Batch 23744, Loss: 2.0954\n",
      "  Batch 23776, Loss: 2.2056\n",
      "  Batch 23808, Loss: 2.2672\n",
      "  Batch 23840, Loss: 2.1583\n",
      "  Batch 23872, Loss: 2.2636\n",
      "  Batch 23904, Loss: 2.1778\n",
      "  Batch 23936, Loss: 2.0939\n",
      "  Batch 23968, Loss: 2.1831\n",
      "  Batch 24000, Loss: 2.2726\n",
      "  Batch 24032, Loss: 2.0944\n",
      "  Batch 24064, Loss: 2.1770\n",
      "  Batch 24096, Loss: 2.1409\n",
      "  Batch 24128, Loss: 2.1385\n",
      "  Batch 24160, Loss: 2.1386\n",
      "  Batch 24192, Loss: 2.1586\n",
      "  Batch 24224, Loss: 2.2391\n",
      "  Batch 24256, Loss: 2.1849\n",
      "  Batch 24288, Loss: 2.1367\n",
      "  Batch 24320, Loss: 2.1831\n",
      "  Batch 24352, Loss: 2.1774\n",
      "  Batch 24384, Loss: 2.0933\n",
      "  Batch 24416, Loss: 2.1741\n",
      "  Batch 24448, Loss: 2.1626\n",
      "  Batch 24480, Loss: 2.2002\n",
      "  Batch 24512, Loss: 2.1397\n",
      "  Batch 24544, Loss: 2.1143\n",
      "  Batch 24576, Loss: 1.9749\n",
      "  Batch 24608, Loss: 2.1994\n",
      "  Batch 24640, Loss: 2.1685\n",
      "  Batch 24672, Loss: 2.1556\n",
      "  Batch 24704, Loss: 2.1749\n",
      "  Batch 24736, Loss: 2.2142\n",
      "  Batch 24768, Loss: 2.1939\n",
      "  Batch 24800, Loss: 2.1252\n",
      "  Batch 24832, Loss: 2.2068\n",
      "  Batch 24864, Loss: 2.1347\n",
      "  Batch 24896, Loss: 2.0058\n",
      "  Batch 24928, Loss: 2.0531\n",
      "  Batch 24960, Loss: 2.1390\n",
      "  Batch 24992, Loss: 2.3342\n",
      "  Batch 25024, Loss: 2.1479\n",
      "  Batch 25056, Loss: 2.1680\n",
      "  Batch 25088, Loss: 2.1941\n",
      "  Batch 25120, Loss: 2.2298\n",
      "  Batch 25152, Loss: 2.1461\n",
      "  Batch 25184, Loss: 2.1630\n",
      "  Batch 25216, Loss: 2.1421\n",
      "  Batch 25248, Loss: 2.2803\n",
      "  Batch 25280, Loss: 2.1709\n",
      "  Batch 25312, Loss: 2.1624\n",
      "  Batch 25344, Loss: 2.1851\n",
      "  Batch 25376, Loss: 2.0924\n",
      "  Batch 25408, Loss: 2.1828\n",
      "  Batch 25440, Loss: 2.1014\n",
      "  Batch 25472, Loss: 2.2778\n",
      "  Batch 25504, Loss: 2.2662\n",
      "  Batch 25536, Loss: 2.1758\n",
      "  Batch 25568, Loss: 2.2451\n",
      "  Batch 25600, Loss: 2.1186\n",
      "  Batch 25632, Loss: 2.1082\n",
      "  Batch 25664, Loss: 2.2226\n",
      "  Batch 25696, Loss: 2.1155\n",
      "  Batch 25728, Loss: 2.2017\n",
      "  Batch 25760, Loss: 2.1032\n",
      "  Batch 25792, Loss: 1.9354\n",
      "  Batch 25824, Loss: 2.2448\n",
      "  Batch 25856, Loss: 2.1777\n",
      "  Batch 25888, Loss: 2.2109\n",
      "  Batch 25920, Loss: 2.0858\n",
      "  Batch 25952, Loss: 2.2546\n",
      "  Batch 25984, Loss: 2.1559\n",
      "  Batch 26016, Loss: 2.1295\n",
      "  Batch 26048, Loss: 2.1953\n",
      "  Batch 26080, Loss: 2.0024\n",
      "  Batch 26112, Loss: 2.2297\n",
      "  Batch 26144, Loss: 2.1309\n",
      "  Batch 26176, Loss: 2.1915\n",
      "  Batch 26208, Loss: 2.2071\n",
      "  Batch 26240, Loss: 2.0745\n",
      "  Batch 26272, Loss: 2.2222\n",
      "  Batch 26304, Loss: 2.1904\n",
      "  Batch 26336, Loss: 2.1872\n",
      "  Batch 26368, Loss: 2.1976\n",
      "  Batch 26400, Loss: 2.1636\n",
      "  Batch 26432, Loss: 2.1843\n",
      "  Batch 26464, Loss: 2.1424\n",
      "  Batch 26496, Loss: 2.2150\n",
      "  Batch 26528, Loss: 2.2155\n",
      "  Batch 26560, Loss: 2.1426\n",
      "  Batch 26592, Loss: 2.0280\n",
      "  Batch 26624, Loss: 2.1085\n",
      "  Batch 26656, Loss: 2.0484\n",
      "  Batch 26688, Loss: 2.0570\n",
      "  Batch 26720, Loss: 2.0157\n",
      "  Batch 26752, Loss: 2.1920\n",
      "  Batch 26784, Loss: 2.1382\n",
      "  Batch 26816, Loss: 2.1187\n",
      "  Batch 26848, Loss: 2.1407\n",
      "  Batch 26880, Loss: 2.0681\n",
      "  Batch 26912, Loss: 2.1045\n",
      "  Batch 26944, Loss: 2.1156\n",
      "  Batch 26976, Loss: 2.1397\n",
      "  Batch 27008, Loss: 2.1844\n",
      "  Batch 27040, Loss: 2.1295\n",
      "  Batch 27072, Loss: 2.0752\n",
      "  Batch 27104, Loss: 2.1169\n",
      "  Batch 27136, Loss: 2.0132\n",
      "  Batch 27168, Loss: 2.1056\n",
      "  Batch 27200, Loss: 2.1283\n",
      "  Batch 27232, Loss: 1.9813\n",
      "  Batch 27264, Loss: 2.0455\n",
      "  Batch 27296, Loss: 2.1541\n",
      "  Batch 27328, Loss: 2.1126\n",
      "  Batch 27360, Loss: 2.0409\n",
      "  Batch 27392, Loss: 2.0628\n",
      "  Batch 27424, Loss: 2.3101\n",
      "  Batch 27456, Loss: 2.0825\n",
      "  Batch 27488, Loss: 2.0086\n",
      "  Batch 27520, Loss: 2.2846\n",
      "  Batch 27552, Loss: 2.1221\n",
      "  Batch 27584, Loss: 2.1224\n",
      "  Batch 27616, Loss: 2.1930\n",
      "  Batch 27648, Loss: 2.1715\n",
      "  Batch 27680, Loss: 2.1423\n",
      "  Batch 27712, Loss: 2.2078\n",
      "  Batch 27744, Loss: 2.1237\n",
      "  Batch 27776, Loss: 2.0684\n",
      "  Batch 27808, Loss: 2.0639\n",
      "  Batch 27840, Loss: 2.2148\n",
      "  Batch 27872, Loss: 2.0792\n",
      "  Batch 27904, Loss: 2.1292\n",
      "  Batch 27936, Loss: 2.0548\n",
      "  Batch 27968, Loss: 2.1159\n",
      "  Batch 28000, Loss: 2.1694\n",
      "  Batch 28032, Loss: 1.9918\n",
      "  Batch 28064, Loss: 2.1834\n",
      "  Batch 28096, Loss: 2.2540\n",
      "  Batch 28128, Loss: 2.1157\n",
      "  Batch 28160, Loss: 2.1483\n",
      "  Batch 28192, Loss: 1.9849\n",
      "  Batch 28224, Loss: 2.0886\n",
      "  Batch 28256, Loss: 1.9560\n",
      "  Batch 28288, Loss: 2.1051\n",
      "  Batch 28320, Loss: 2.1026\n",
      "  Batch 28352, Loss: 2.1300\n",
      "  Batch 28384, Loss: 2.0031\n",
      "  Batch 28416, Loss: 2.0974\n",
      "  Batch 28448, Loss: 2.1714\n",
      "  Batch 28480, Loss: 2.1002\n",
      "  Batch 28512, Loss: 2.2186\n",
      "  Batch 28544, Loss: 2.1909\n",
      "  Batch 28576, Loss: 2.2490\n",
      "  Batch 28608, Loss: 2.1328\n",
      "  Batch 28640, Loss: 2.1877\n",
      "  Batch 28672, Loss: 2.1601\n",
      "  Batch 28704, Loss: 2.1847\n",
      "  Batch 28736, Loss: 2.1356\n",
      "  Batch 28768, Loss: 2.2224\n",
      "  Batch 28800, Loss: 2.0444\n",
      "  Batch 28832, Loss: 2.2761\n",
      "  Batch 28864, Loss: 2.0132\n",
      "  Batch 28896, Loss: 2.1745\n",
      "  Batch 28928, Loss: 2.1836\n",
      "  Batch 28960, Loss: 2.0542\n",
      "  Batch 28992, Loss: 2.0905\n",
      "  Batch 29024, Loss: 2.3068\n",
      "  Batch 29056, Loss: 2.0062\n",
      "  Batch 29088, Loss: 2.1731\n",
      "  Batch 29120, Loss: 2.0086\n",
      "  Batch 29152, Loss: 2.0481\n",
      "  Batch 29184, Loss: 2.1829\n",
      "  Batch 29216, Loss: 2.2823\n",
      "  Batch 29248, Loss: 2.1735\n",
      "  Batch 29280, Loss: 2.1523\n",
      "  Batch 29312, Loss: 2.0861\n",
      "  Batch 29344, Loss: 2.2785\n",
      "  Batch 29376, Loss: 2.0722\n",
      "  Batch 29408, Loss: 1.9833\n",
      "  Batch 29440, Loss: 2.1830\n",
      "  Batch 29472, Loss: 2.2300\n",
      "  Batch 29504, Loss: 2.1330\n",
      "  Batch 29536, Loss: 2.0746\n",
      "  Batch 29568, Loss: 2.1278\n",
      "  Batch 29600, Loss: 2.0750\n",
      "  Batch 29632, Loss: 2.1334\n",
      "  Batch 29664, Loss: 2.0128\n",
      "  Batch 29696, Loss: 2.0980\n",
      "  Batch 29728, Loss: 2.1219\n",
      "  Batch 29760, Loss: 2.2188\n",
      "  Batch 29792, Loss: 2.0931\n",
      "  Batch 29824, Loss: 2.1373\n",
      "  Batch 29856, Loss: 2.1067\n",
      "  Batch 29888, Loss: 2.0502\n",
      "  Batch 29920, Loss: 2.0120\n",
      "  Batch 29952, Loss: 2.1145\n",
      "  Batch 29984, Loss: 2.1751\n",
      "  Batch 30016, Loss: 2.2186\n",
      "  Batch 30048, Loss: 2.0655\n",
      "  Batch 30080, Loss: 2.0646\n",
      "  Batch 30112, Loss: 2.1302\n",
      "  Batch 30144, Loss: 2.1168\n",
      "  Batch 30176, Loss: 2.1181\n",
      "  Batch 30208, Loss: 2.1728\n",
      "  Batch 30240, Loss: 2.0401\n",
      "  Batch 30272, Loss: 2.1214\n",
      "  Batch 30304, Loss: 2.0723\n",
      "  Batch 30336, Loss: 2.1484\n",
      "  Batch 30368, Loss: 2.1671\n",
      "  Batch 30400, Loss: 2.2546\n",
      "  Batch 30432, Loss: 2.0537\n",
      "  Batch 30464, Loss: 1.9562\n",
      "  Batch 30496, Loss: 2.1990\n",
      "  Batch 30528, Loss: 2.1351\n",
      "  Batch 30560, Loss: 2.1982\n",
      "  Batch 30592, Loss: 1.9973\n",
      "  Batch 30624, Loss: 2.0310\n",
      "  Batch 30656, Loss: 2.1663\n",
      "  Batch 30688, Loss: 2.1840\n",
      "  Batch 30720, Loss: 2.1280\n",
      "  Batch 30752, Loss: 2.0352\n",
      "  Batch 30784, Loss: 2.0748\n",
      "  Batch 30816, Loss: 2.1181\n",
      "  Batch 30848, Loss: 2.0612\n",
      "  Batch 30880, Loss: 2.1160\n",
      "  Batch 30912, Loss: 2.1038\n",
      "  Batch 30944, Loss: 2.1373\n",
      "  Batch 30976, Loss: 2.2483\n",
      "  Batch 31008, Loss: 2.1958\n",
      "  Batch 31040, Loss: 2.2331\n",
      "  Batch 31072, Loss: 2.1388\n",
      "  Batch 31104, Loss: 2.0730\n",
      "  Batch 31136, Loss: 1.9620\n",
      "  Batch 31168, Loss: 2.1624\n",
      "  Batch 31200, Loss: 2.1714\n",
      "  Batch 31232, Loss: 2.3009\n",
      "  Batch 31264, Loss: 2.0730\n",
      "  Batch 31296, Loss: 2.0768\n",
      "  Batch 31328, Loss: 2.2313\n",
      "  Batch 31360, Loss: 2.1858\n",
      "  Batch 31392, Loss: 2.2535\n",
      "  Batch 31424, Loss: 2.0969\n",
      "  Batch 31456, Loss: 2.1459\n",
      "  Batch 31488, Loss: 2.0424\n",
      "  Batch 31520, Loss: 1.9961\n",
      "  Batch 31552, Loss: 2.0692\n",
      "  Batch 31584, Loss: 2.1093\n",
      "  Batch 31616, Loss: 2.1252\n",
      "  Batch 31648, Loss: 2.1447\n",
      "  Batch 31680, Loss: 2.1683\n",
      "  Batch 31712, Loss: 2.0897\n",
      "  Batch 31744, Loss: 2.0784\n",
      "  Batch 31776, Loss: 2.0335\n",
      "  Batch 31808, Loss: 2.0611\n",
      "  Batch 31840, Loss: 2.0072\n",
      "  Batch 31872, Loss: 2.1125\n",
      "  Batch 31904, Loss: 2.1645\n",
      "  Batch 31936, Loss: 2.1472\n",
      "  Batch 31968, Loss: 2.0496\n",
      "  Batch 32000, Loss: 2.1245\n",
      "  Batch 32032, Loss: 2.0605\n",
      "  Batch 32064, Loss: 2.1544\n",
      "  Batch 32096, Loss: 2.1950\n",
      "  Batch 32128, Loss: 2.0637\n",
      "  Batch 32160, Loss: 2.0807\n",
      "  Batch 32192, Loss: 2.1996\n",
      "  Batch 32224, Loss: 2.0609\n",
      "  Batch 32256, Loss: 2.0982\n",
      "  Batch 32288, Loss: 2.0877\n",
      "  Batch 32320, Loss: 2.0825\n",
      "  Batch 32352, Loss: 2.1256\n",
      "  Batch 32384, Loss: 2.1060\n",
      "  Batch 32416, Loss: 2.0364\n",
      "  Batch 32448, Loss: 2.0695\n",
      "  Batch 32480, Loss: 2.0373\n",
      "  Batch 32512, Loss: 2.2162\n",
      "  Batch 32544, Loss: 2.1947\n",
      "  Batch 32576, Loss: 2.1378\n",
      "  Batch 32608, Loss: 1.9558\n",
      "  Batch 32640, Loss: 2.0813\n",
      "  Batch 32672, Loss: 2.0999\n",
      "  Batch 32704, Loss: 2.0053\n",
      "  Batch 32736, Loss: 2.0594\n",
      "  Batch 32768, Loss: 1.9784\n",
      "  Batch 32800, Loss: 1.9528\n",
      "  Batch 32832, Loss: 2.1585\n",
      "  Batch 32864, Loss: 1.9809\n",
      "  Batch 32896, Loss: 2.1183\n",
      "  Batch 32928, Loss: 2.0624\n",
      "  Batch 32960, Loss: 2.1685\n",
      "  Batch 32992, Loss: 2.1009\n",
      "  Batch 33024, Loss: 2.1122\n",
      "  Batch 33056, Loss: 1.9651\n",
      "  Batch 33088, Loss: 2.0759\n",
      "  Batch 33120, Loss: 2.0575\n",
      "  Batch 33152, Loss: 2.2438\n",
      "  Batch 33184, Loss: 2.0238\n",
      "  Batch 33216, Loss: 1.9802\n",
      "  Batch 33248, Loss: 2.0728\n",
      "  Batch 33280, Loss: 2.1776\n",
      "  Batch 33312, Loss: 2.1557\n",
      "  Batch 33344, Loss: 2.0192\n",
      "  Batch 33376, Loss: 2.1245\n",
      "  Batch 33408, Loss: 2.0457\n",
      "  Batch 33440, Loss: 2.1147\n",
      "  Batch 33472, Loss: 2.1482\n",
      "  Batch 33504, Loss: 2.1115\n",
      "  Batch 33536, Loss: 2.1899\n",
      "  Batch 33568, Loss: 2.1381\n",
      "  Batch 33600, Loss: 2.0134\n",
      "  Batch 33632, Loss: 2.0469\n",
      "  Batch 33664, Loss: 2.1145\n",
      "  Batch 33696, Loss: 2.1233\n",
      "  Batch 33728, Loss: 2.0797\n",
      "  Batch 33760, Loss: 2.0067\n",
      "  Batch 33792, Loss: 2.0533\n",
      "Epoch 3/3, Loss: 2.2620\n",
      "\n",
      "Eğitim süresi: 245.49900446666666 dakika\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "459909cccf6c2779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T15:20:46.130871Z",
     "start_time": "2026-01-04T15:20:46.128071Z"
    }
   },
   "source": [
    "def translate(model, src_tokenizer, tgt_tokenizer, sentence, max_len=50, device='mps'):\n",
    "    \"\"\"\n",
    "    Tek bir cümleyi çevir (Greedy Decoding).\n",
    "\n",
    "    Args:\n",
    "        model: Eğitilmiş Transformer modeli\n",
    "        src_tokenizer: Kaynak dil tokenizer (Türkçe)\n",
    "        tgt_tokenizer: Hedef dil tokenizer (İngilizce)\n",
    "        sentence: Çevrilecek cümle\n",
    "        max_len: Maksimum çıktı uzunluğu\n",
    "        device: 'cpu' veya 'mps'\n",
    "\n",
    "    Returns:\n",
    "        Çevrilmiş cümle\n",
    "    \"\"\"\n",
    "    model.eval()  # Evaluation mode (dropout kapalı)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 1️⃣ Kaynak cümleyi tokenize et\n",
    "        src_ids = src_tokenizer.encode(sentence)  # [BOS, ..., EOS]\n",
    "        src_tensor = torch.tensor([src_ids]).to(device)  # (1, src_len)\n",
    "\n",
    "        print(f\"📥 Girdi: {sentence}\")\n",
    "        print(f\"   Tokenlar: {src_tokenizer.encode(sentence)}\")\n",
    "        print(f\"   IDs: {src_ids}\")\n",
    "\n",
    "        # 2️⃣ Encoder'dan geçir\n",
    "        src_mask = model.make_src_mask(src_tensor)\n",
    "        encoder_output = model.encode(src_tensor, src_mask)\n",
    "\n",
    "        # 3️⃣ Decoder başlangıcı: sadece <BOS>\n",
    "        tgt_ids = [tgt_tokenizer.bos_token_id]\n",
    "\n",
    "        # 4️⃣ Autoregressive üretim\n",
    "        for step in range(max_len):\n",
    "            tgt_tensor = torch.tensor([tgt_ids]).to(device)  # (1, current_len)\n",
    "            tgt_mask = model.make_tgt_mask(tgt_tensor)\n",
    "\n",
    "            # Decoder çıktısı\n",
    "            logits = model.decode(tgt_tensor, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "            # Son pozisyonun olasılıkları\n",
    "            next_token_logits = logits[0, -1, :]  # (vocab_size,)\n",
    "\n",
    "            # En yüksek olasılıklı token (greedy)\n",
    "            next_token_id = next_token_logits.argmax().item()\n",
    "\n",
    "            # Token'ı ekle\n",
    "            tgt_ids.append(next_token_id)\n",
    "\n",
    "            # <EOS> geldiyse dur\n",
    "            if next_token_id == tgt_tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        # 5️⃣ Decode et\n",
    "        translation = tgt_tokenizer.decode(tgt_ids)\n",
    "\n",
    "        print(f\"📤 Çıktı: {translation}\")\n",
    "        print(f\"   IDs: {tgt_ids}\")\n",
    "\n",
    "    return translation\n"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "b5a0a30d6ab3e38f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T15:20:50.357798Z",
     "start_time": "2026-01-04T15:20:46.136715Z"
    }
   },
   "source": [
    "sentence =\"How to care about our health?\"\n",
    "translation = translate(model,en_tokenizer,tr_tokenizer,sentence,max_len=max_seq_len,device=device)\n",
    "print(f\"Çeviri: {translation}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Girdi: How to care about our health?\n",
      "   Tokenlar: [2, 273, 35, 680, 298, 367, 408, 793, 3]\n",
      "   IDs: [2, 273, 35, 680, 298, 367, 408, 793, 3]\n",
      "📤 Çıktı: sağlıkmızla nasıl bakılacağınız?\n",
      "   IDs: [2, 575, 757, 184, 4, 485, 427, 742, 772, 85, 595, 795, 3]\n",
      "Çeviri: sağlıkmızla nasıl bakılacağınız?\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "27f2be27d4029c92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T15:33:16.865723Z",
     "start_time": "2026-01-04T15:33:16.864115Z"
    }
   },
   "source": [
    "torch.save(model.state_dict(), 'model/final_model.pth')\n",
    "print(f'Model kaydedildi: model/final_model.pth')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model kaydedildi: model/final_model.pth\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "763e9e7ab6aef6ad",
   "metadata": {},
   "source": [
    "### loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7ffbc5c04e98eeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T15:20:50.740034Z",
     "start_time": "2026-01-04T15:20:50.492125Z"
    }
   },
   "source": [
    "model_path = 'model/final_model.pth'\n",
    "\n",
    "saved_model = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_len=max_seq_len,\n",
    "    dropout=0.1,\n",
    "    padd_idx=0\n",
    ").to(device)\n",
    "\n",
    "# Load state dictionary\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Filter out unexpected keys\n",
    "model_state_dict = saved_model.state_dict()\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if k in model_state_dict}\n",
    "model_state_dict.update(filtered_state_dict)\n",
    "saved_model.load_state_dict(model_state_dict)\n",
    "saved_model.to(device)\n",
    "saved_model.eval()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding): Embedding(800, 256)\n",
       "    )\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (token_embedding): TokenEmbedding(\n",
       "      (embedding): Embedding(800, 256)\n",
       "    )\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (masked_self_attention): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attention): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (W_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (output_projections): Linear(in_features=256, out_features=800, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "70b49ec7b4152492",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T15:20:51.155199Z",
     "start_time": "2026-01-04T15:20:50.748775Z"
    }
   },
   "source": [
    "sentence =\"But how can I prove it?\"\n",
    "translation = translate(saved_model,en_tokenizer,tr_tokenizer,sentence,max_len=max_seq_len,device=device)\n",
    "print(f\"Çeviri: {translation}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Girdi: But how can I prove it?\n",
      "   Tokenlar: [2, 260, 273, 157, 570, 76, 52, 84, 793, 3]\n",
      "   IDs: [2, 260, 273, 157, 570, 76, 52, 84, 793, 3]\n",
      "📤 Çıktı: ama bunu nasıl kanıtlayabilirim?\n",
      "   IDs: [2, 661, 7, 172, 485, 12, 296, 756, 4, 48, 143, 54, 795, 3]\n",
      "Çeviri: ama bunu nasıl kanıtlayabilirim?\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "8c42aa136bd203e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T15:23:10.700837Z",
     "start_time": "2026-01-04T15:23:10.010380Z"
    }
   },
   "source": [
    "sentence =\"Good morning everyone!\"\n",
    "translation = translate(saved_model,en_tokenizer,tr_tokenizer,sentence,max_len=max_seq_len,device=device)\n",
    "print(f\"Çeviri: {translation}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Girdi: Good morning everyone!\n",
      "   Tokenlar: [2, 241, 128, 29, 23, 714, 676, 323, 1, 3]\n",
      "   IDs: [2, 241, 128, 29, 23, 714, 676, 323, 1, 3]\n",
      "📤 Çıktı: İyi sabah herkes için iyi sabah ⁇ \n",
      "   IDs: [2, 174, 759, 749, 195, 762, 65, 218, 753, 40, 57, 469, 195, 762, 65, 1, 3]\n",
      "Çeviri: İyi sabah herkes için iyi sabah ⁇ \n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T15:25:37.773658Z",
     "start_time": "2026-01-04T15:25:37.116415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence =\"What is your name?\"\n",
    "translation = translate(saved_model,en_tokenizer,tr_tokenizer,sentence,max_len=max_seq_len,device=device)\n",
    "print(f\"Çeviri: {translation}\")"
   ],
   "id": "9e0c62411fb08d8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Girdi: What is your name?\n",
      "   Tokenlar: [2, 350, 56, 261, 49, 414, 793, 3]\n",
      "   IDs: [2, 350, 56, 261, 49, 414, 793, 3]\n",
      "📤 Çıktı: adınız nedir?\n",
      "   IDs: [2, 651, 595, 69, 459, 795, 3]\n",
      "Çeviri: adınız nedir?\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T15:26:22.996290Z",
     "start_time": "2026-01-04T15:26:22.735266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence =\"One more example sentence for translation.\"\n",
    "translation = translate(saved_model,en_tokenizer,tr_tokenizer,sentence,max_len=max_seq_len,device=device)\n",
    "print(f\"Çeviri: {translation}\")"
   ],
   "id": "b6f92d85b31491ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Girdi: One more example sentence for translation.\n",
      "   Tokenlar: [2, 253, 222, 109, 522, 42, 11, 43, 252, 68, 475, 759, 57, 772, 3]\n",
      "   IDs: [2, 253, 222, 109, 522, 42, 11, 43, 252, 68, 475, 759, 57, 772, 3]\n",
      "📤 Çıktı: çeviri için bir daha fazla örnek cümlesi.\n",
      "   IDs: [2, 605, 8, 749, 57, 28, 129, 283, 650, 24, 122, 124, 752, 142, 773, 3]\n",
      "Çeviri: çeviri için bir daha fazla örnek cümlesi.\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5d297b9f84240883"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
